[{"content":"About Me I’m the creator of TillyNet—an enterprise-grade, self-hosted lab environment designed to simulate real-world production networks. Built from scratch using open-source tooling, routed VLAN segmentation, containerized services, internal PKI, SSO, recursive DNS, and automated deployment pipelines, TillyNet continues to evolve as a full-stack testing platform for SDN, automation, and advanced networking.\nProfessionally, I support the U.S. Department of Defense as a Network Engineer focused on secure enclave operations, high-availability infrastructure, and enterprise-scale automation. I manage and maintain the U.S. Marine Corps’ classified and unclassified Base Area Networks (BAN/LAN) across SDA fabric sites—delivering mission-critical connectivity for thousands of users. My work spans routing architecture, encrypted transport, OSPF/BGP redistribution, real-time diagnostics, multi-VRF segmentation, and full-stack SDA operations using Cisco Catalyst Center.\nI hold the Cisco Certified Network Professional – Enterprise (CCNP Enterprise) certification, along with the Cisco Certified Specialist – Enterprise Core (CCS-ECore) and Automating and Programming Cisco Enterprise Solutions (ENAUTO) specialist certifications. These reflect my depth in enterprise networking, software-defined architectures, and network automation. I also maintain Cisco CCNA and CompTIA Security+, and I operate under a DoD Tier 3 Secret Clearance, enabling me to support both classified and unclassified domains confidently and securely.\nI specialize in proactive network reliability—identifying, diagnosing, and resolving complex issues before they impact operations—and continuously improving the resilience and visibility of the infrastructure I support. Whether operating independently or in collaboration with Watch Officers, I maintain real-time situational awareness of routing behavior, authentication systems, device health, and enclave interoperability.\nOutside my DoD work, TillyNet serves as my platform to push boundaries: SDN, LISP/VXLAN, multi-VRF segmentation, PKI hierarchy engineering, reverse-proxy and identity integration, IaC workflows, Python API tooling, and distributed authentication systems. It’s where I experiment, break things, learn, and rebuild with precision.\nWhat You\u0026rsquo;ll Find Here Blog posts detailing each phase of the network and service buildout Troubleshooting insights gained from hands-on diagnostics Python automation and workflows using GitHub Actions and cron jobs Infrastructure-as-code experiments with scripting tools that work with network devices Connect GitHub LinkedIn Thanks for visiting!\n","permalink":"https://blog.tillynet.com/about/","summary":"\u003ch1 id=\"about-me\"\u003eAbout Me\u003c/h1\u003e\n\u003cp\u003eI’m the creator of \u003cstrong\u003eTillyNet\u003c/strong\u003e—an enterprise-grade, self-hosted lab environment designed to simulate real-world production networks. Built from scratch using open-source tooling, routed VLAN segmentation, containerized services, internal PKI, SSO, recursive DNS, and automated deployment pipelines, TillyNet continues to evolve as a full-stack testing platform for SDN, automation, and advanced networking.\u003c/p\u003e\n\u003cp\u003eProfessionally, I support the \u003cstrong\u003eU.S. Department of Defense\u003c/strong\u003e as a Network Engineer focused on secure enclave operations, high-availability infrastructure, and enterprise-scale automation. I manage and maintain the U.S. Marine Corps’ classified and unclassified Base Area Networks (BAN/LAN) across SDA fabric sites—delivering mission-critical connectivity for thousands of users. My work spans routing architecture, encrypted transport, OSPF/BGP redistribution, real-time diagnostics, multi-VRF segmentation, and full-stack SDA operations using Cisco Catalyst Center.\u003c/p\u003e","title":"About"},{"content":"Implementing Enterprise-Grade 802.1X EAP-TLS Authentication in My Home Lab Introduction Network Access Control (NAC) has become a cornerstone of enterprise security, ensuring that only authorized devices can connect to corporate networks. While technologies like 802.1X are commonplace in enterprise environments, implementing them in a home lab presents unique challenges and learning opportunities. In this article, I\u0026rsquo;ll walk through my complete implementation of 802.1X EAP-TLS authentication using FreeRADIUS, Samba4 Active Directory, and Cisco Catalyst switches.\nThis implementation goes beyond simple password-based authentication by leveraging X.509 certificates for device authentication, providing the same level of security found in enterprise environments while serving as an excellent learning platform for understanding enterprise network security architectures.\nArchitecture Overview My home lab environment consists of several key components that work together to provide enterprise-grade network access control:\nNetwork Infrastructure:\nManagement VLAN: 172.16.99.0/24 Domain: tillynet.lan Samba4 Active Directory Domain Controller: 172.30.30.30 pfSense Firewall with FreeRADIUS: 172.16.99.1 Cisco Catalyst 2960 Switch serving as the Network Access Server (NAS) Windows 10/11 domain-joined workstation: TILLYPC Security Infrastructure:\nTwo-tier PKI with offline Root CA Samba4-hosted Intermediate Certificate Authority EAP-TLS authentication protocol Certificate-based device authentication The architecture follows enterprise best practices by separating the authentication server (FreeRADIUS), directory services (Samba4 AD), and network enforcement (Cisco switch) into distinct components that communicate via standardized protocols.\nCertificate Infrastructure Design The foundation of any EAP-TLS implementation is a robust Public Key Infrastructure. I designed a two-tier PKI that mirrors enterprise deployments:\nRoot Certificate Authority The Root CA operates offline for maximum security, issuing only intermediate certificates. This design ensures that the root private key remains protected while allowing operational flexibility through the intermediate CA.\nIntermediate Certificate Authority I integrated the Intermediate CA directly into my Samba4 Active Directory environment. This approach provides several advantages:\nCentralized certificate management Integration with AD security groups Simplified certificate lifecycle management Realistic enterprise-like setup Server Certificate Generation For the FreeRADIUS server certificate, I developed a shell script that automates the certificate generation process while ensuring proper extensions and Subject Alternative Names (SANs):\n#!/bin/bash SERVICE_FQDN=$1 BASE_DIR=\u0026#34;/usr/local/samba/private/tls\u0026#34; KEY_SIZE=2048 DAYS_VALID=825 INTERMEDIATE_KEY=\u0026#34;${BASE_DIR}/intermediate.key\u0026#34; INTERMEDIATE_CRT=\u0026#34;${BASE_DIR}/intermediate.crt\u0026#34; CA_CHAIN=\u0026#34;${BASE_DIR}/ca-chain.crt\u0026#34; if [[ -z \u0026#34;$SERVICE_FQDN\u0026#34; ]]; then echo \u0026#34;Usage: $0 \u0026lt;service.fqdn\u0026gt;\u0026#34; exit 1 fi OUTPUT_DIR=\u0026#34;${BASE_DIR}/${SERVICE_FQDN}\u0026#34; mkdir -p \u0026#34;${OUTPUT_DIR}\u0026#34; # Generate private key and certificate signing request openssl req -new -newkey rsa:${KEY_SIZE} -nodes \\ -keyout \u0026#34;${OUTPUT_DIR}/${SERVICE_FQDN}.key\u0026#34; \\ -out \u0026#34;${OUTPUT_DIR}/${SERVICE_FQDN}.csr\u0026#34; \\ -subj \u0026#34;/CN=${SERVICE_FQDN}\u0026#34; \\ -addext \u0026#34;subjectAltName = DNS:${SERVICE_FQDN}\u0026#34; # Create certificate extensions EXT_FILE=\u0026#34;${OUTPUT_DIR}/v3_ext.cnf\u0026#34; cat \u0026gt; \u0026#34;${EXT_FILE}\u0026#34; \u0026lt;\u0026lt;EOF [ v3_req ] basicConstraints = CA:FALSE keyUsage = digitalSignature, keyEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [ alt_names ] DNS.1 = ${SERVICE_FQDN} EOF # Sign certificate with intermediate CA openssl x509 -req \\ -in \u0026#34;${OUTPUT_DIR}/${SERVICE_FQDN}.csr\u0026#34; \\ -CA \u0026#34;${INTERMEDIATE_CRT}\u0026#34; \\ -CAkey \u0026#34;${INTERMEDIATE_KEY}\u0026#34; \\ -CAcreateserial \\ -out \u0026#34;${OUTPUT_DIR}/${SERVICE_FQDN}.crt\u0026#34; \\ -days ${DAYS_VALID} -sha256 \\ -extfile \u0026#34;${EXT_FILE}\u0026#34; \\ -extensions v3_req # Build complete certificate chain cat \u0026#34;${OUTPUT_DIR}/${SERVICE_FQDN}.crt\u0026#34; \u0026#34;${CA_CHAIN}\u0026#34; \u0026gt; \u0026#34;${OUTPUT_DIR}/${SERVICE_FQDN}-fullchain.crt\u0026#34; rm -f \u0026#34;${EXT_FILE}\u0026#34; This script ensures that the server certificate includes the proper Extended Key Usage (EKU) for server authentication and maintains the complete certificate chain required for client validation.\nClient Certificate Generation Client certificates require different extensions, specifically the Client Authentication EKU. I created a separate script for generating client certificates:\n#!/bin/bash FQDN=\u0026#34;$1\u0026#34; CERT_DIR=\u0026#34;/usr/local/samba/private/tls/$FQDN\u0026#34; mkdir -p \u0026#34;$CERT_DIR\u0026#34; # Generate client certificate with appropriate extensions openssl req -new -nodes -newkey rsa:2048 \\ -keyout \u0026#34;$CERT_DIR/$FQDN.key\u0026#34; \\ -out \u0026#34;$CERT_DIR/$FQDN.csr\u0026#34; \\ -subj \u0026#34;/CN=$FQDN\u0026#34; \\ -config \u0026lt;(cat \u0026lt;\u0026lt;EOF [ req ] default_bits = 2048 prompt = no default_md = sha256 req_extensions = v3_req distinguished_name = dn [ dn ] CN = $FQDN [ v3_req ] basicConstraints = CA:FALSE keyUsage = digitalSignature, keyEncipherment extendedKeyUsage = clientAuth subjectAltName = @alt_names [ alt_names ] DNS.1 = $FQDN EOF ) # Sign with intermediate CA openssl x509 -req \\ -in \u0026#34;$CERT_DIR/$FQDN.csr\u0026#34; \\ -CA /usr/local/samba/private/tls/intermediate.crt \\ -CAkey /usr/local/samba/private/tls/intermediate.key \\ -CAcreateserial \\ -out \u0026#34;$CERT_DIR/$FQDN.crt\u0026#34; \\ -days 825 -sha256 \\ -extfile \u0026lt;(cat \u0026lt;\u0026lt;EOF [ v3_req ] basicConstraints = CA:FALSE keyUsage = digitalSignature, keyEncipherment extendedKeyUsage = clientAuth subjectAltName = @alt_names [ alt_names ] DNS.1 = $FQDN EOF ) cat \u0026#34;$CERT_DIR/$FQDN.crt\u0026#34; /usr/local/samba/private/tls/ca-chain.crt \u0026gt; \u0026#34;$CERT_DIR/$FQDN-fullchain.crt\u0026#34; PKCS#12 Export for Windows Windows workstations require certificates in PKCS#12 format. I automated this conversion process:\n#!/bin/bash FQDN=\u0026#34;$1\u0026#34; PFX_PASSWORD=\u0026#34;${2:-\u0026#34;\u0026#34;}\u0026#34; CERT_BASE_DIR=\u0026#34;/usr/local/samba/private/tls\u0026#34; CERT_DIR=\u0026#34;$CERT_BASE_DIR/$FQDN\u0026#34; KEY_FILE=\u0026#34;$CERT_DIR/$FQDN.key\u0026#34; CERT_FILE=\u0026#34;$CERT_DIR/$FQDN.crt\u0026#34; CHAIN_FILE=\u0026#34;$CERT_BASE_DIR/ca-chain.crt\u0026#34; PFX_OUTPUT=\u0026#34;$CERT_DIR/$FQDN.pfx\u0026#34; # Validate all required files exist for file in \u0026#34;$KEY_FILE\u0026#34; \u0026#34;$CERT_FILE\u0026#34; \u0026#34;$CHAIN_FILE\u0026#34;; do if [[ ! -f \u0026#34;$file\u0026#34; ]]; then echo \u0026#34;Error: Required file not found: $file\u0026#34; exit 1 fi done # Generate PKCS#12 bundle openssl pkcs12 -export \\ -inkey \u0026#34;$KEY_FILE\u0026#34; \\ -in \u0026#34;$CERT_FILE\u0026#34; \\ -certfile \u0026#34;$CHAIN_FILE\u0026#34; \\ -out \u0026#34;$PFX_OUTPUT\u0026#34; \\ -passout pass:\u0026#34;$PFX_PASSWORD\u0026#34; This approach ensures that Windows clients receive certificates with the complete certificate chain, enabling proper validation against the root CA.\nFreeRADIUS Configuration Deep Dive Configuring FreeRADIUS for EAP-TLS with Active Directory integration required careful attention to several components. The pfSense FreeRADIUS package provides a web interface, but understanding the underlying configuration is crucial for troubleshooting.\nRADIUS Client Configuration I configured the Cisco switch as a RADIUS client (NAS) with these parameters:\nClient Name: TL-ASW1-2960C Client IP Address: The management IP of the switch Shared Secret: A complex pre-shared key matching the switch\u0026rsquo;s AAA configuration NAS Type: Cisco The shared secret must be identical on both the switch and FreeRADIUS server. I used a 32-character randomly generated string to ensure security.\nLDAP Integration Challenges and Solutions The most complex aspect of this implementation was configuring FreeRADIUS to authenticate Windows computer accounts against Active Directory. This presented several challenges that required deep understanding of both LDAP and Windows authentication mechanisms.\nInitial LDAP Configuration Attempts:\nMy first attempt used user-focused LDAP settings:\nBase DN: DC=tillynet,DC=lan Filter: (sAMAccountName=%{Stripped-User-Name:-%{User-Name}}) Base Filter: (objectClass=person) This configuration failed because Windows computer authentication works differently from user authentication. Computer accounts authenticate using the format host/computername.domain, and they are stored in the CN=Computers container by default.\nCorrected LDAP Configuration:\nAfter extensive troubleshooting, I implemented these settings:\nBase DN: CN=Computers,DC=tillynet,DC=lan Filter: (\u0026amp;(objectClass=computer)(|(sAMAccountName=%{mschap:User-Name}$)(servicePrincipalName=%{User-Name}))) Base Filter: (objectClass=computer) LDAP Authentication: Enabled LDAP Authorization: Enabled (critical for proper operation) The filter deserves special explanation:\nobjectClass=computer ensures we only match computer accounts sAMAccountName=%{mschap:User-Name}$ handles the computer account name with the required dollar sign suffix servicePrincipalName=%{User-Name} handles the host/computername.domain format used during authentication EAP-TLS Configuration The EAP configuration required specific settings for proper TLS tunnel operation:\nCritical EAP Settings:\nDefault EAP Type: TLS TLS Configuration: Server Certificate: radius.tillynet.lan Private Key: Corresponding private key Certificate Chain: Complete CA chain PEAP Configuration: Default EAP Type: MSCHAPV2 (for inner tunnel, though not used in pure EAP-TLS) Copy Request to Tunnel: Yes Use Tunneled Reply: Yes The \u0026ldquo;Copy Request to Tunnel\u0026rdquo; and \u0026ldquo;Use Tunneled Reply\u0026rdquo; settings are crucial for passing authentication attributes between the outer and inner authentication contexts.\nInterface Binding I configured FreeRADIUS to bind specifically to the management interface (172.16.99.1) rather than all interfaces. This approach provides better security and clearer troubleshooting by ensuring RADIUS traffic flows through the intended interface.\nGroup Policy Configuration for 802.1X Deploying 802.1X authentication to Windows workstations requires careful Group Policy configuration. I created a comprehensive GPO that handles both certificate deployment and network authentication settings.\nCertificate Deployment Strategy While I could have used automatic certificate enrollment, I chose manual certificate deployment for this lab to better understand the process. In production environments, I would recommend:\nCertificate Templates configured for auto-enrollment Group Policy-based certificate deployment Certificate lifecycle management through AD Certificate Services Wired Network Policy Configuration I created a new GPO with the following configuration path: Computer Configuration → Policies → Windows Settings → Security Settings → Wired Network (IEEE 802.3) Policies\nPolicy Settings:\nPolicy Name: \u0026ldquo;Enterprise 802.1X EAP-TLS Authentication\u0026rdquo; Use Windows Wired Auto Config: Enabled Network Authentication Method: Smart Card or other Certificate (EAP-TLS) Authentication Mode: Computer authentication Validate server certificate: Enabled Connect to these servers: radius.tillynet.lan Trusted Root Certification Authorities: My root CA Use simple certificate selection: Enabled GPO Deployment and Verification I linked the GPO to the Workstations OU where my test computer resides. To verify proper application, I used:\ngpresult /r /scope computer gpupdate /force The gpresult command confirmed that the wired network policy was being applied correctly to the computer.\nCisco Catalyst 2960 Configuration The network switch serves as the 802.1X authenticator, enforcing authentication before allowing network access. My configuration implements industry-standard practices for enterprise environments.\nGlobal AAA Configuration # Enable new AAA model\raaa new-model\r# Configure RADIUS server\rradius-server host 172.16.99.1 auth-port 1812 acct-port 1813 key YourSharedSecret\rradius-server timeout 10\rradius-server retransmit 3\r# Configure 802.1X authentication\raaa authentication dot1x default group radius The timeout and retransmit values balance responsiveness with reliability. A 10-second timeout allows for network latency while preventing excessive delays during authentication failures.\n802.1X Global Configuration # Enable 802.1X system-wide\rdot1x system-auth-control This command enables 802.1X globally on the switch. Individual ports must still be configured for 802.1X authentication.\nPort-Level Configuration I configured a test port (FastEthernet0/2) for 802.1X authentication:\ninterface FastEthernet0/2\rswitchport mode access\rauthentication port-control auto\rdot1x pae authenticator\rspanning-tree portfast Configuration Explanation:\nswitchport mode access: Configures the port as an access port authentication port-control auto: Enables 802.1X authentication (port blocked until successful authentication) dot1x pae authenticator: Sets the port as an 802.1X authenticator spanning-tree portfast: Reduces convergence time for end-device connections Comprehensive Troubleshooting Journey The implementation process involved extensive troubleshooting that provided valuable insights into 802.1X authentication mechanisms. I\u0026rsquo;ll detail the major issues encountered and their resolutions.\nIssue 1: RADIUS Port Configuration Mismatch Symptoms: Initial authentication attempts showed no communication between the switch and FreeRADIUS server.\nInvestigation Process: I enabled debugging on the Cisco switch:\nterminal monitor\rdebug radius\rdebug dot1x all The debug output revealed:\n*Feb 10 23:25:41.810: RADIUS(00000000): Send Access-Request to 172.16.99.1:1645 Root Cause: The switch was using legacy RADIUS ports (1645/1646) instead of the standard ports (1812/1813).\nResolution: I updated the switch configuration to explicitly specify the correct ports:\nradius-server host 172.16.99.1 auth-port 1812 acct-port 1813 key YourSharedSecret Verification: After the configuration change, the debug output showed:\n*Feb 10 23:39:15.446: RADIUS: Received from id 1645/36 172.16.99.1:1812, Access-Reject, len 44 This confirmed that RADIUS communication was working, though authentication was still failing.\nIssue 2: FreeRADIUS LDAP Filter Configuration Symptoms: RADIUS packets were reaching the server, but authentication consistently failed with \u0026ldquo;Access-Reject\u0026rdquo; responses.\nInvestigation Process: I examined the FreeRADIUS logs at /var/log/radius.log and found:\n[ldap] ERROR: Unable to create filter\r[ldap] ERROR: Failed to create LDAP filter Root Cause Analysis: The LDAP filter was configured for user authentication rather than computer authentication. Windows computer accounts have distinct characteristics:\nThey authenticate using the host/computername.domain format Computer accounts are stored in CN=Computers by default Computer account sAMAccountName includes a trailing dollar sign Resolution: I reconfigured the LDAP settings:\nBase DN: Changed from DC=tillynet,DC=lan to CN=Computers,DC=tillynet,DC=lan Filter: Updated to (\u0026amp;(objectClass=computer)(|(sAMAccountName=%{mschap:User-Name}$)(servicePrincipalName=%{User-Name}))) Base Filter: Changed from (objectClass=person) to (objectClass=computer) Additional Configuration: I also enabled LDAP Authorization, which is required for FreeRADIUS to properly process LDAP responses and make authorization decisions.\nIssue 3: EAP-TLS Tunnel Configuration Symptoms: Even after resolving the LDAP issues, authentication was intermittently failing with TLS-related errors.\nInvestigation Process: Using radiusd -X in debug mode, I observed that the EAP-TLS handshake was completing, but the inner authentication was failing.\nRoot Cause: The EAP tunnel configuration was not properly copying authentication attributes between the outer and inner authentication contexts.\nResolution: I updated the EAP-PEAP configuration:\nCopy Request to Tunnel: Changed from \u0026ldquo;No\u0026rdquo; to \u0026ldquo;Yes\u0026rdquo; Use Tunneled Reply: Changed from \u0026ldquo;No\u0026rdquo; to \u0026ldquo;Yes\u0026rdquo; These settings ensure that authentication attributes are properly passed through the TLS tunnel, allowing the inner authentication mechanism to access the necessary user/computer information.\nIssue 4: Certificate Validation Problems Symptoms: Some authentication attempts failed with certificate validation errors on the client side.\nInvestigation Process: I used Windows Event Viewer to examine the System log and found certificate chain validation errors.\nRoot Cause: The Windows client was not properly validating the FreeRADIUS server certificate because:\nThe certificate chain was incomplete The server certificate\u0026rsquo;s Common Name didn\u0026rsquo;t match the configured server name Resolution:\nComplete Certificate Chain: I ensured that the FreeRADIUS server was configured with the complete certificate chain (server cert + intermediate CA + root CA) Certificate Subject Verification: I verified that the server certificate\u0026rsquo;s CN matched the FQDN configured in the client\u0026rsquo;s 802.1X profile Root CA Installation: I confirmed that the root CA certificate was properly installed in the Windows client\u0026rsquo;s Trusted Root Certification Authorities store Issue 5: Network Timeout and Performance Optimization Symptoms: Authentication was working but taking an excessive amount of time, sometimes causing timeouts.\nInvestigation Process: I analyzed the authentication flow timing and identified several bottlenecks:\nLDAP query timeouts Certificate validation delays Network latency between components Resolution: I optimized several timeout values:\nRADIUS Server Timeout: Adjusted to 10 seconds on the switch LDAP Connection Pooling: Enabled persistent LDAP connections Certificate Caching: Configured client-side certificate caching Advanced Troubleshooting Techniques Throughout this implementation, I developed several troubleshooting methodologies that proved invaluable:\nPacket Capture Analysis I used tcpdump on the pfSense firewall to capture and analyze RADIUS traffic:\ntcpdump -i em0 -s0 -w radius_capture.pcap udp port 1812 This approach allowed me to verify that RADIUS packets were reaching the server and to analyze the timing of authentication flows.\nLDAP Testing To validate LDAP connectivity and search filters, I used ldapsearch directly from the FreeRADIUS host:\nldapsearch -x -H ldaps://172.30.30.30:636 \\ -D \u0026#34;CN=ldapbind,CN=Users,DC=tillynet,DC=lan\u0026#34; \\ -W -b \u0026#34;CN=Computers,DC=tillynet,DC=lan\u0026#34; \\ \u0026#34;(\u0026amp;(objectClass=computer)(sAMAccountName=TILLYPC$))\u0026#34; This command helped verify that the LDAP filter was correctly matching computer accounts.\nCertificate Validation Testing I used OpenSSL to test certificate chain validation:\nopenssl verify -CAfile ca-chain.crt radius.tillynet.lan.crt This command confirmed that the certificate chain was properly constructed and that all certificates were valid.\nWindows Event Log Analysis On the Windows client, I monitored several event logs:\nSystem Log: For 802.1X authentication events Security Log: For authentication and authorization events Applications and Services Logs → Microsoft → Windows → Wired-AutoConfig: For detailed 802.1X debugging Performance Optimization and Best Practices Based on my implementation experience, I identified several optimization opportunities:\nConnection Pooling I configured FreeRADIUS to maintain persistent connections to the LDAP server, reducing authentication latency and improving reliability.\nCertificate Caching I enabled certificate caching on both the server and client sides to reduce certificate validation overhead during subsequent authentications.\nMonitoring and Alerting I implemented monitoring for:\nRADIUS authentication success/failure rates LDAP server connectivity Certificate expiration dates Switch port authentication status Security Hardening I applied several security hardening measures:\nRestricted RADIUS communication to specific VLANs Implemented certificate revocation checking Configured detailed audit logging Applied the principle of least privilege to service accounts Lessons Learned and Best Practices This implementation provided several key insights that would be valuable for enterprise deployments:\nCertificate Management Automate Certificate Lifecycle: In production environments, implement automated certificate enrollment and renewal Certificate Templates: Use properly configured certificate templates that include the necessary EKUs and SANs Certificate Monitoring: Implement proactive monitoring for certificate expiration LDAP Integration Service Account Security: Use dedicated service accounts with minimal required permissions for LDAP binds Connection Security: Always use LDAPS (LDAP over TLS) for authentication traffic Search Optimization: Design LDAP searches to be as specific as possible to reduce directory server load Network Design VLAN Segmentation: Implement proper VLAN segmentation to isolate authentication traffic Redundancy: Deploy multiple RADIUS servers for high availability Monitoring: Implement comprehensive monitoring of the authentication infrastructure Troubleshooting Methodology Layered Approach: Troubleshoot authentication issues by examining each layer (network, RADIUS, LDAP, certificates) Comprehensive Logging: Enable detailed logging at all layers during initial deployment Test Automation: Develop automated tests to validate authentication functionality Future Enhancements This implementation serves as a foundation for several potential enhancements:\nDynamic VLAN Assignment I plan to implement dynamic VLAN assignment based on computer group membership, allowing for automatic network segmentation based on device classification.\nCertificate Lifecycle Management I\u0026rsquo;m working on implementing automated certificate enrollment and renewal using AD Certificate Services templates and Group Policy.\nMonitoring and Analytics I\u0026rsquo;m developing a comprehensive monitoring solution that will track authentication patterns, identify potential security issues, and provide detailed reporting on network access.\nIntegration with Network Access Control I\u0026rsquo;m exploring integration with additional NAC solutions to provide more granular access control based on device health, compliance status, and user context.\nConclusion Implementing 802.1X EAP-TLS authentication in my home lab provided invaluable hands-on experience with enterprise network security technologies. The process involved overcoming numerous technical challenges, from certificate infrastructure design to complex LDAP integration issues.\nThe key takeaways from this implementation include:\nCertificate Infrastructure is Critical: A well-designed PKI is essential for EAP-TLS success. Proper certificate templates, chain validation, and lifecycle management are crucial.\nLDAP Integration Complexity: Integrating FreeRADIUS with Active Directory for computer authentication requires deep understanding of Windows authentication mechanisms and LDAP schema.\nTroubleshooting Methodology: Systematic troubleshooting using debug logs, packet captures, and component testing is essential for identifying and resolving authentication issues.\nSecurity and Performance Balance: Implementing strong security while maintaining good performance requires careful optimization of timeouts, connection pooling, and caching mechanisms.\nThis implementation now serves as a robust foundation for exploring advanced network access control concepts and provides a realistic environment for testing enterprise security scenarios. The experience gained from building this system from the ground up has significantly enhanced my understanding of enterprise network security architecture and the complex interactions between its various components.\n","permalink":"https://blog.tillynet.com/on-premise-engineering-labs/implementing-enterprise-grade-802.1x-eap-tls-authentication-in-my-home-lab/","summary":"\u003ch1 id=\"implementing-enterprise-grade-8021x-eap-tls-authentication-in-my-home-lab\"\u003eImplementing Enterprise-Grade 802.1X EAP-TLS Authentication in My Home Lab\u003c/h1\u003e\n\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eNetwork Access Control (NAC) has become a cornerstone of enterprise security, ensuring that only authorized devices can connect to corporate networks. While technologies like 802.1X are commonplace in enterprise environments, implementing them in a home lab presents unique challenges and learning opportunities. In this article, I\u0026rsquo;ll walk through my complete implementation of 802.1X EAP-TLS authentication using FreeRADIUS, Samba4 Active Directory, and Cisco Catalyst switches.\u003c/p\u003e","title":"Implementing Enterprise-Grade 802.1X EAP-TLS Authentication in My Home Lab"},{"content":"Overview This post documents my journey building a production-grade hybrid cloud infrastructure that securely connects my on-premises homelab environment with AWS. The implementation demonstrates enterprise-level network segmentation, DNS integration, and security practices using Infrastructure as Code (IaC) principles.\nArchitecture Goals My primary objectives were to:\nEstablish secure connectivity between on-premises and AWS environments Implement seamless DNS resolution across both environments Maintain security isolation while enabling necessary communication Use Infrastructure as Code for reproducible deployments Follow enterprise best practices for hybrid cloud architectures On-Premises Infrastructure Overview My existing homelab infrastructure includes:\nSamba4 Active Directory Domain Controller (172.30.30.30) - Authoritative DNS for tillynet.lan Pi-hole DNS Server (172.21.21.21) - Recursive DNS with ad-blocking pfSense Firewall/Router - Network segmentation and routing VLAN Segmentation: VLAN 1: Default/legacy (172.16.7.0/24) VLAN 14: Guest Wi-Fi (172.16.14.0/24) VLAN 21: Production (172.21.21.0/24) VLAN 99: Management (172.16.99.0/24) Infrastructure subnet: (172.30.30.0/24) DNS Hierarchy My DNS architecture follows enterprise patterns:\nClient Queries → Samba4 AD DC (Authoritative) → Pi-hole (Recursive) → External DNS This design provides centralized domain management while maintaining ad-blocking and filtering capabilities.\nAWS Infrastructure Design Network Architecture I designed the AWS VPC to avoid IP conflicts with my on-premises networks:\n# locals.tf locals { # AWS VPC CIDR - ensuring no overlap with home networks vpc_cidr = \u0026#34;10.1.0.0/16\u0026#34; # Subnet CIDRs public_subnet_cidr = \u0026#34;10.1.1.0/24\u0026#34; private_subnet_cidr = \u0026#34;10.1.2.0/24\u0026#34; # My home network CIDRs home_networks = [ \u0026#34;172.16.7.0/24\u0026#34;, # Default/legacy VLAN 1 \u0026#34;172.16.14.0/24\u0026#34;, # Guest Wi-Fi VLAN 14 \u0026#34;172.21.21.0/24\u0026#34;, # Production VLAN 21 \u0026#34;172.16.99.0/24\u0026#34;, # Management VLAN 99 \u0026#34;172.30.30.0/24\u0026#34; # Internal Infrastructure Subnet ] } VPC and Core Infrastructure # main.tf # VPC for hybrid connectivity resource \u0026#34;aws_vpc\u0026#34; \u0026#34;hybrid_vpc\u0026#34; { cidr_block = local.vpc_cidr enable_dns_hostnames = true enable_dns_support = true tags = merge(local.common_tags, { Name = \u0026#34;${var.project_name}-hybrid-vpc\u0026#34; }) } # Public Subnet (for NAT Gateway, bastion, etc.) resource \u0026#34;aws_subnet\u0026#34; \u0026#34;public_subnet\u0026#34; { vpc_id = aws_vpc.hybrid_vpc.id cidr_block = local.public_subnet_cidr availability_zone = data.aws_availability_zones.available.names[0] map_public_ip_on_launch = true tags = merge(local.common_tags, { Name = \u0026#34;${var.project_name}-public-subnet\u0026#34; Type = \u0026#34;Public\u0026#34; }) } # Private Subnet (for hybrid workloads) resource \u0026#34;aws_subnet\u0026#34; \u0026#34;private_subnet\u0026#34; { vpc_id = aws_vpc.hybrid_vpc.id cidr_block = local.private_subnet_cidr availability_zone = data.aws_availability_zones.available.names[0] tags = merge(local.common_tags, { Name = \u0026#34;${var.project_name}-private-subnet\u0026#34; Type = \u0026#34;Private\u0026#34; }) } This architecture provides:\nNetwork isolation between public and private resources NAT Gateway for outbound internet access from private subnet Route table separation for granular traffic control VPN Connectivity Implementation Customer Gateway and VPN Connection # Customer Gateway (represents my pfSense firewall) resource \u0026#34;aws_customer_gateway\u0026#34; \u0026#34;tillynet_cgw\u0026#34; { bgp_asn = 65000 # Standard private ASN ip_address = var.home_public_ip type = \u0026#34;ipsec.1\u0026#34; tags = merge(local.common_tags, { Name = \u0026#34;${var.project_name}-tillynet-gateway\u0026#34; }) } # Virtual Private Gateway resource \u0026#34;aws_vpn_gateway\u0026#34; \u0026#34;hybrid_vgw\u0026#34; { vpc_id = aws_vpc.hybrid_vpc.id tags = merge(local.common_tags, { Name = \u0026#34;${var.project_name}-vpn-gateway\u0026#34; }) } # VPN Connection resource \u0026#34;aws_vpn_connection\u0026#34; \u0026#34;tillynet_vpn_enhanced\u0026#34; { customer_gateway_id = aws_customer_gateway.tillynet_cgw.id vpn_gateway_id = aws_vpn_gateway.hybrid_vgw.id type = \u0026#34;ipsec.1\u0026#34; static_routes_only = true # Enhanced tunnel options with correct AWS values tunnel1_ike_versions = [\u0026#34;ikev2\u0026#34;] tunnel1_phase1_encryption_algorithms = [\u0026#34;AES256\u0026#34;] tunnel1_phase1_integrity_algorithms = [\u0026#34;SHA2-256\u0026#34;] tunnel1_phase1_dh_group_numbers = [14] tunnel1_phase2_encryption_algorithms = [\u0026#34;AES256\u0026#34;] tunnel1_phase2_integrity_algorithms = [\u0026#34;SHA2-256\u0026#34;] tunnel1_phase2_dh_group_numbers = [14] tunnel2_ike_versions = [\u0026#34;ikev2\u0026#34;] tunnel2_phase1_encryption_algorithms = [\u0026#34;AES256\u0026#34;] tunnel2_phase1_integrity_algorithms = [\u0026#34;SHA2-256\u0026#34;] tunnel2_phase1_dh_group_numbers = [14] tunnel2_phase2_encryption_algorithms = [\u0026#34;AES256\u0026#34;] tunnel2_phase2_integrity_algorithms = [\u0026#34;SHA2-256\u0026#34;] tunnel2_phase2_dh_group_numbers = [14] tags = merge(local.common_tags, { Name = \u0026#34;${var.project_name}-vpn-connection-enhanced\u0026#34; }) } VPN Route Configuration # VPN Connection Routes (for my home networks) resource \u0026#34;aws_vpn_connection_route\u0026#34; \u0026#34;home_routes_enhanced\u0026#34; { count = length(local.home_networks) vpn_connection_id = aws_vpn_connection.tillynet_vpn_enhanced.id destination_cidr_block = local.home_networks[count.index] } # Propagate VPN routes to private route table resource \u0026#34;aws_vpn_gateway_route_propagation\u0026#34; \u0026#34;private_propagation_enhanced\u0026#34; { vpn_gateway_id = aws_vpn_gateway.hybrid_vgw.id route_table_id = aws_route_table.private_rt.id depends_on = [aws_vpn_connection.tillynet_vpn_enhanced] } This configuration automatically advertises all my on-premises networks to AWS and enables route propagation for seamless connectivity.\nDNS Infrastructure Design Route 53 Resolver Endpoints To enable bi-directional DNS resolution, I implemented Route 53 Resolver endpoints:\n# dns.tf # Route 53 Resolver Inbound Endpoint (for on-prem to AWS queries) resource \u0026#34;aws_route53_resolver_endpoint\u0026#34; \u0026#34;inbound\u0026#34; { name = \u0026#34;${var.project_name}-inbound-resolver\u0026#34; direction = \u0026#34;INBOUND\u0026#34; security_group_ids = [aws_security_group.dns_resolver_sg.id] ip_address { subnet_id = aws_subnet.private_subnet.id ip = \u0026#34;10.1.2.10\u0026#34; } ip_address { subnet_id = aws_subnet.public_subnet.id ip = \u0026#34;10.1.1.10\u0026#34; } tags = merge(local.common_tags, { Name = \u0026#34;${var.project_name}-inbound-resolver\u0026#34; }) } # Route 53 Resolver Outbound Endpoint (for AWS to on-prem queries) resource \u0026#34;aws_route53_resolver_endpoint\u0026#34; \u0026#34;outbound\u0026#34; { name = \u0026#34;${var.project_name}-outbound-resolver\u0026#34; direction = \u0026#34;OUTBOUND\u0026#34; security_group_ids = [aws_security_group.dns_resolver_sg.id] ip_address { subnet_id = aws_subnet.private_subnet.id ip = \u0026#34;10.1.2.11\u0026#34; } ip_address { subnet_id = aws_subnet.public_subnet.id ip = \u0026#34;10.1.1.11\u0026#34; } tags = merge(local.common_tags, { Name = \u0026#34;${var.project_name}-outbound-resolver\u0026#34; }) } Forward Resolution Rules # Forwarding rule to send tillynet.lan queries to my Samba4 DC resource \u0026#34;aws_route53_resolver_rule\u0026#34; \u0026#34;onprem_forward\u0026#34; { domain_name = \u0026#34;tillynet.lan\u0026#34; name = \u0026#34;${var.project_name}-onprem-forward\u0026#34; rule_type = \u0026#34;FORWARD\u0026#34; resolver_endpoint_id = aws_route53_resolver_endpoint.outbound.id target_ip { ip = \u0026#34;172.30.30.30\u0026#34; # my Samba4 DC IP port = 53 } tags = merge(local.common_tags, { Name = \u0026#34;${var.project_name}-onprem-forward\u0026#34; }) } This enables AWS instances to resolve my on-premises domain names.\nPrivate Hosted Zone # Private hosted zone for aws.tillynet.lan resource \u0026#34;aws_route53_zone\u0026#34; \u0026#34;aws_private\u0026#34; { name = \u0026#34;aws.tillynet.lan\u0026#34; vpc { vpc_id = aws_vpc.hybrid_vpc.id } tags = merge(local.common_tags, { Name = \u0026#34;${var.project_name}-aws-private-zone\u0026#34; }) } Security Implementation Security Groups I implemented least-privilege security groups for each component:\n# DNS Resolver Security Group resource \u0026#34;aws_security_group\u0026#34; \u0026#34;dns_resolver_sg\u0026#34; { name_prefix = \u0026#34;${var.project_name}-dns-resolver-\u0026#34; description = \u0026#34;Security group for Route 53 Resolver endpoints\u0026#34; vpc_id = aws_vpc.hybrid_vpc.id ingress { description = \u0026#34;DNS from on-premises networks\u0026#34; from_port = 53 to_port = 53 protocol = \u0026#34;udp\u0026#34; cidr_blocks = local.home_networks } ingress { description = \u0026#34;DNS TCP from on-premises networks\u0026#34; from_port = 53 to_port = 53 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = local.home_networks } egress { description = \u0026#34;DNS to on-premises\u0026#34; from_port = 53 to_port = 53 protocol = \u0026#34;udp\u0026#34; cidr_blocks = local.home_networks } tags = merge(local.common_tags, { Name = \u0026#34;${var.project_name}-dns-resolver-sg\u0026#34; }) } # Private Instance Security Group resource \u0026#34;aws_security_group\u0026#34; \u0026#34;private_sg\u0026#34; { name_prefix = \u0026#34;${var.project_name}-private-\u0026#34; description = \u0026#34;Security group for private instances\u0026#34; vpc_id = aws_vpc.hybrid_vpc.id ingress { description = \u0026#34;SSH from home networks\u0026#34; from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = local.home_networks } ingress { description = \u0026#34;HTTPS from home networks\u0026#34; from_port = 443 to_port = 443 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = local.home_networks } egress { description = \u0026#34;All outbound\u0026#34; from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = merge(local.common_tags, { Name = \u0026#34;${var.project_name}-private-sg\u0026#34; }) } Bastion Host Security The bastion host implements a critical security pattern:\n# Bastion Host Security Group resource \u0026#34;aws_security_group\u0026#34; \u0026#34;bastion_sg\u0026#34; { name_prefix = \u0026#34;${var.project_name}-bastion-\u0026#34; description = \u0026#34;Security group for bastion host\u0026#34; vpc_id = aws_vpc.hybrid_vpc.id ingress { description = \u0026#34;SSH from internet (backup access)\u0026#34; from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;${var.home_public_ip}/32\u0026#34;] } # Note: SSH from on-premises networks is intentionally blocked # This forces proper access patterns for security egress { description = \u0026#34;All outbound\u0026#34; from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = merge(local.common_tags, { Name = \u0026#34;${var.project_name}-bastion-sg\u0026#34; }) } This design enforces different access patterns:\nBastion host: Internet → AWS entry point only Private instances: On-premises ↔ AWS communication No lateral movement from on-premises to bastion EC2 Instance Deployment IAM Roles and Instance Profiles # IAM role for EC2 instances resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;ec2_hybrid_role\u0026#34; { name = \u0026#34;${var.project_name}-ec2-hybrid-role\u0026#34; assume_role_policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [ { Action = \u0026#34;sts:AssumeRole\u0026#34; Effect = \u0026#34;Allow\u0026#34; Principal = { Service = \u0026#34;ec2.amazonaws.com\u0026#34; } } ] }) tags = local.common_tags } # IAM policy for EC2 instances resource \u0026#34;aws_iam_role_policy\u0026#34; \u0026#34;ec2_hybrid_policy\u0026#34; { name = \u0026#34;${var.project_name}-ec2-hybrid-policy\u0026#34; role = aws_iam_role.ec2_hybrid_role.id policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [ { Effect = \u0026#34;Allow\u0026#34; Action = [ \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ChangeResourceRecordSets\u0026#34;, \u0026#34;route53:GetChange\u0026#34; ] Resource = \u0026#34;*\u0026#34; } ] }) } Instance Configuration # Private Instance for hybrid workloads resource \u0026#34;aws_instance\u0026#34; \u0026#34;hybrid_app\u0026#34; { ami = data.aws_ami.amazon_linux.id instance_type = \u0026#34;t3.micro\u0026#34; key_name = var.key_pair_name subnet_id = aws_subnet.private_subnet.id vpc_security_group_ids = [aws_security_group.private_sg.id] iam_instance_profile = aws_iam_instance_profile.ec2_hybrid_profile.name user_data = base64encode(templatefile(\u0026#34;${path.module}/user-data/app-userdata.sh\u0026#34;, { hostname = \u0026#34;app01\u0026#34; project = var.project_name environment = \u0026#34;hybrid\u0026#34; samba_dc_ip = \u0026#34;172.30.30.30\u0026#34; dns_resolver_ip = \u0026#34;10.1.2.10\u0026#34; })) tags = merge(local.common_tags, { Name = \u0026#34;${var.project_name}-app01\u0026#34; Role = \u0026#34;Application\u0026#34; }) } DNS Integration Challenges and Solutions Initial Approach: DNS Delegation My first approach attempted to use DNS delegation from Samba4 to AWS Route 53:\n# Create AWS subdomain zone sudo samba-tool dns zonecreate 172.30.30.30 aws.tillynet.lan -U Administrator # Add NS record for delegation sudo samba-tool dns add 172.30.30.30 tillynet.lan aws NS aws-resolver.tillynet.lan -U Administrator # Add glue record sudo samba-tool dns add 172.30.30.30 tillynet.lan aws-resolver A 10.1.2.10 -U Administrator Problems Encountered Resolver Connectivity Issues\n# Testing both resolver IPs nslookup app01.aws.tillynet.lan 10.1.1.10 # Timeout nslookup app01.aws.tillynet.lan 10.1.2.10 # Success By design only the private subnet resolver (10.1.2.10) was reachable from my on-prem networks since traffic between my on-prem networks and aws-public-subnets was not permitted.\nSamba4 Delegation Behavior\nSamba4 doesn\u0026rsquo;t follow NS delegation for subdomains like traditional DNS servers. Even with correct delegation setup, queries weren\u0026rsquo;t being forwarded properly.\nConflicting NS Records\nHaving both samba.tillynet.lan and aws-resolver.tillynet.lan as nameservers created resolution conflicts.\nFinal Solution: Direct Zone Management I resolved the issues by making Samba4 directly authoritative for the AWS subdomain:\n# Remove delegation records sudo samba-tool dns delete 172.30.30.30 tillynet.lan aws NS aws-resolver.tillynet.lan -U Administrator sudo samba-tool dns delete 172.30.30.30 tillynet.lan aws-resolver A 10.1.2.10 -U Administrator # Add records directly to the aws zone sudo samba-tool dns add 172.30.30.30 aws.tillynet.lan app01 A 10.1.2.76 -U Administrator sudo samba-tool dns add 172.30.30.30 aws.tillynet.lan bastion A 10.1.1.101 -U Administrator This approach provides:\nImmediate resolution without delegation complexity Central management of all DNS records Better performance (no network hops to AWS) Simplified troubleshooting Connectivity Testing and Validation VPN Tunnel Verification # Test connectivity using allowed ports (SSH) ssh 10.1.2.76 # Success - shows VPN is working DNS Resolution Testing # Test from on-premises nslookup app01.aws.tillynet.lan 172.30.30.30 # Returns: 10.1.2.76 # Test SSH using DNS names ssh -i keypair.pem ec2-user@app01.aws.tillynet.lan # Success! Cross-Environment Access Patterns From On-Premises to AWS:\nPrivate instances: Direct SSH via VPN tunnel Bastion host: SSH from internet only (security best practice) From AWS to On-Premises:\nAll on-premises services accessible via VPN DNS resolution works bi-directionally Security Best Practices Implemented Network Segmentation VPC isolation with no overlapping IP ranges Security groups implementing least-privilege access Subnet separation between public and private resources Access Control Bastion host restricted to internet access only Private instances accessible from on-premises networks IAM roles instead of embedded credentials Monitoring and Logging VPC Flow Logs for traffic analysis CloudTrail for API auditing Security group logging for access attempts Automation and Infrastructure as Code The entire infrastructure is managed through Terraform, providing:\nVariable Configuration # terraform.tfvars aws_region = \u0026#34;us-west-1\u0026#34; home_public_ip = \u0026#34;XXX.XXX.XXX.XXX\u0026#34; project_name = \u0026#34;tillynet-hybrid\u0026#34; key_pair_name = \u0026#34;tillynet-aws-keypair-general\u0026#34; Modular Design Separate files for different components (dns.tf, security-groups.tf, instances.tf) Reusable variables and locals Consistent tagging strategy Deployment Validation terraform validate terraform plan terraform apply Lessons Learned DNS Delegation Complexity Challenge: Samba4 doesn\u0026rsquo;t handle DNS delegation like traditional DNS servers. Solution: Direct zone management is often simpler and more reliable for hybrid environments.\nSecurity Group Design Challenge: Balancing security with functionality. Solution: Implement different access patterns for different resource types (bastion vs. private instances).\nRoute 53 Resolver Placement Challenge: Not all resolver endpoints may be reachable. Solution: Test connectivity to all endpoints and use only working ones.\nPerformance and Cost Considerations Network Performance VPN tunnel latency: ~75-85ms (acceptable for hybrid operations) DNS resolution: Sub-second response times Throughput: Sufficient for management and application traffic Cost Optimization t3.micro instances: Free tier eligible NAT Gateway: ~$32/month (necessary for private subnet internet access) VPN Connection: ~$36/month (fixed cost) Route 53 Resolver: ~$0.125/hour per endpoint Future Enhancements Planned Improvements Certificate Management: Extend on-premises PKI to AWS services Service Mesh: Implement secure service-to-service communication Monitoring Integration: Centralized logging and monitoring Automation: Ansible playbooks for configuration management Scalability Considerations Multi-AZ deployment for high availability Auto Scaling Groups for dynamic capacity Load balancers for service distribution Container orchestration for microservices Conclusion This hybrid infrastructure implementation demonstrates enterprise-grade practices for securely connecting on-premises and cloud environments. The combination of proper network segmentation, security controls, and DNS integration provides a solid foundation for hybrid cloud operations.\nKey achievements:\nSecure VPN connectivity between environments Seamless DNS resolution across hybrid infrastructure Defense-in-depth security with multiple isolation layers Infrastructure as Code for reproducible deployments Scalable architecture ready for service expansion The pragmatic approach of using direct DNS zone management over complex delegation proved that sometimes the simplest solution that works is better than a theoretically perfect solution that doesn\u0026rsquo;t. This infrastructure now serves as the foundation for deploying hybrid applications and services across both environments.\nTechnical Specifications Infrastructure Components:\nAWS VPC with public/private subnets Site-to-site VPN with static routing Route 53 Resolver endpoints for DNS EC2 instances with IAM roles Security groups with least-privilege access Network Architecture:\nOn-premises: Multiple VLANs (172.16.x.x/24, 172.21.21.0/24, 172.30.30.0/24) AWS: 10.1.0.0/16 with /24 subnets VPN: IPsec with AES256 encryption DNS Architecture:\nSamba4 AD DC: Authoritative for tillynet.lan and aws.tillynet.lan Pi-hole: Recursive resolver with ad-blocking Route 53: AWS service resolution and forwarding ","permalink":"https://blog.tillynet.com/cloud-engineering-labs/aws-lab-4/building-a-secure-hybrid-cloud-infrastructure-with-aws-vpn-and-dns-integration/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eThis post documents my journey building a production-grade hybrid cloud infrastructure that securely connects my on-premises homelab environment with AWS. The implementation demonstrates enterprise-level network segmentation, DNS integration, and security practices using Infrastructure as Code (IaC) principles.\u003c/p\u003e\n\u003ch2 id=\"architecture-goals\"\u003eArchitecture Goals\u003c/h2\u003e\n\u003cp\u003eMy primary objectives were to:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eEstablish secure connectivity\u003c/strong\u003e between on-premises and AWS environments\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eImplement seamless DNS resolution\u003c/strong\u003e across both environments\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMaintain security isolation\u003c/strong\u003e while enabling necessary communication\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eUse Infrastructure as Code\u003c/strong\u003e for reproducible deployments\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eFollow enterprise best practices\u003c/strong\u003e for hybrid cloud architectures\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"on-premises-infrastructure-overview\"\u003eOn-Premises Infrastructure Overview\u003c/h2\u003e\n\u003cp\u003eMy existing homelab infrastructure includes:\u003c/p\u003e","title":"Building a Secure Hybrid Cloud Infrastructure with AWS VPN and DNS Integration"},{"content":"In this post, I\u0026rsquo;ll walk through the complete process of establishing a Site-to-Site VPN connection between my on-premise pfSense firewall and AWS, creating a true hybrid cloud environment. This project demonstrates enterprise-grade network integration, secure tunnel establishment, and Infrastructure as Code principles.\nProject Overview The goal was to extend my existing home lab network (TillyNet) to AWS, enabling seamless communication between on-premise resources and cloud workloads. This creates opportunities for hybrid applications, cloud bursting, and distributed infrastructure management.\nArchitecture Summary On-Premise Network (TillyNet) AWS Cloud Environment\r┌─────────────────────────────┐ ┌─────────────────────────────┐\r│ pfSense Firewall │ │ AWS VPC (10.1.0.0/16) │\r│ Multiple VLANs: │◄────────┤ │\r│ - VLAN 1: 172.16.7.0/24 │ IPsec │ - Public: 10.1.1.0/24 │\r│ - VLAN 14: 172.16.14.0/24 │ Tunnel │ - Private: 10.1.2.0/24 │\r│ - VLAN 21: 172.21.21.0/24 │ │ │\r│ - VLAN 99: 172.16.99.0/24 │ │ EC2 Instances │\r│ - XPS Subnet: 172.30.30.0/24│ │ NAT Gateway │\r└─────────────────────────────┘ └─────────────────────────────┘ Prerequisites On-Premise Requirements: pfSense firewall with public IP address Properly configured VLANs and routing Administrative access to firewall configuration AWS Requirements: AWS account with appropriate IAM permissions AWS CLI configured with access keys Terraform installed locally Network Requirements: Non-overlapping IP address spaces Static public IP address (recommended) Understanding of IPsec protocols Phase 1: AWS Infrastructure Deployment The first phase involves creating the AWS networking infrastructure using Terraform. This approach ensures reproducible, version-controlled infrastructure that can be easily modified and redeployed.\nTerraform Project Structure I organized the Terraform code into logical files for maintainability:\ntillynet-aws-hybrid-deployment/\r├── main.tf # Primary resource definitions\r├── variables.tf # Input variable declarations\r├── outputs.tf # Output value definitions\r├── locals.tf # Local value computations\r├── data.tf # Data source definitions\r├── terraform.tfvars # Variable value assignments\r└── README.md # Project documentation Variable Definitions The variables.tf file defines all configurable parameters for the deployment:\n# variables.tf - Input parameter definitions variable \u0026#34;aws_region\u0026#34; { description = \u0026#34;AWS region for VPC deployment\u0026#34; type = string default = \u0026#34;us-west-1\u0026#34; validation { condition = can(regex(\u0026#34;^[a-z]{2}-[a-z]+-[0-9]$\u0026#34;, var.aws_region)) error_message = \u0026#34;AWS region must be in valid format (e.g., us-west-1).\u0026#34; } } variable \u0026#34;home_public_ip\u0026#34; { description = \u0026#34;Public IP address of on-premise network\u0026#34; type = string validation { condition = can(cidrhost(\u0026#34;${var.home_public_ip}/32\u0026#34;, 0)) error_message = \u0026#34;Must be a valid IPv4 address.\u0026#34; } } variable \u0026#34;project_name\u0026#34; { description = \u0026#34;Project identifier for resource naming and tagging\u0026#34; type = string default = \u0026#34;tillynet-hybrid\u0026#34; validation { condition = can(regex(\u0026#34;^[a-zA-Z][a-zA-Z0-9-]*$\u0026#34;, var.project_name)) error_message = \u0026#34;Project name must start with letter and contain only alphanumeric characters and hyphens.\u0026#34; } } variable \u0026#34;vpc_cidr\u0026#34; { description = \u0026#34;CIDR block for AWS VPC\u0026#34; type = string default = \u0026#34;10.1.0.0/16\u0026#34; validation { condition = can(cidrhost(var.vpc_cidr, 0)) error_message = \u0026#34;VPC CIDR must be a valid IPv4 CIDR block.\u0026#34; } } variable \u0026#34;public_subnet_cidr\u0026#34; { description = \u0026#34;CIDR block for public subnet\u0026#34; type = string default = \u0026#34;10.1.1.0/24\u0026#34; } variable \u0026#34;private_subnet_cidr\u0026#34; { description = \u0026#34;CIDR block for private subnet\u0026#34; type = string default = \u0026#34;10.1.2.0/24\u0026#34; } The validation blocks ensure that input values meet expected formats and constraints, preventing common configuration errors during deployment.\nLocal Values and Data Sources The locals.tf file computes reusable values and establishes naming conventions:\n# locals.tf - Computed values and constants locals { # Standardized resource tagging for cost allocation and management common_tags = { Project = var.project_name Environment = \u0026#34;hybrid\u0026#34; ManagedBy = \u0026#34;Terraform\u0026#34; CreatedAt = timestamp() Purpose = \u0026#34;Hybrid-Cloud-Connectivity\u0026#34; } # Consistent naming convention across all resources name_prefix = \u0026#34;${var.project_name}-hybrid\u0026#34; # On-premise network definitions for VPN routing # These represent the existing VLAN structure in my home lab home_networks = [ \u0026#34;172.16.7.0/24\u0026#34;, # Default/Legacy VLAN \u0026#34;172.16.14.0/24\u0026#34;, # Guest Wi-Fi Network \u0026#34;172.21.21.0/24\u0026#34;, # Production Services (DNS, etc.) \u0026#34;172.16.99.0/24\u0026#34;, # Management Network \u0026#34;172.30.30.0/24\u0026#34; # Proxmox XPS Subnet ] } The data.tf file queries AWS for dynamic information:\n# data.tf - External data source queries # Current AWS region information data \u0026#34;aws_region\u0026#34; \u0026#34;current\u0026#34; {} # AWS account identity for resource ARN construction data \u0026#34;aws_caller_identity\u0026#34; \u0026#34;current\u0026#34; {} # Available Availability Zones in the selected region data \u0026#34;aws_availability_zones\u0026#34; \u0026#34;available\u0026#34; { state = \u0026#34;available\u0026#34; } # Latest Amazon Linux 2 AMI for EC2 instances data \u0026#34;aws_ami\u0026#34; \u0026#34;amazon_linux\u0026#34; { most_recent = true owners = [\u0026#34;amazon\u0026#34;] filter { name = \u0026#34;name\u0026#34; values = [\u0026#34;amzn2-ami-hvm-*-x86_64-gp2\u0026#34;] } filter { name = \u0026#34;virtualization-type\u0026#34; values = [\u0026#34;hvm\u0026#34;] } } Core Network Infrastructure The main.tf file contains the primary infrastructure definitions. Here\u0026rsquo;s the VPC and networking setup:\n# main.tf - Primary infrastructure resources # Provider configuration terraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; } } } provider \u0026#34;aws\u0026#34; { region = var.aws_region } # Virtual Private Cloud - The foundation of our AWS network resource \u0026#34;aws_vpc\u0026#34; \u0026#34;hybrid_vpc\u0026#34; { cidr_block = var.vpc_cidr enable_dns_hostnames = true # Required for proper hostname resolution enable_dns_support = true # Enables DNS resolution within VPC tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-vpc\u0026#34; }) } # Internet Gateway - Provides internet access to public subnets resource \u0026#34;aws_internet_gateway\u0026#34; \u0026#34;hybrid_igw\u0026#34; { vpc_id = aws_vpc.hybrid_vpc.id tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-igw\u0026#34; }) } # Public Subnet - Hosts NAT Gateway and potential bastion hosts resource \u0026#34;aws_subnet\u0026#34; \u0026#34;public_subnet\u0026#34; { vpc_id = aws_vpc.hybrid_vpc.id cidr_block = var.public_subnet_cidr availability_zone = data.aws_availability_zones.available.names[0] map_public_ip_on_launch = true # Auto-assign public IPs to instances tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-public-subnet\u0026#34; Type = \u0026#34;Public\u0026#34; }) } # Private Subnet - Hosts hybrid workloads accessible via VPN resource \u0026#34;aws_subnet\u0026#34; \u0026#34;private_subnet\u0026#34; { vpc_id = aws_vpc.hybrid_vpc.id cidr_block = var.private_subnet_cidr availability_zone = data.aws_availability_zones.available.names[0] tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-private-subnet\u0026#34; Type = \u0026#34;Private\u0026#34; }) } # Elastic IP for NAT Gateway - Provides static outbound IP resource \u0026#34;aws_eip\u0026#34; \u0026#34;nat_eip\u0026#34; { domain = \u0026#34;vpc\u0026#34; tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-nat-eip\u0026#34; }) depends_on = [aws_internet_gateway.hybrid_igw] } # NAT Gateway - Enables outbound internet access for private subnet resource \u0026#34;aws_nat_gateway\u0026#34; \u0026#34;hybrid_nat\u0026#34; { allocation_id = aws_eip.nat_eip.id subnet_id = aws_subnet.public_subnet.id tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-nat-gateway\u0026#34; }) depends_on = [aws_internet_gateway.hybrid_igw] } Routing Configuration Proper routing ensures traffic flows correctly between subnets and to the internet:\n# Public Route Table - Directs traffic to Internet Gateway resource \u0026#34;aws_route_table\u0026#34; \u0026#34;public_rt\u0026#34; { vpc_id = aws_vpc.hybrid_vpc.id # Default route to internet for public subnet route { cidr_block = \u0026#34;0.0.0.0/0\u0026#34; gateway_id = aws_internet_gateway.hybrid_igw.id } tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-public-rt\u0026#34; Type = \u0026#34;Public\u0026#34; }) } # Private Route Table - Directs traffic to NAT Gateway for internet access resource \u0026#34;aws_route_table\u0026#34; \u0026#34;private_rt\u0026#34; { vpc_id = aws_vpc.hybrid_vpc.id # Default route through NAT Gateway for outbound traffic route { cidr_block = \u0026#34;0.0.0.0/0\u0026#34; nat_gateway_id = aws_nat_gateway.hybrid_nat.id } tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-private-rt\u0026#34; Type = \u0026#34;Private\u0026#34; }) } # Route Table Associations - Link subnets to appropriate route tables resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;public_rta\u0026#34; { subnet_id = aws_subnet.public_subnet.id route_table_id = aws_route_table.public_rt.id } resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;private_rta\u0026#34; { subnet_id = aws_subnet.private_subnet.id route_table_id = aws_route_table.private_rt.id } VPN Infrastructure Components The Site-to-Site VPN requires three AWS components: Customer Gateway, Virtual Private Gateway, and the VPN Connection itself.\n# Customer Gateway - Represents the on-premise pfSense firewall resource \u0026#34;aws_customer_gateway\u0026#34; \u0026#34;tillynet_cgw\u0026#34; { bgp_asn = 65000 # Private ASN required by AWS (not used for static routing) ip_address = var.home_public_ip type = \u0026#34;ipsec.1\u0026#34; # Only supported VPN type tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-customer-gateway\u0026#34; }) } # Virtual Private Gateway - AWS-side VPN endpoint resource \u0026#34;aws_vpn_gateway\u0026#34; \u0026#34;hybrid_vgw\u0026#34; { vpc_id = aws_vpc.hybrid_vpc.id tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-vpn-gateway\u0026#34; }) } # Site-to-Site VPN Connection with enhanced security parameters resource \u0026#34;aws_vpn_connection\u0026#34; \u0026#34;tillynet_vpn\u0026#34; { customer_gateway_id = aws_customer_gateway.tillynet_cgw.id vpn_gateway_id = aws_vpn_gateway.hybrid_vgw.id type = \u0026#34;ipsec.1\u0026#34; static_routes_only = true # Use static routing instead of BGP # Enhanced tunnel options for improved security tunnel1_ike_versions = [\u0026#34;ikev2\u0026#34;] tunnel1_phase1_encryption_algorithms = [\u0026#34;AES256\u0026#34;] tunnel1_phase1_integrity_algorithms = [\u0026#34;SHA2-256\u0026#34;] tunnel1_phase1_dh_group_numbers = [14] tunnel1_phase2_encryption_algorithms = [\u0026#34;AES256\u0026#34;] tunnel1_phase2_integrity_algorithms = [\u0026#34;SHA2-256\u0026#34;] tunnel1_phase2_dh_group_numbers = [14] tunnel2_ike_versions = [\u0026#34;ikev2\u0026#34;] tunnel2_phase1_encryption_algorithms = [\u0026#34;AES256\u0026#34;] tunnel2_phase1_integrity_algorithms = [\u0026#34;SHA2-256\u0026#34;] tunnel2_phase1_dh_group_numbers = [14] tunnel2_phase2_encryption_algorithms = [\u0026#34;AES256\u0026#34;] tunnel2_phase2_integrity_algorithms = [\u0026#34;SHA2-256\u0026#34;] tunnel2_phase2_dh_group_numbers = [14] tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-vpn-connection\u0026#34; }) } # Static routes for on-premise networks resource \u0026#34;aws_vpn_connection_route\u0026#34; \u0026#34;home_routes\u0026#34; { count = length(local.home_networks) vpn_connection_id = aws_vpn_connection.tillynet_vpn.id destination_cidr_block = local.home_networks[count.index] } # Enable automatic route propagation to private route table resource \u0026#34;aws_vpn_gateway_route_propagation\u0026#34; \u0026#34;private_propagation\u0026#34; { vpn_gateway_id = aws_vpn_gateway.hybrid_vgw.id route_table_id = aws_route_table.private_rt.id } Test Instance for Connectivity Validation To validate the hybrid connection, I deployed a test EC2 instance with appropriate security configurations:\n# Security Group for test instance - Allows connectivity from on-premise networks resource \u0026#34;aws_security_group\u0026#34; \u0026#34;test_instance_sg\u0026#34; { name_prefix = \u0026#34;${local.name_prefix}-test-sg-\u0026#34; description = \u0026#34;Security group for hybrid connectivity testing\u0026#34; vpc_id = aws_vpc.hybrid_vpc.id # Allow ICMP (ping) from all on-premise networks ingress { description = \u0026#34;ICMP from on-premise networks\u0026#34; from_port = -1 to_port = -1 protocol = \u0026#34;icmp\u0026#34; cidr_blocks = local.home_networks } # Allow SSH access from on-premise networks ingress { description = \u0026#34;SSH from on-premise networks\u0026#34; from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = local.home_networks } # Allow all outbound traffic egress { description = \u0026#34;All outbound traffic\u0026#34; from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-test-sg\u0026#34; }) lifecycle { create_before_destroy = true } } # Test EC2 instance in private subnet resource \u0026#34;aws_instance\u0026#34; \u0026#34;test_instance\u0026#34; { ami = data.aws_ami.amazon_linux.id instance_type = \u0026#34;t2.micro\u0026#34; # Free tier eligible subnet_id = aws_subnet.private_subnet.id vpc_security_group_ids = [aws_security_group.test_instance_sg.id] # User data script for instance initialization user_data = base64encode(\u0026lt;\u0026lt;-EOF #!/bin/bash yum update -y yum install -y htop tree wget curl # Create identification file cat \u0026gt; /etc/motd \u0026lt;\u0026lt; \u0026#39;WELCOME\u0026#39; ************************************************* * AWS Hybrid Connectivity Test Instance * ************************************************* WELCOME echo \u0026#34;$(date): Hybrid test instance initialized\u0026#34; \u0026gt;\u0026gt; /var/log/hybrid-setup.log EOF ) tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-test-instance\u0026#34; Purpose = \u0026#34;Hybrid connectivity validation\u0026#34; }) } Output Definitions The outputs.tf file defines important values needed for pfSense configuration:\n# outputs.tf - Important values for manual configuration steps output \u0026#34;vpn_connection_details\u0026#34; { description = \u0026#34;VPN connection configuration parameters for pfSense\u0026#34; value = { vpn_connection_id = aws_vpn_connection.tillynet_vpn.id tunnel_1_address = aws_vpn_connection.tillynet_vpn.tunnel1_address tunnel_2_address = aws_vpn_connection.tillynet_vpn.tunnel2_address tunnel_1_psk = aws_vpn_connection.tillynet_vpn.tunnel1_preshared_key tunnel_2_psk = aws_vpn_connection.tillynet_vpn.tunnel2_preshared_key customer_gateway_ip = var.home_public_ip } sensitive = true # Hides sensitive values in console output } output \u0026#34;aws_vpc_info\u0026#34; { description = \u0026#34;AWS VPC infrastructure details\u0026#34; value = { vpc_id = aws_vpc.hybrid_vpc.id vpc_cidr = aws_vpc.hybrid_vpc.cidr_block public_subnet_id = aws_subnet.public_subnet.id private_subnet_id = aws_subnet.private_subnet.id vpn_gateway_id = aws_vpn_gateway.hybrid_vgw.id test_instance_ip = aws_instance.test_instance.private_ip } } output \u0026#34;connectivity_test_commands\u0026#34; { description = \u0026#34;Commands for testing hybrid connectivity\u0026#34; value = { ping_aws_from_home = \u0026#34;ping ${aws_instance.test_instance.private_ip}\u0026#34; ping_home_from_aws = \u0026#34;ping [your-home-network-ip]\u0026#34; ssh_to_aws = \u0026#34;ssh ec2-user@${aws_instance.test_instance.private_ip}\u0026#34; } } Variable Values Configuration The terraform.tfvars file contains environment-specific values:\n# terraform.tfvars - Environment-specific configuration values # Note: This file should not be committed to version control aws_region = \u0026#34;us-west-1\u0026#34; home_public_ip = \u0026#34;[YOUR_PUBLIC_IP_HERE]\u0026#34; # Replace with actual public IP project_name = \u0026#34;tillynet-hybrid\u0026#34; vpc_cidr = \u0026#34;10.1.0.0/16\u0026#34; public_subnet_cidr = \u0026#34;10.1.1.0/24\u0026#34; private_subnet_cidr = \u0026#34;10.1.2.0/24\u0026#34; Phase 2: AWS Infrastructure Deployment With the Terraform configuration complete, I deployed the AWS infrastructure:\nDeployment Process # Initialize Terraform and download required providers terraform init # Validate configuration syntax and logic terraform validate # Review planned changes before deployment terraform plan # Deploy the infrastructure terraform apply Deployment Verification After successful deployment, I verified the infrastructure:\n# Retrieve VPN connection details for pfSense configuration terraform output vpn_connection_details # Get AWS infrastructure information terraform output aws_vpc_info # Verify VPN connection status aws ec2 describe-vpn-connections --vpn-connection-ids [VPN_CONNECTION_ID] The deployment created:\nVPC with public and private subnets Internet Gateway and NAT Gateway for connectivity Virtual Private Gateway attached to the VPC Customer Gateway representing my pfSense firewall Site-to-Site VPN connection with two redundant tunnels Static routes for all on-premise networks Test EC2 instance with appropriate security groups Phase 3: pfSense IPsec Configuration With AWS infrastructure deployed, I configured the pfSense firewall to establish the IPsec tunnels.\nPhase 1 (IKE) Configuration For each tunnel, I created Phase 1 entries in pfSense:\nNavigation: VPN \u0026gt; IPsec \u0026gt; Add P1\nTunnel 1 Configuration: Disabled: Unchecked Key Exchange version: IKEv2 Internet Protocol: IPv4 Interface: WAN Remote Gateway: [AWS_TUNNEL_1_ADDRESS] Description: AWS VPN Tunnel 1 Authentication Settings: Authentication Method: Mutual PSK Negotiation Mode: Main My identifier: My IP address Peer identifier: Peer IP address Pre-Shared Key: [AWS_TUNNEL_1_PSK] Encryption Parameters: Encryption Algorithm: AES 256 bits Hash Algorithm: SHA2-256 DH Group: 14 (2048 bit) Lifetime: 28800 seconds Advanced Options: NAT Traversal: Auto Dead Peer Detection: Enabled Delay: 10 seconds Max failures: 3 Phase 2 (IPsec) Configuration For each Phase 1, I created corresponding Phase 2 entries:\nNavigation: VPN \u0026gt; IPsec \u0026gt; Show Phase 2 Entries \u0026gt; Add P2\nNetwork Configuration: Mode: Tunnel IPv4 Local Network: Network - 0.0.0.0/0 (any) Remote Network: Network - 10.1.0.0/16 (AWS VPC) Security Parameters: Protocol: ESP Encryption Algorithms: AES 256 bits Hash Algorithms: SHA2-256 PFS key group: 14 (2048 bit) Lifetime: 3600 seconds Advanced Configuration: Automatically ping host: 10.1.0.1 I replicated this configuration for Tunnel 2 with the appropriate endpoint address and pre-shared key.\nFirewall Rules Configuration IPsec Interface Rules Navigation: Firewall \u0026gt; Rules \u0026gt; IPsec\nI created a rule to allow all traffic through the VPN tunnels (this rule will later be more restrictive but for now this allowed testing traffic through the tunnel):\nAction: Pass Interface: IPsec Address Family: IPv4 Protocol: Any Source: Any Destination: Any Description: Allow all traffic through VPN tunnels VLAN Interface Rules For each VLAN interface, I added rules allowing traffic to AWS:\nNavigation: Firewall \u0026gt; Rules \u0026gt; [VLAN_Interface]\nExample for Management VLAN:\nAction: Pass Interface: MGMT Protocol: Any Source: MGMT net (172.16.99.0/24) Destination: 10.1.0.0/16 Description: Allow Management VLAN to AWS VPC Phase 4: Connection Establishment and Testing Tunnel Establishment After applying all configurations, I established the tunnels:\nStatus \u0026gt; IPsec \u0026gt; Overview Clicked \u0026ldquo;Connect P1 and P2s\u0026rdquo; for both tunnels Verified both tunnels showed \u0026ldquo;Established\u0026rdquo; status AWS-Side Verification I confirmed tunnel status from AWS:\n# Check tunnel status aws ec2 describe-vpn-connections --vpn-connection-ids [VPN_ID] \\ --query \u0026#39;VpnConnections[0].VgwTelemetry\u0026#39; Expected output showed Tunnel 1 as \u0026ldquo;UP\u0026rdquo; and Tunnel 2 as \u0026ldquo;DOWN\u0026rdquo; (normal for active/passive configuration).\nConnectivity Testing Test 1: AWS Infrastructure Ping # From pfSense Diagnostics \u0026gt; Ping ping [AWS_EC2_INSTANCE_IP] # Should succeed Test 2: Bidirectional Connectivity # From on-premise management network ping [AWS_EC2_INSTANCE_IP] # From AWS EC2 instance ping [ON_PREMISE_DEVICE_IP] Test 3: Application-Level Testing # SSH from on-premise to AWS (if keys configured) ssh ec2-user@[AWS_EC2_INSTANCE_IP] # HTTP/HTTPS services (if configured) curl http://[AWS_EC2_INSTANCE_IP] Troubleshooting and Resolution Challenge 1: Route Table Issues Problem: pfSense wasn\u0026rsquo;t automatically creating routes to AWS networks despite established tunnels.\nSolution: Added \u0026ldquo;Automatically ping host\u0026rdquo; parameter in Phase 2 configuration pointing to AWS VPC gateway (10.1.0.1). This forces pfSense to maintain active routes through the tunnel.\nChallenge 2: AWS Security Group Configuration Problem: Initial ping tests failed due to restrictive default security groups.\nSolution: Created explicit security group rules allowing ICMP and SSH traffic from on-premise network ranges to AWS resources.\nChallenge 3: Tunnel Authentication Issues Problem: Initial configuration used weaker encryption (SHA-1, DH Group 2).\nSolution: Updated both AWS VPN connection and pfSense configuration to use stronger parameters:\nIKEv2 instead of IKEv1 AES-256 instead of AES-128 SHA2-256 instead of SHA-1 DH Group 14 instead of Group 2 Security Considerations Network Segmentation AWS VPC uses non-overlapping IP space (10.1.0.0/16) On-premise networks remain segmented by existing VLAN structure VPN provides encrypted tunnel between environments Access Control Security groups limit AWS resource access to specific on-premise networks pfSense firewall rules control which VLANs can access AWS resources Principle of least privilege applied throughout Encryption Standards IKEv2 with AES-256 encryption SHA2-256 integrity checking Perfect Forward Secrecy (PFS) enabled Strong Diffie-Hellman groups (Group 14) Monitoring and Logging AWS VPC Flow Logs capture network traffic pfSense logs IPsec tunnel status and traffic CloudTrail logs AWS API activities Performance and Cost Considerations Network Performance VPN provides approximately 1.25 Gbps throughput Latency depends on geographic distance to AWS region Dual tunnels provide redundancy and failover capability Cost Structure AWS VPN Connection: $36/month base cost NAT Gateway: $45/month plus data processing Data transfer charges apply for cross-VPN traffic EC2 instances: Variable based on usage Optimization Opportunities Use VPC Endpoints to reduce NAT Gateway usage Implement lifecycle policies for temporary resources Monitor data transfer patterns for cost optimization Future Enhancements Automation Improvements Implement Terraform modules for reusability Add automated testing for tunnel connectivity Create CI/CD pipeline for infrastructure changes Security Enhancements Implement AWS Config for compliance monitoring Add AWS GuardDuty for threat detection Configure AWS Security Hub for centralized security management Operational Improvements Set up CloudWatch monitoring for VPN metrics Implement automated failover testing Add backup VPN connection for additional redundancy Conclusion This project successfully established a production-grade Site-to-Site VPN between my on-premise pfSense infrastructure and AWS, creating a true hybrid cloud environment. The implementation demonstrates several key technical concepts:\nInfrastructure as Code: Using Terraform ensured reproducible, version-controlled infrastructure deployment with proper validation and documentation.\nNetwork Security: The solution implements enterprise-grade encryption and access controls while maintaining network segmentation and the principle of least privilege.\nHybrid Architecture: The established connection enables seamless communication between on-premise and cloud resources, opening possibilities for distributed applications, cloud bursting, and hybrid backup strategies.\nOperational Excellence: Comprehensive monitoring, logging, and documentation support ongoing management and troubleshooting.\nThe resulting infrastructure provides a solid foundation for hybrid cloud applications and demonstrates practical implementation of enterprise networking concepts in a home lab environment. This setup now enables exploration of advanced hybrid scenarios including cross-environment authentication, distributed databases, and multi-cloud architectures.\nKey Takeaways Planning is Critical: Proper IP address planning prevents conflicts and simplifies routing configuration.\nSecurity by Design: Implementing strong encryption and access controls from the beginning is easier than retrofitting security later.\nTesting Methodology: Systematic testing from basic connectivity to application-level validation ensures reliable operation.\nDocumentation Value: Thorough documentation accelerates troubleshooting and enables knowledge sharing.\nThis hybrid cloud setup now serves as a platform for continued learning and experimentation with enterprise cloud technologies and networking concepts.\n","permalink":"https://blog.tillynet.com/cloud-engineering-labs/aws-lab-3/establishing-aws-site-to-site-vpn-with-on-premise-pfsense-infrastructure/","summary":"\u003cp\u003eIn this post, I\u0026rsquo;ll walk through the complete process of establishing a Site-to-Site VPN connection between my on-premise pfSense firewall and AWS, creating a true hybrid cloud environment. This project demonstrates enterprise-grade network integration, secure tunnel establishment, and Infrastructure as Code principles.\u003c/p\u003e\n\u003ch2 id=\"project-overview\"\u003eProject Overview\u003c/h2\u003e\n\u003cp\u003eThe goal was to extend my existing home lab network (TillyNet) to AWS, enabling seamless communication between on-premise resources and cloud workloads. This creates opportunities for hybrid applications, cloud bursting, and distributed infrastructure management.\u003c/p\u003e","title":"Building a Hybrid Cloud: AWS Site-to-Site VPN with pfSense and Terraform"},{"content":"Executive Summary This document outlines the implementation of a secure Amazon S3 infrastructure using Infrastructure as Code (IaC) principles with Terraform. The lab demonstrates enterprise-grade security practices including network isolation, least-privilege access controls, comprehensive audit logging, and cost-optimized storage configurations. The implementation addresses common security challenges in cloud storage while maintaining operational efficiency and compliance requirements.\nArchitecture Overview Infrastructure Components The lab implements a multi-tier architecture consisting of:\nNetwork Layer: Custom VPC with public and private subnets, NAT Gateway for outbound internet access Compute Layer: Bastion host for secure access, private EC2 instance with IAM role-based S3 access Storage Layer: Encrypted S3 bucket with versioning, lifecycle policies, and access logging Security Layer: IAM roles with least-privilege policies, VPC endpoints for private connectivity Monitoring Layer: CloudTrail for API auditing, S3 access logs for request-level monitoring Network Architecture Internet Gateway\r|\rPublic Subnet (10.0.1.0/24)\r|\rBastion Host\r|\rPrivate Subnet (10.0.2.0/24)\r|\rPrivate EC2 Instance -----\u0026gt; S3 VPC Endpoint -----\u0026gt; S3 Bucket\r|\rNAT Gateway (for outbound internet access) ┌─────────────────────────────────────────────────────────────┐\r│ My Existing VPC │\r│ ┌─────────────────┐ ┌─────────────────────┐ │\r│ │ Public Subnet │ │ Private Subnet │ │\r│ │ │ │ │ │\r│ │ ┌───────────┐ │ │ ┌───────────────┐ │ │\r│ │ │ Bastion │ │ │ │ Private EC2 │ │ │\r│ │ │ Host │ │──────────────│ │ + IAM Role │ │ │\r│ │ │ │ │ │ │ │ │ │\r│ │ └───────────┘ │ │ └───────────────┘ │ │\r│ └─────────────────┘ └─────────────────────┘ │\r└─────────────────────────────────────────────────────────────┘\r│\r│ IAM Role Permissions\r▼\r┌─────────────────────────────────────────────────────────────┐\r│ Secure S3 Bucket │\r│ • Server-Side Encryption (SSE-S3 or SSE-KMS) │\r│ • Public Access Blocked │\r│ • Bucket Policy Enforcement │\r│ • CloudTrail Logging │\r└─────────────────────────────────────────────────────────────┘ Infrastructure Implementation 1. VPC and Network Configuration The foundation of the infrastructure is a custom VPC providing network isolation and controlled access patterns.\nVPC Configuration (vpc.tf) resource \u0026#34;aws_vpc\u0026#34; \u0026#34;main\u0026#34; { cidr_block = var.vpc_cidr enable_dns_hostnames = true enable_dns_support = true tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-vpc\u0026#34; }) } Purpose: Creates an isolated network environment with DNS resolution enabled for service discovery and hostname resolution within the VPC.\nSubnet Architecture Public Subnet: Hosts the bastion host with direct internet access through an Internet Gateway.\nresource \u0026#34;aws_subnet\u0026#34; \u0026#34;public\u0026#34; { vpc_id = aws_vpc.main.id cidr_block = var.public_subnet_cidr availability_zone = data.aws_availability_zones.available.names[0] map_public_ip_on_launch = true tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-public-subnet\u0026#34; Type = \u0026#34;Public\u0026#34; }) } Private Subnet: Hosts application workloads with no direct internet access, using NAT Gateway for outbound connectivity.\nresource \u0026#34;aws_subnet\u0026#34; \u0026#34;private\u0026#34; { vpc_id = aws_vpc.main.id cidr_block = var.private_subnet_cidr availability_zone = data.aws_availability_zones.available.names[0] tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-private-subnet\u0026#34; Type = \u0026#34;Private\u0026#34; }) } Design Rationale: This subnet architecture implements defense-in-depth by isolating internet-facing resources from internal application resources, reducing the attack surface and providing granular network-level access controls.\nNAT Gateway Implementation resource \u0026#34;aws_nat_gateway\u0026#34; \u0026#34;main\u0026#34; { allocation_id = aws_eip.nat.id subnet_id = aws_subnet.public.id tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-nat-gateway\u0026#34; }) depends_on = [aws_internet_gateway.main] } Purpose: Enables outbound internet connectivity for private subnet resources while preventing inbound internet access. Essential for package updates, AWS API calls, and other outbound services.\n2. S3 Bucket Configuration The S3 bucket implementation follows AWS security best practices with multiple layers of protection.\nPrimary S3 Bucket (s3.tf) resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;secure_bucket\u0026#34; { bucket = local.bucket_name tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-secure-bucket\u0026#34; Purpose = \u0026#34;Secure data storage\u0026#34; Compliance = \u0026#34;Enterprise\u0026#34; DataClass = \u0026#34;Confidential\u0026#34; }) } Naming Strategy: Uses a combination of project prefix and random suffix to ensure global uniqueness while maintaining recognizability.\nSecurity Hardening Public Access Block: Prevents accidental public exposure of bucket contents.\nresource \u0026#34;aws_s3_bucket_public_access_block\u0026#34; \u0026#34;secure_bucket_pab\u0026#34; { bucket = aws_s3_bucket.secure_bucket.id block_public_acls = true block_public_policy = true ignore_public_acls = true restrict_public_buckets = true } Server-Side Encryption: Implements AES-256 encryption for data at rest.\nresource \u0026#34;aws_s3_bucket_server_side_encryption_configuration\u0026#34; \u0026#34;secure_bucket_encryption\u0026#34; { bucket = aws_s3_bucket.secure_bucket.id rule { apply_server_side_encryption_by_default { sse_algorithm = \u0026#34;AES256\u0026#34; } } } Versioning: Enables object versioning for data protection and recovery capabilities.\nresource \u0026#34;aws_s3_bucket_versioning\u0026#34; \u0026#34;secure_bucket_versioning\u0026#34; { bucket = aws_s3_bucket.secure_bucket.id versioning_configuration { status = \u0026#34;Enabled\u0026#34; } } Lifecycle Management resource \u0026#34;aws_s3_bucket_lifecycle_configuration\u0026#34; \u0026#34;secure_bucket_lifecycle\u0026#34; { bucket = aws_s3_bucket.secure_bucket.id rule { id = \u0026#34;intelligent_tiering\u0026#34; status = \u0026#34;Enabled\u0026#34; filter { prefix = \u0026#34;\u0026#34; } transition { days = 30 storage_class = \u0026#34;STANDARD_IA\u0026#34; } transition { days = 90 storage_class = \u0026#34;GLACIER\u0026#34; } noncurrent_version_expiration { noncurrent_days = 90 } abort_incomplete_multipart_upload { days_after_initiation = 7 } } } Cost Optimization Strategy: Automatically transitions objects to lower-cost storage classes based on access patterns, reducing storage costs by up to 70% while maintaining data availability.\n3. IAM Security Model The lab implements a least-privilege access model using IAM roles and policies.\nEC2 Instance Role (iam.tf) resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;ec2_s3_role\u0026#34; { name = \u0026#34;${local.name_prefix}-ec2-s3-role\u0026#34; assume_role_policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [ { Action = \u0026#34;sts:AssumeRole\u0026#34; Effect = \u0026#34;Allow\u0026#34; Principal = { Service = \u0026#34;ec2.amazonaws.com\u0026#34; } } ] }) tags = local.common_tags } Trust Policy: Allows only EC2 instances to assume this role, preventing unauthorized access from other AWS services or external entities.\nS3 Access Policy resource \u0026#34;aws_iam_role_policy\u0026#34; \u0026#34;ec2_s3_policy\u0026#34; { name = \u0026#34;${local.name_prefix}-ec2-s3-policy\u0026#34; role = aws_iam_role.ec2_s3_role.id policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [ { Effect = \u0026#34;Allow\u0026#34; Action = [ \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:DeleteObjectVersion\u0026#34; ] Resource = [ aws_s3_bucket.secure_bucket.arn, \u0026#34;${aws_s3_bucket.secure_bucket.arn}/*\u0026#34; ] } ] }) } Principle of Least Privilege: Grants only the minimum permissions necessary for application functionality, scoped specifically to the target S3 bucket.\n4. VPC Endpoints for Private Connectivity VPC Endpoints eliminate the need for internet routing of S3 traffic, improving security and reducing costs.\nS3 VPC Endpoint (s3.tf) resource \u0026#34;aws_vpc_endpoint\u0026#34; \u0026#34;s3_endpoint\u0026#34; { vpc_id = aws_vpc.main.id service_name = \u0026#34;com.amazonaws.${data.aws_region.current.name}.s3\u0026#34; vpc_endpoint_type = \u0026#34;Gateway\u0026#34; route_table_ids = [aws_route_table.private.id] policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [ { Effect = \u0026#34;Allow\u0026#34; Principal = \u0026#34;*\u0026#34; Action = [ \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:DeleteObjectVersion\u0026#34; ] Resource = [ aws_s3_bucket.secure_bucket.arn, \u0026#34;${aws_s3_bucket.secure_bucket.arn}/*\u0026#34;, \u0026#34;arn:aws:s3:::amazonlinux-2-repos-${data.aws_region.current.name}\u0026#34;, \u0026#34;arn:aws:s3:::amazonlinux-2-repos-${data.aws_region.current.name}/*\u0026#34; ] } ] }) tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-s3-vpc-endpoint\u0026#34; }) } Security Benefits:\nTraffic remains within AWS network backbone Eliminates exposure to internet-based attacks Provides additional layer of access control through endpoint policies Cost Benefits:\nReduces NAT Gateway data processing charges by ~78% No hourly charges for Gateway endpoints Lower latency for S3 operations 5. EC2 Infrastructure Bastion Host Configuration (main.tf) resource \u0026#34;aws_instance\u0026#34; \u0026#34;bastion\u0026#34; { ami = data.aws_ami.amazon_linux.id instance_type = var.instance_type key_name = var.key_pair_name subnet_id = aws_subnet.public.id vpc_security_group_ids = [aws_security_group.bastion_sg.id] associate_public_ip_address = true user_data = base64encode(templatefile(\u0026#34;${path.module}/user-data/bastion-userdata.sh\u0026#34;, { hostname = \u0026#34;${local.name_prefix}-bastion\u0026#34; })) tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-bastion\u0026#34; Role = \u0026#34;Bastion\u0026#34; }) } Purpose: Provides secure SSH access to private subnet resources while maintaining network isolation. Acts as a controlled entry point for administrative access.\nPrivate Instance Configuration resource \u0026#34;aws_instance\u0026#34; \u0026#34;private_ec2\u0026#34; { ami = data.aws_ami.amazon_linux.id instance_type = var.instance_type key_name = var.key_pair_name subnet_id = aws_subnet.private.id vpc_security_group_ids = [aws_security_group.private_sg.id] iam_instance_profile = aws_iam_instance_profile.ec2_profile.name user_data = base64encode(templatefile(\u0026#34;${path.module}/user-data/private-userdata.sh\u0026#34;, { hostname = \u0026#34;${local.name_prefix}-private\u0026#34; bucket_name = local.bucket_name region = data.aws_region.current.name })) tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-private\u0026#34; Role = \u0026#34;Application\u0026#34; }) } Security Configuration: Instance is placed in private subnet with no direct internet access, relies on IAM role for S3 access instead of hardcoded credentials.\n6. Security Groups Security groups implement stateful firewall rules controlling network access.\nBastion Security Group resource \u0026#34;aws_security_group\u0026#34; \u0026#34;bastion_sg\u0026#34; { name_prefix = \u0026#34;${local.name_prefix}-bastion-sg\u0026#34; vpc_id = aws_vpc.main.id description = \u0026#34;Security group for bastion host\u0026#34; ingress { description = \u0026#34;SSH from internet\u0026#34; from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = var.allowed_ssh_cidrs } egress { description = \u0026#34;All outbound traffic\u0026#34; from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-bastion-sg\u0026#34; }) } Access Control: Restricts SSH access to specified IP ranges, preventing unauthorized access attempts.\nPrivate Instance Security Group resource \u0026#34;aws_security_group\u0026#34; \u0026#34;private_sg\u0026#34; { name_prefix = \u0026#34;${local.name_prefix}-private-sg\u0026#34; vpc_id = aws_vpc.main.id description = \u0026#34;Security group for private EC2 instances\u0026#34; ingress { description = \u0026#34;SSH from bastion\u0026#34; from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; security_groups = [aws_security_group.bastion_sg.id] } egress { description = \u0026#34;HTTPS outbound\u0026#34; from_port = 443 to_port = 443 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } egress { description = \u0026#34;HTTP outbound\u0026#34; from_port = 80 to_port = 80 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-private-sg\u0026#34; }) } Principle of Least Privilege: Allows only necessary connections (SSH from bastion, HTTPS/HTTP outbound for package updates and AWS API calls).\nAudit and Monitoring Implementation 1. CloudTrail Configuration CloudTrail provides comprehensive API-level auditing for all AWS service interactions.\nCloudTrail Setup (cloudtrail.tf) resource \u0026#34;aws_cloudtrail\u0026#34; \u0026#34;main_trail\u0026#34; { name = \u0026#34;${local.name_prefix}-cloudtrail\u0026#34; s3_bucket_name = aws_s3_bucket.cloudtrail_logs.bucket include_global_service_events = true is_multi_region_trail = false enable_logging = true event_selector { read_write_type = \u0026#34;All\u0026#34; include_management_events = true data_resource { type = \u0026#34;AWS::S3::Object\u0026#34; values = [\u0026#34;${aws_s3_bucket.secure_bucket.arn}/*\u0026#34;] } } tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-cloudtrail\u0026#34; }) } Monitoring Scope:\nManagement Events: API calls for AWS service configuration changes Data Events: Object-level operations on S3 bucket contents Global Services: IAM, CloudFront, and other global service events CloudTrail Log Storage resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;cloudtrail_logs\u0026#34; { bucket = \u0026#34;${local.name_prefix}-cloudtrail-logs-${random_id.cloudtrail_suffix.hex}\u0026#34; tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-cloudtrail-logs\u0026#34; Purpose = \u0026#34;CloudTrail audit logs\u0026#34; }) } Security Measures:\nSeparate bucket for audit logs to prevent tampering Server-side encryption enabled Public access blocked Lifecycle policies for cost management 2. S3 Access Logging S3 access logs provide detailed request-level information for security analysis and performance monitoring.\nAccess Log Configuration (s3-logging.tf) resource \u0026#34;aws_s3_bucket_logging\u0026#34; \u0026#34;secure_bucket_logging\u0026#34; { bucket = aws_s3_bucket.secure_bucket.id target_bucket = aws_s3_bucket.access_logs.id target_prefix = \u0026#34;access-logs/\u0026#34; } Log Information Captured:\nRequest timestamp and processing time Client IP address and User-Agent HTTP method and response code Bytes transferred and object size Authentication and authorization details Access Log Storage Bucket resource \u0026#34;aws_s3_bucket\u0026#34; \u0026#34;access_logs\u0026#34; { bucket = \u0026#34;${local.name_prefix}-access-logs-${random_id.access_logs_suffix.hex}\u0026#34; tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-access-logs\u0026#34; Purpose = \u0026#34;S3 access logs\u0026#34; }) } Cost Optimization: Lifecycle policies automatically transition logs to cheaper storage classes and delete old logs to manage storage costs.\nProblem-Solution Analysis 1. Network Security Challenges Problem: Traditional cloud deployments often expose resources directly to the internet, increasing attack surface and security risks.\nSolution Implemented:\nPrivate subnet isolation for application workloads Bastion host for controlled administrative access VPC endpoints for private AWS service connectivity Security groups implementing least-privilege network access Impact: Reduced attack surface by eliminating direct internet access to application resources while maintaining necessary connectivity.\n2. Access Control and Authentication Problem: Hard-coded credentials and overly permissive IAM policies create security vulnerabilities and compliance issues.\nSolution Implemented:\nIAM roles with least-privilege policies Instance profiles for credential-free access Resource-specific permissions scoped to individual buckets Regular credential rotation through role assumption Impact: Eliminated credential management overhead while ensuring secure, auditable access to cloud resources.\n3. Data Protection and Compliance Problem: Unencrypted data storage and lack of access auditing create compliance and security gaps.\nSolution Implemented:\nServer-side encryption for all S3 objects Object versioning for data recovery capabilities Comprehensive audit logging through CloudTrail and S3 access logs Public access blocking to prevent accidental exposure Impact: Achieved enterprise-grade data protection with comprehensive audit trails for compliance requirements.\n4. Cost Optimization Problem: Cloud storage costs can escalate quickly without proper lifecycle management and connectivity optimization.\nSolution Implemented:\nLifecycle policies for automatic storage class transitions VPC endpoints to reduce NAT Gateway charges Intelligent log retention policies Multipart upload cleanup to prevent storage waste Impact: Reduced storage costs by approximately 70% through automated lifecycle management and eliminated unnecessary data transfer charges.\n5. VPC Endpoint Policy Restrictions Problem Encountered: During testing, VPC endpoint policies blocked legitimate access to Amazon Linux package repositories, causing package installation failures.\nRoot Cause: VPC endpoint policies act as additional access control layers. When policies are too restrictive, they can block necessary system operations even when IAM permissions allow access.\nResolution Applied:\npolicy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [ { Effect = \u0026#34;Allow\u0026#34; Principal = \u0026#34;*\u0026#34; Action = [ \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:PutObject\u0026#34; ] Resource = [ aws_s3_bucket.secure_bucket.arn, \u0026#34;${aws_s3_bucket.secure_bucket.arn}/*\u0026#34;, \u0026#34;arn:aws:s3:::amazonlinux-2-repos-*\u0026#34;, \u0026#34;arn:aws:s3:::amazonlinux-2-repos-*/*\u0026#34; ] } ] }) Learning: VPC endpoint policies require careful consideration of all S3 resources that instances need to access, including system repositories and AWS service dependencies.\nOperational Procedures 1. Access Patterns Administrative Access:\n# Connect to bastion host ssh -i keypair.pem ec2-user@\u0026lt;bastion-public-ip\u0026gt; # Connect to private instance through bastion ssh -A -i keypair.pem -o ProxyJump=ec2-user@\u0026lt;bastion-public-ip\u0026gt; ec2-user@\u0026lt;private-ip\u0026gt; S3 Operations:\n# List bucket contents aws s3 ls s3://bucket-name/ # Upload files aws s3 cp file.txt s3://bucket-name/path/ # Download files aws s3 cp s3://bucket-name/path/file.txt ./ 2. Monitoring and Analysis CloudTrail Log Analysis:\n# Download recent CloudTrail logs aws s3 cp s3://cloudtrail-bucket/recent-log.json.gz ./ # Extract and analyze events gunzip recent-log.json.gz jq \u0026#39;.Records[] | select(.eventSource == \u0026#34;s3.amazonaws.com\u0026#34;)\u0026#39; recent-log.json S3 Access Log Analysis:\n# Download access logs aws s3 cp s3://access-logs-bucket/access-logs/recent.log ./ # Analyze request patterns awk \u0026#39;{print $8}\u0026#39; recent.log | sort | uniq -c | sort -nr Security Considerations 1. Data Classification The implementation supports multiple data security levels:\nPublic: No restrictions (not implemented in this secure configuration) Internal: Accessible within VPC through proper authentication Confidential: Encrypted at rest and in transit, comprehensive audit logging Restricted: Additional access controls and monitoring (future enhancement) 2. Compliance Framework Alignment The architecture aligns with several compliance frameworks:\nSOC 2: Comprehensive logging and access controls ISO 27001: Risk-based security controls and continuous monitoring NIST Cybersecurity Framework: Defense-in-depth implementation GDPR: Data protection through encryption and access controls 3. Threat Mitigation Threats Addressed:\nData Breaches: Encryption, access controls, and network isolation Insider Threats: Least-privilege access and comprehensive audit logging Configuration Drift: Infrastructure as Code ensures consistent configurations Unauthorized Access: Multi-layer authentication and authorization Cost Analysis 1. Infrastructure Costs (Monthly Estimates) EC2 Instances: ~$30-50 (t3.micro instances) NAT Gateway: ~$45 (fixed cost + data processing) S3 Storage: Variable based on usage, optimized through lifecycle policies CloudTrail: $2 per 100,000 events for data events VPC Endpoints: Free for Gateway endpoints (S3, DynamoDB) 2. Cost Optimization Strategies Storage Class Transitions: Automatic movement to cheaper storage classes VPC Endpoints: Reduced NAT Gateway data processing charges Log Lifecycle Policies: Automated deletion of old audit logs Resource Tagging: Detailed cost allocation and optimization opportunities Conclusion This lab demonstrates the implementation of enterprise-grade secure cloud storage infrastructure using AWS services and Infrastructure as Code principles. The solution addresses critical security, compliance, and cost optimization requirements while maintaining operational efficiency. The multi-layered security approach, comprehensive audit capabilities, and automated cost optimization features provide a solid foundation for production workloads requiring high security standards.\nThe implementation serves as a reference architecture for organizations seeking to deploy secure, compliant, and cost-effective cloud storage solutions while maintaining operational agility through automation and Infrastructure as Code practices.\nFuture Enhancements Potential improvements to consider:\nMulti-AZ deployment for high availability AWS Config for configuration compliance monitoring AWS Security Hub for centralized security findings management AWS GuardDuty for threat detection and monitoring Cross-region replication for disaster recovery AWS KMS for customer-managed encryption keys AWS Systems Manager for patch management and configuration Amazon Macie for data discovery and classification ","permalink":"https://blog.tillynet.com/cloud-engineering-labs/aws-lab-2/secure-s3-infrastructure-lab---technical-documentation/","summary":"\u003ch2 id=\"executive-summary\"\u003eExecutive Summary\u003c/h2\u003e\n\u003cp\u003eThis document outlines the implementation of a secure Amazon S3 infrastructure using Infrastructure as Code (IaC) principles with Terraform. The lab demonstrates enterprise-grade security practices including network isolation, least-privilege access controls, comprehensive audit logging, and cost-optimized storage configurations. The implementation addresses common security challenges in cloud storage while maintaining operational efficiency and compliance requirements.\u003c/p\u003e\n\u003ch2 id=\"architecture-overview\"\u003eArchitecture Overview\u003c/h2\u003e\n\u003ch3 id=\"infrastructure-components\"\u003eInfrastructure Components\u003c/h3\u003e\n\u003cp\u003eThe lab implements a multi-tier architecture consisting of:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eNetwork Layer\u003c/strong\u003e: Custom VPC with public and private subnets, NAT Gateway for outbound internet access\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCompute Layer\u003c/strong\u003e: Bastion host for secure access, private EC2 instance with IAM role-based S3 access\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eStorage Layer\u003c/strong\u003e: Encrypted S3 bucket with versioning, lifecycle policies, and access logging\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eSecurity Layer\u003c/strong\u003e: IAM roles with least-privilege policies, VPC endpoints for private connectivity\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMonitoring Layer\u003c/strong\u003e: CloudTrail for API auditing, S3 access logs for request-level monitoring\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"network-architecture\"\u003eNetwork Architecture\u003c/h3\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eInternet Gateway\r\n       |\r\n   Public Subnet (10.0.1.0/24)\r\n       |\r\n   Bastion Host\r\n       |\r\n   Private Subnet (10.0.2.0/24)\r\n       |\r\n   Private EC2 Instance -----\u0026gt; S3 VPC Endpoint -----\u0026gt; S3 Bucket\r\n       |\r\n   NAT Gateway (for outbound internet access)\n\u003c/code\u003e\u003c/pre\u003e\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e┌─────────────────────────────────────────────────────────────┐\r\n│                    My Existing VPC                          │\r\n│  ┌─────────────────┐              ┌─────────────────────┐   │\r\n│  │  Public Subnet  │              │  Private Subnet     │   │\r\n│  │                 │              │                     │   │\r\n│  │  ┌───────────┐  │              │  ┌───────────────┐  │   │\r\n│  │  │ Bastion   │  │              │  │ Private EC2   │  │   │\r\n│  │  │ Host      │  │──────────────│  │ + IAM Role    │  │   │\r\n│  │  │           │  │              │  │               │  │   │\r\n│  │  └───────────┘  │              │  └───────────────┘  │   │\r\n│  └─────────────────┘              └─────────────────────┘   │\r\n└─────────────────────────────────────────────────────────────┘\r\n                               │\r\n                               │ IAM Role Permissions\r\n                               ▼\r\n┌─────────────────────────────────────────────────────────────┐\r\n│                    Secure S3 Bucket                         │\r\n│  • Server-Side Encryption (SSE-S3 or SSE-KMS)               │\r\n│  • Public Access Blocked                                    │\r\n│  • Bucket Policy Enforcement                                │\r\n│  • CloudTrail Logging                                       │\r\n└─────────────────────────────────────────────────────────────┘\n\u003c/code\u003e\u003c/pre\u003e\u003ch2 id=\"infrastructure-implementation\"\u003eInfrastructure Implementation\u003c/h2\u003e\n\u003ch3 id=\"1-vpc-and-network-configuration\"\u003e1. VPC and Network Configuration\u003c/h3\u003e\n\u003cp\u003eThe foundation of the infrastructure is a custom VPC providing network isolation and controlled access patterns.\u003c/p\u003e","title":"Building Secure AWS S3 Infrastructure with Terraform - Complete Lab Guide"},{"content":"Overview This document summarizes the technical work completed to build, configure, and automate a secure AWS VPC lab using Terraform. It covers manual and automated infrastructure deployment steps, with detailed explanations for each Terraform configuration component, designed to help replicate the setup at any time in the future.\nObjectives Build a segmented AWS VPC network with public and private subnets.\nLaunch EC2 instances in both public and private subnets.\nSecure access using Security Groups.\nEnable VPC Flow Logs for traffic monitoring.\nManage the infrastructure using Terraform, with variables and data sources referencing existing AWS resources.\nPrerequisites An AWS Free Tier account.\nAWS CLI installed and configured (aws configure) with IAM user access keys.\nTerraform installed on the local workstation.\nAn existing VPC created manually in AWS.\nAn EC2 SSH Key Pair created in AWS (with the .pem file downloaded and stored securely).\nTerraform Project Structure /terraform-vpc-lab\r├── provider.tf\r├── variables.tf\r├── data.tf\r├── locals.tf\r├── main.tf\r├── outputs.tf\r├── terraform.tfvars\r├── deploy-terraform.ps1\r└── user-data\r├── private-userdata.sh\r└── public-userdata.sh Each file has a specific purpose, described in detail below.\nprovider.tf provider \u0026#34;aws\u0026#34; { region = \u0026#34;us-east-1\u0026#34; } Explanation:\nDefines the AWS provider Terraform will use.\nSets the default AWS region to us-west-1. This can be adjusted or turned into a configurable variable if needed.\nvariables.tf The variables.tf file defines all the input variables your Terraform configuration uses, making it:\nFlexible Resusable Easier to manage across environments (dev, staging, prod) By declaring variables here we avoid hardcoding values in main.tf or other resource files. # variables.tf - defines and validates variables variable \u0026#34;vpc_cidr\u0026#34; { description = \u0026#34;CIDR block for the new VPC\u0026#34; type = string default = \u0026#34;10.0.0.0/16\u0026#34; validation { condition = can(cidrhost(var.vpc_cidr, 0)) error_message = \u0026#34;VPC CIDR must be a valid IPv4 CIDR block.\u0026#34; } } # defines which AWS Availability zone to deploy into variable \u0026#34;availability_zone\u0026#34; { description = \u0026#34;The AWS Availability Zone\u0026#34; type = string default = \u0026#34;us-west-1a\u0026#34; # regex validation to ensure the format matches AWS AZ patterns validation { condition = can(regex(\u0026#34;^[a-z]{2}-[a-z]+-[0-9][a-z]$\u0026#34;, var.availability_zone)) error_message = \u0026#34;Availability zone must be in the format like \u0026#39;us-west-1a\u0026#39;.\u0026#34; } } # Defines the IP range for the public subnet variable \u0026#34;public_subnet_cidr\u0026#34; { description = \u0026#34;CIDR block for the public subnet\u0026#34; type = string default = \u0026#34;10.0.1.0/24\u0026#34; # CIDR validation check validation { condition = can(cidrhost(var.public_subnet_cidr, 0)) error_message = \u0026#34;Public subnet CIDR must be a valid IPv4 CIDR block.\u0026#34; } } # Defines the IP range for the private subnet variable \u0026#34;private_subnet_cidr\u0026#34; { description = \u0026#34;CIDR block for the private subnet\u0026#34; type = string default = \u0026#34;10.0.2.0/24\u0026#34; # CIDR validation check validation { condition = can(cidrhost(var.private_subnet_cidr, 0)) error_message = \u0026#34;Private subnet CIDR must be a valid IPv4 CIDR block.\u0026#34; } } # Specifies the name of the EC2 SSH key pair used for connecting to instances. variable \u0026#34;key_name\u0026#34; { description = \u0026#34;The name of the EC2 SSH key pair\u0026#34; type = string } # Used in security group rules to restrict SSH access # requires a /32 CIDR notation variable \u0026#34;home_ip\u0026#34; { description = \u0026#34;Your home public IP address with /32 mask (e.g., 203.0.113.1/32)\u0026#34; type = string # Regex validation to ensure proper format validation { condition = can(regex(\u0026#34;^([0-9]{1,3}\\\\.){3}[0-9]{1,3}/32$\u0026#34;, var.home_ip)) error_message = \u0026#34;Home IP must be a valid IPv4 address with /32 mask (e.g., 203.0.113.1/32).\u0026#34; } } # Used for tagging AWS resources for identification # Helps track whick resources belong to whick project variable \u0026#34;project_name\u0026#34; { description = \u0026#34;Name of the project for resource tagging\u0026#34; type = string default = \u0026#34;terraform-vpc-demo\u0026#34; } # Tags resources or configures settings per environment variable \u0026#34;environment\u0026#34; { description = \u0026#34;Environment name (dev, staging, prod)\u0026#34; type = string default = \u0026#34;dev\u0026#34; # Ensures only use valid environment names validation { condition = contains([\u0026#34;dev\u0026#34;, \u0026#34;staging\u0026#34;, \u0026#34;prod\u0026#34;], var.environment) error_message = \u0026#34;Environment must be one of: dev, staging, prod.\u0026#34; } } # Controls the size of the EC2 instances # Below is freet-tier eligible variable \u0026#34;instance_type\u0026#34; { description = \u0026#34;EC2 instance type\u0026#34; type = string default = \u0026#34;t2.micro\u0026#34; } # Controls whether DNS hostnames is enabled for the VPC # Belowis set to true - best practice variable \u0026#34;enable_dns_hostnames\u0026#34; { description = \u0026#34;Enable DNS hostnames in the VPC\u0026#34; type = bool default = true } # Controls whether DNS support is enabled for the VPC # Below set to true - best practice variable \u0026#34;enable_dns_support\u0026#34; { description = \u0026#34;Enable DNS support in the VPC\u0026#34; type = bool default = true } Explanation:\nDefines all required input variables, including the VPC, subnet CIDRs, key pair name, and the home public IP address for secure SSH access. Modular and adaptable to different setups. Safter through built-in validation blocks. Easier to manage across environments by externalizing critical settings. terraform.tfvars # terraform.tfvars - define sensitive values key_name = \u0026#34;AWS_key_name\u0026#34; home_ip = \u0026#34;publicIP/32\u0026#34; project_name = \u0026#34;project name\u0026#34; environment = \u0026#34;dev\u0026#34; vpc_cidr = \u0026#34;10.0.0.0/16\u0026#34; public_subnet_cidr = \u0026#34;10.0.1.0/24\u0026#34; private_subnet_cidr = \u0026#34;10.0.2.0/24\u0026#34; availability_zone = \u0026#34;us-west-1a\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; Explanation:\nProvides actual values for the variables defined in variables.tf.\nThis file is automatically loaded by Terraform and should be excluded from version control for sensitive values.\ndata.tf This file defines Terraform data sources, which allows the ability to query AWS for existing dynamic information that is not created or managed directly, such as available regions, caller identity, AMI IDs, or availability zones.\nThese values can be used throughout Terraform configurations for dynamic, up-to-date references.\n# data.tf - Centralize data sources # Fetches the AWS region that Terraform is currently operating in # Useful to reference the region dynamically in resources or outputs without hardcoding var.region everwhere data \u0026#34;aws_region\u0026#34; \u0026#34;current\u0026#34; {} # Returns details about the active AWS identity: AWS account ID, Amazon Resource Name (ARN), User ID # Useful for tagging, auditing, or conditional logic tied to the current account data \u0026#34;aws_caller_identity\u0026#34; \u0026#34;current\u0026#34; {} # Returns a list of available AZs in the current region # Can be used to distribute resources across zones for HA # below is only including AZs in the \u0026#34;available\u0026#34; state data \u0026#34;aws_availability_zones\u0026#34; \u0026#34;available\u0026#34; { state = \u0026#34;available\u0026#34; } # Looks up the most recent official Amazon Linux 2 AMI # Ensures that EC2 instances are always built from the latest Amazon-maintained AMI, reducing the need to hardcode AMI IDs data \u0026#34;aws_ami\u0026#34; \u0026#34;amazon_linux\u0026#34; { most_recent = true owners = [\u0026#34;amazon\u0026#34;] filter { name = \u0026#34;name\u0026#34; values = [\u0026#34;amzn2-ami-hvm-*-x86_64-gp2\u0026#34;] # HVM-based, general-purpose SSD, 64-bit } filter { name = \u0026#34;virtualization-type\u0026#34; # sets virtualization type to HVM values = [\u0026#34;hvm\u0026#34;] } } Explanation:\nUses Terraform\u0026rsquo;s data sources to reference existing AWS resources without Terraform managing or creating them. Makes Terraform configuration more dynamic and adaptable. Reduce hardcoding of values that might change between regions, accounts, or over time. Help ensure that the infrastructure always targets the correct, most up-to-date AWS resources. locals.tf The locals block in Terrraform defines computed values or constants that can be reused throughout the configuration. The computed values and constants help reduce repetition, centralize logic, and make configurations easier to maintain. These are like reusable variables, but they can include computed or derived values (not just user-provided inputs).\n# locals.tf - Define reusable values and computed tags # `locals {` starts the block locals { # Common tags to apply to all resources # Defines a reusable map of tags common_tags = { Project = var.project_name Environment = var.environment ManagedBy = \u0026#34;Terraform\u0026#34; CreatedAt = timestamp() # Computed at runtime using current runtime stamp } # Resource naming convention # Useful for consistent resource names or tags across environments name_prefix = \u0026#34;${var.project_name}-${var.environment}\u0026#34; # Computed values # These locals make it easier to refer to the values without repeating longer expressions vpc_cidr = var.vpc_cidr # references the variable block region = data.aws_region.current.name # references the data block } Explanation:\nDefines local values to standardize instance naming across the configuration. Can apply the local.common_tags to all taggable resources, ensuring consistency across infrastructure. Code consistency (single place to define tag maps or name patterns) Maintainability (change once, propagate everywhere) Readability (clear, descriptive references) main.tf This is the core infrastructure definition file in Terraform. It contains all the resource blocks to define and build AWS network, security, compute resources, logging, and permissions.\nKey sections explained:\nVPC Creation resource \u0026#34;aws_vpc\u0026#34; \u0026#34;main\u0026#34; { cidr_block = var.vpc_cidr enable_dns_hostnames = var.enable_dns_hostnames enable_dns_support = var.enable_dns_support tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-vpc\u0026#34; }) } Creates a new VPC using the specified CIDR block and DNS settings. Applies consistent tags merged from local common tags. Internet Gateway resource \u0026#34;aws_internet_gateway\u0026#34; \u0026#34;main\u0026#34; { vpc_id = aws_vpc.main.id tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-igw\u0026#34; }) } Attaches an internet gateway to the new VPC, enabling internet access for public resources. Subnets # Create public subnet resource \u0026#34;aws_subnet\u0026#34; \u0026#34;public\u0026#34; { vpc_id = aws_vpc.main.id cidr_block = var.public_subnet_cidr availability_zone = var.availability_zone map_public_ip_on_launch = true tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-public-subnet\u0026#34; Type = \u0026#34;Public\u0026#34; }) } # Create private subnet resource \u0026#34;aws_subnet\u0026#34; \u0026#34;private\u0026#34; { vpc_id = aws_vpc.main.id cidr_block = var.private_subnet_cidr availability_zone = var.availability_zone tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-private-subnet\u0026#34; Type = \u0026#34;Private\u0026#34; }) } Creates a public subnet and a private subnet in the specified availability zone. The public subnet allows public IP assignments; the private one does not. Elastic IP and NAT Gateway # Allocate Elastic IP for NAT Gateway resource \u0026#34;aws_eip\u0026#34; \u0026#34;nat\u0026#34; { domain = \u0026#34;vpc\u0026#34; tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-nat-eip\u0026#34; }) depends_on = [aws_internet_gateway.main] } # Create NAT Gateway in public subnet resource \u0026#34;aws_nat_gateway\u0026#34; \u0026#34;nat\u0026#34; { allocation_id = aws_eip.nat.id subnet_id = aws_subnet.public.id tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-nat-gateway\u0026#34; }) depends_on = [aws_internet_gateway.main] } Allocates a static Elastic IP and attaches it to a NAT Gateway in the public subnet, allowing private subnet instances to access the internet outbound. Route Tables # Create public route table resource \u0026#34;aws_route_table\u0026#34; \u0026#34;public\u0026#34; { vpc_id = aws_vpc.main.id route { cidr_block = \u0026#34;0.0.0.0/0\u0026#34; gateway_id = aws_internet_gateway.main.id } tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-public-rt\u0026#34; Type = \u0026#34;Public\u0026#34; }) } # Associate public route table with public subnet resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;public_assoc\u0026#34; { subnet_id = aws_subnet.public.id route_table_id = aws_route_table.public.id } # Create private route table (via NAT Gateway) resource \u0026#34;aws_route_table\u0026#34; \u0026#34;private\u0026#34; { vpc_id = aws_vpc.main.id # Changed from data.aws_vpc.existing.id route { cidr_block = \u0026#34;0.0.0.0/0\u0026#34; nat_gateway_id = aws_nat_gateway.nat.id } tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-private-rt\u0026#34; Type = \u0026#34;Private\u0026#34; }) } # Associate private route table with private subnet resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;private_assoc\u0026#34; { subnet_id = aws_subnet.private.id route_table_id = aws_route_table.private.id } Creates a public route table with a default route to the internet gateway.\nCreates a private route table with a default route to the NAT gateway.\nAssociates the correct route table with each subnet.\nSecurity Groups # Create Security Group for public EC2 resource \u0026#34;aws_security_group\u0026#34; \u0026#34;public_sg\u0026#34; { name_prefix = \u0026#34;${local.name_prefix}-public-\u0026#34; description = \u0026#34;Security group for public EC2 instances - allows SSH from specified IP\u0026#34; vpc_id = aws_vpc.main.id ingress { description = \u0026#34;SSH from home IP\u0026#34; from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [var.home_ip] } egress { description = \u0026#34;All outbound traffic\u0026#34; from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-public-sg\u0026#34; Type = \u0026#34;Public\u0026#34; }) lifecycle { create_before_destroy = true } } # Create Security Group for private EC2 resource \u0026#34;aws_security_group\u0026#34; \u0026#34;private_sg\u0026#34; { name_prefix = \u0026#34;${local.name_prefix}-private-\u0026#34; description = \u0026#34;Security group for private EC2 instances - allows SSH from public subnet\u0026#34; vpc_id = aws_vpc.main.id ingress { description = \u0026#34;SSH from public subnet\u0026#34; from_port = 22 to_port = 22 protocol = \u0026#34;tcp\u0026#34; security_groups = [aws_security_group.public_sg.id] } egress { description = \u0026#34;All outbound traffic\u0026#34; from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-private-sg\u0026#34; Type = \u0026#34;Private\u0026#34; }) lifecycle { create_before_destroy = true } } Public SG allows SSH access only from the specified home IP.\nPrivate SG allows SSH access only from the public instance SG (used as a bastion host).\nEC2 Instances # Launch EC2 in public subnet resource \u0026#34;aws_instance\u0026#34; \u0026#34;public_ec2\u0026#34; { ami = data.aws_ami.amazon_linux.id instance_type = var.instance_type subnet_id = aws_subnet.public.id key_name = var.key_name vpc_security_group_ids = [aws_security_group.public_sg.id] user_data = base64encode(templatefile(\u0026#34;${path.module}/user-data/public-userdata.sh\u0026#34;, { hostname = \u0026#34;${local.name_prefix}-public\u0026#34; })) tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-public-ec2\u0026#34; Type = \u0026#34;Public\u0026#34; Role = \u0026#34;Bastion\u0026#34; }) } # Launch EC2 in private subnet resource \u0026#34;aws_instance\u0026#34; \u0026#34;private_ec2\u0026#34; { ami = data.aws_ami.amazon_linux.id instance_type = var.instance_type subnet_id = aws_subnet.private.id key_name = var.key_name vpc_security_group_ids = [aws_security_group.private_sg.id] user_data = base64encode(templatefile(\u0026#34;${path.module}/user-data/private-userdata.sh\u0026#34;, { hostname = \u0026#34;${local.name_prefix}-private\u0026#34; })) tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-private-ec2\u0026#34; Type = \u0026#34;Private\u0026#34; Role = \u0026#34;Backend\u0026#34; }) } Launches an EC2 instance in the public subnet (Amazon Linux 2), serving as a bastion or jump host.\nLaunches an EC2 instance in the private subnet (Amazon Linux 2), serving as a backend node.\nBoth use user data scripts (public-userdata.sh, private-userdata.sh) and consistent tagging.\nCloudWatch Log Group for VPC Flow Logs # Create CloudWatch Log Group for VPC Flow Logs resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;vpc_logs\u0026#34; { name = \u0026#34;/aws/vpc/flowlogs/${local.name_prefix}\u0026#34; retention_in_days = 7 tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-vpc-flow-logs\u0026#34; }) } Creates a CloudWatch log group to hold the VPC Flow Logs, with a 7-day retention period. IAM Role and Policy for VPC Flow Logs # Create IAM role for flow logs resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;flow_logs_role\u0026#34; { name_prefix = \u0026#34;${local.name_prefix}-flow-logs-\u0026#34; assume_role_policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34;, Statement = [{ Action = \u0026#34;sts:AssumeRole\u0026#34;, Effect = \u0026#34;Allow\u0026#34;, Principal = { Service = \u0026#34;vpc-flow-logs.amazonaws.com\u0026#34; } }] }) tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-flow-logs-role\u0026#34; }) } # Create custom IAM policy for flow logs resource \u0026#34;aws_iam_role_policy\u0026#34; \u0026#34;flow_logs_policy\u0026#34; { name_prefix = \u0026#34;${local.name_prefix}-flow-logs-\u0026#34; role = aws_iam_role.flow_logs_role.id policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34;, Statement = [ { Effect = \u0026#34;Allow\u0026#34;, Action = [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34; ], Resource = \u0026#34;${aws_cloudwatch_log_group.vpc_logs.arn}:*\u0026#34; } ] }) } Creates an IAM role with permissions for VPC Flow Logs to push data to CloudWatch Logs. Enable VPC Flow Logs # Enable VPC Flow Logs resource \u0026#34;aws_flow_log\u0026#34; \u0026#34;vpc_flow_logs\u0026#34; { log_destination_type = \u0026#34;cloud-watch-logs\u0026#34; log_destination = aws_cloudwatch_log_group.vpc_logs.arn traffic_type = \u0026#34;ALL\u0026#34; vpc_id = aws_vpc.main.id # Changed from data.aws_vpc.existing.id iam_role_arn = aws_iam_role.flow_logs_role.arn tags = merge(local.common_tags, { Name = \u0026#34;${local.name_prefix}-vpc-flow-logs\u0026#34; }) } Enables detailed traffic logging on the VPC, writing all traffic data (accepted and rejected) to CloudWatch Logs. Explanation:\nUses locals for consistent naming and tags. Applies merge patterns for reusable tag blocks. Includes lifecycle rules (like create_before_destroy) for safer updates. outputs.tf The outputs.tf file defines what Terraform will print after terraform apply finishes. This helps to get key resource, IDs, IPs, and metadata. It makes it easier to connect or interact with deployed resources. It can also feed outputs into other Terraform modules or systems.\n# outputs.tf # Outputs VPC Information # Useful for confirming network foundation and account context output \u0026#34;vpc_info\u0026#34; { description = \u0026#34;Information about the created VPC\u0026#34; value = { vpc_id = aws_vpc.main.id vpc_cidr = aws_vpc.main.cidr_block igw_id = aws_internet_gateway.main.id region = data.aws_region.current.name account_id = data.aws_caller_identity.current.account_id } } # Outputs Subnet Information # Helps verify where subnets landed output \u0026#34;subnet_info\u0026#34; { description = \u0026#34;Information about created subnets\u0026#34; value = { public_subnet = { id = aws_subnet.public.id cidr_block = aws_subnet.public.cidr_block az = aws_subnet.public.availability_zone } private_subnet = { id = aws_subnet.private.id cidr_block = aws_subnet.private.cidr_block az = aws_subnet.private.availability_zone } } } # Outputs Security Group Information # Useful to reference or modify SGs later output \u0026#34;security_group_info\u0026#34; { description = \u0026#34;Information about created security groups\u0026#34; value = { public_sg_id = aws_security_group.public_sg.id private_sg_id = aws_security_group.private_sg.id } } # Outputs EC2 Instance Information # Critical for connecting, troubleshooting, or automating follow-up tasks output \u0026#34;instance_info\u0026#34; { description = \u0026#34;Information about EC2 instances\u0026#34; value = { public_instance = { id = aws_instance.public_ec2.id public_ip = aws_instance.public_ec2.public_ip private_ip = aws_instance.public_ec2.private_ip ami_id = aws_instance.public_ec2.ami } private_instance = { id = aws_instance.private_ec2.id private_ip = aws_instance.private_ec2.private_ip ami_id = aws_instance.private_ec2.ami } } } # Outputs NAT Gateway Information # Helps confirm NAT setup and outbound address output \u0026#34;nat_gateway_info\u0026#34; { description = \u0026#34;Information about NAT Gateway\u0026#34; value = { nat_gateway_id = aws_nat_gateway.nat.id elastic_ip = aws_eip.nat.public_ip } } # Outputs VPC Flow Logs Information # Useful to pull logs, configure alerts, or analyze traffic output \u0026#34;flow_logs_info\u0026#34; { description = \u0026#34;Information about VPC Flow Logs\u0026#34; value = { log_group_name = aws_cloudwatch_log_group.vpc_logs.name log_group_arn = aws_cloudwatch_log_group.vpc_logs.arn } } # Outputs SSH Command Shortcuts # Improves usability, especially for fast testing or handover output \u0026#34;ssh_commands\u0026#34; { description = \u0026#34;SSH commands to connect to instances\u0026#34; value = { public_instance = \u0026#34;ssh -i ~/.ssh/${var.key_name}.pem ec2-user@${aws_instance.public_ec2.public_ip}\u0026#34; private_instance = \u0026#34;ssh -i ~/.ssh/${var.key_name}.pem -o ProxyJump=ec2-user@${aws_instance.public_ec2.public_ip} ec2-user@${aws_instance.private_ec2.private_ip}\u0026#34; } } Explanation:\nWell-organized and grouped outputs. Includes useful operational details (IPs, IDs, names) Provides ready-to-use SSH access tips Possible Improvements\nAdd sensitive = true to hide any sensitive values (like internal IPs or commands) from Terraform CLI output. Provide outputs formatted as maps or lists. Will be useful for scaling to multiple instances or subnets. User Data Scripts User data scripts are executed when EC2 instances boot for the first time. They provide a way to customize the instance configuration, install software, and set up initial system state. Both the public and private instances use bash scripts that perform similar initialization tasks.\npublic-userdata.sh #!/bin/bash # user-data/public-userdata.sh yum update -y yum install -y htop tree wget curl # Set hostname hostnamectl set-hostname ${hostname} echo \u0026#34;127.0.0.1 ${hostname}\u0026#34; \u0026gt;\u0026gt; /etc/hosts # Install CloudWatch agent yum install -y amazon-cloudwatch-agent # Create a welcome message cat \u0026gt; /etc/motd \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; ***************************************************** * Welcome to the Public EC2 Instance (Bastion) * * This instance has internet access and can * * be used to access private instances * ***************************************************** EOF # Log instance startup echo \u0026#34;$(date): Public instance ${hostname} started successfully\u0026#34; \u0026gt;\u0026gt; /var/log/terraform-deployment.log Explanation:\nUpdates system packages to latest versions for security. Installs essential system administration tools (htop for process monitoring, tree for directory visualization, wget and curl for downloads). Sets a dynamic hostname using the variable passed from Terraform\u0026rsquo;s templatefile function. Configures the hostname in /etc/hosts for proper local name resolution. Installs the CloudWatch agent for monitoring and log collection. Creates a Message of the Day (MOTD) that displays when users SSH into the instance, clearly identifying it as a bastion host. Logs the successful startup to a local file for troubleshooting and audit purposes. private-userdata.sh #!/bin/bash # user-data/private-userdata.sh yum update -y yum install -y htop tree wget curl # Set hostname hostnamectl set-hostname ${hostname} echo \u0026#34;127.0.0.1 ${hostname}\u0026#34; \u0026gt;\u0026gt; /etc/hosts # Install CloudWatch agent yum install -y amazon-cloudwatch-agent # Create a welcome message cat \u0026gt; /etc/motd \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; ***************************************************** * Welcome to the Private EC2 Instance * * This instance is in a private subnet and * * accessible only through the bastion host * ***************************************************** EOF # Log instance startup echo \u0026#34;$(date): Private instance ${hostname} started successfully\u0026#34; \u0026gt;\u0026gt; /var/log/terraform-deployment.log Explanation:\nPerforms identical system initialization tasks as the public instance script. Uses the same package management and tool installation approach. Sets the hostname dynamically using the Terraform-provided variable. Installs CloudWatch agent for consistent monitoring across both instances. Creates a distinct MOTD that identifies this as a private instance accessible only through the bastion host. Logs startup information to the same log file format for consistent operational tracking. Integration with Terraform Both user data scripts are referenced in the main.tf file using Terraform\u0026rsquo;s templatefile function:\nuser_data = base64encode(templatefile(\u0026#34;${path.module}/user-data/public-userdata.sh\u0026#34;, { hostname = \u0026#34;${local.name_prefix}-public\u0026#34; })) The templatefile function allows Terraform to substitute variables into the scripts at deployment time. The hostname variable is populated with the project name and environment prefix, ensuring consistent naming across the infrastructure. The base64encode function is required because AWS expects user data to be base64 encoded.\nThese scripts establish a baseline configuration for both instances, ensuring they have the necessary tools for administration and monitoring while clearly identifying their role in the infrastructure through the welcome messages.\nDeployment Workflow Utilizing Terraform Commands Initialize the Terraform project:\nterraform init Format the configuration:\nterraform fmt Validate the configuration:\nterraform validate Review the execution plan:\nterraform plan Apply the configuration to deploy resources:\nterraform apply When finished, clean up resources:\nterraform destroy Deployment Workflow Utilizing PowerShell Script This script is designed to automate and safeguard the Terraform deployment workflow. It ensures prerequisites, verifies configuration, assists the user, and executes Terraform commands in a structured, safe order.\n# Terraform Deployment Script # ========================================== # 1. PREREQUISITES CHECK # ========================================== Write-Host \u0026#34;Checking Terraform installation...\u0026#34; -ForegroundColor Yellow terraform --version if ($LASTEXITCODE -ne 0) { Write-Host \u0026#34;ERROR: Terraform not found! Install from: https://www.terraform.io/downloads\u0026#34; -ForegroundColor Red exit 1 } Write-Host \u0026#34;Checking AWS CLI...\u0026#34; -ForegroundColor Yellow aws --version if ($LASTEXITCODE -ne 0) { Write-Host \u0026#34;ERROR: AWS CLI not found! Install from: https://aws.amazon.com/cli/\u0026#34; -ForegroundColor Red exit 1 } Write-Host \u0026#34;Checking AWS credentials...\u0026#34; -ForegroundColor Yellow aws sts get-caller-identity if ($LASTEXITCODE -ne 0) { Write-Host \u0026#34;ERROR: AWS credentials not configured! Run: aws configure\u0026#34; -ForegroundColor Red exit 1 } # ========================================== # 2. PROJECT SETUP # ========================================== Write-Host \u0026#34;Checking project directory...\u0026#34; -ForegroundColor Yellow $CurrentDir = Split-Path -Leaf (Get-Location) Write-Host \u0026#34;Current directory: $CurrentDir\u0026#34; -ForegroundColor Green # Go up one level if we\u0026#39;re in user-data directory if ($CurrentDir -eq \u0026#34;user-data\u0026#34;) { Set-Location \u0026#34;..\u0026#34; $CurrentDir = Split-Path -Leaf (Get-Location) Write-Host \u0026#34;Moved up to: $CurrentDir\u0026#34; -ForegroundColor Green } # Verify Terraform files exist $TerraformFiles = @(\u0026#34;main.tf\u0026#34;, \u0026#34;variables.tf\u0026#34;, \u0026#34;outputs.tf\u0026#34;, \u0026#34;provider.tf\u0026#34;, \u0026#34;data.tf\u0026#34;, \u0026#34;locals.tf\u0026#34;) $MissingFiles = @() foreach ($file in $TerraformFiles) { if (-not (Test-Path $file)) { $MissingFiles += $file } } if ($MissingFiles.Count -gt 0) { Write-Host \u0026#34;ERROR: Missing Terraform files: $($MissingFiles -join \u0026#39;, \u0026#39;)\u0026#34; -ForegroundColor Red Write-Host \u0026#34;Make sure you\u0026#39;re in the correct directory with your .tf files!\u0026#34; -ForegroundColor Red exit 1 } Write-Host \u0026#34;SUCCESS: All Terraform files found\u0026#34; -ForegroundColor Green # Check if terraform.tfvars exists if (-not (Test-Path \u0026#34;terraform.tfvars\u0026#34;)) { Write-Host \u0026#34;Creating terraform.tfvars template...\u0026#34; -ForegroundColor Yellow $TerraformVars = @\u0026#34; # terraform.tfvars - Customize these values for your environment # Required variables (you MUST set these) key_name = \u0026#34;your-key-pair-name\u0026#34; # Replace with your EC2 key pair name home_ip = \u0026#34;203.0.113.1/32\u0026#34; # Replace with your public IP + /32 # Optional customizations project_name = \u0026#34;tillynet-vpc-lab\u0026#34; environment = \u0026#34;dev\u0026#34; vpc_cidr = \u0026#34;10.0.0.0/16\u0026#34; public_subnet_cidr = \u0026#34;10.0.1.0/24\u0026#34; private_subnet_cidr = \u0026#34;10.0.2.0/24\u0026#34; availability_zone = \u0026#34;us-west-1a\u0026#34; instance_type = \u0026#34;t2.micro\u0026#34; # DNS settings enable_dns_hostnames = true enable_dns_support = true \u0026#34;@ $TerraformVars | Out-File -FilePath \u0026#34;terraform.tfvars\u0026#34; -Encoding UTF8 Write-Host \u0026#34;SUCCESS: Created terraform.tfvars template\u0026#34; -ForegroundColor Green Write-Host \u0026#34;IMPORTANT: Edit terraform.tfvars with your actual values before proceeding!\u0026#34; -ForegroundColor Red } # Check user-data directory if (-not (Test-Path \u0026#34;user-data\u0026#34;)) { Write-Host \u0026#34;ERROR: user-data directory not found!\u0026#34; -ForegroundColor Red exit 1 } $UserDataFiles = @(\u0026#34;user-data/public-userdata.sh\u0026#34;, \u0026#34;user-data/private-userdata.sh\u0026#34;) foreach ($file in $UserDataFiles) { if (-not (Test-Path $file)) { Write-Host \u0026#34;ERROR: Missing user-data script: $file\u0026#34; -ForegroundColor Red exit 1 } } Write-Host \u0026#34;SUCCESS: User-data scripts found\u0026#34; -ForegroundColor Green # ========================================== # 3. GET PUBLIC IP # ========================================== Write-Host \u0026#34;Getting your public IP address...\u0026#34; -ForegroundColor Yellow try { $PublicIP = (Invoke-RestMethod -Uri \u0026#34;https://ifconfig.me/ip\u0026#34; -TimeoutSec 10).Trim() Write-Host \u0026#34;SUCCESS: Your public IP: $PublicIP\u0026#34; -ForegroundColor Green Write-Host \u0026#34;Make sure terraform.tfvars has: home_ip = `\u0026#34;$PublicIP/32`\u0026#34;\u0026#34; -ForegroundColor Cyan # Check if terraform.tfvars needs updating $TfVarsContent = Get-Content \u0026#34;terraform.tfvars\u0026#34; -Raw if ($TfVarsContent -match \u0026#34;203\\.0\\.113\\.1/32\u0026#34;) { Write-Host \u0026#34;WARNING: terraform.tfvars still has placeholder IP - update it!\u0026#34; -ForegroundColor Yellow } } catch { Write-Host \u0026#34;ERROR: Could not get public IP. Get it from: https://whatismyipaddress.com/\u0026#34; -ForegroundColor Red } # ========================================== # 4. TERRAFORM DEPLOYMENT # ========================================== Write-Host \u0026#34;`nSTARTING TERRAFORM DEPLOYMENT\u0026#34; -ForegroundColor Magenta Write-Host \u0026#34;======================================\u0026#34; -ForegroundColor Magenta # Function to run Terraform commands function Invoke-TerraformCommand { param( [string]$Command, [string]$Description ) Write-Host \u0026#34;`n$Description\u0026#34; -ForegroundColor Yellow Write-Host \u0026#34;Running: terraform $Command\u0026#34; -ForegroundColor Gray $StartTime = Get-Date Invoke-Expression \u0026#34;terraform $Command\u0026#34; $EndTime = Get-Date $Duration = $EndTime - $StartTime if ($LASTEXITCODE -eq 0) { Write-Host \u0026#34;SUCCESS: $Description completed (took $($Duration.TotalSeconds) seconds)\u0026#34; -ForegroundColor Green return $true } else { Write-Host \u0026#34;ERROR: $Description failed!\u0026#34; -ForegroundColor Red Write-Host \u0026#34;Check the output above for error details.\u0026#34; -ForegroundColor Red return $false } } # Pre-deployment validation Write-Host \u0026#34;`nValidating configuration...\u0026#34; -ForegroundColor Yellow $TfVarsContent = Get-Content \u0026#34;terraform.tfvars\u0026#34; -Raw if ($TfVarsContent -match \u0026#34;your-key-pair-name\u0026#34;) { Write-Host \u0026#34;ERROR: terraform.tfvars still has placeholder key_name!\u0026#34; -ForegroundColor Red Write-Host \u0026#34;Create a key pair in AWS Console: EC2 -\u0026gt; Key Pairs -\u0026gt; Create\u0026#34; -ForegroundColor Yellow exit 1 } # Step 1: Initialize if (-not (Invoke-TerraformCommand \u0026#34;init\u0026#34; \u0026#34;Initializing Terraform\u0026#34;)) { exit 1 } # Step 2: Validate if (-not (Invoke-TerraformCommand \u0026#34;validate\u0026#34; \u0026#34;Validating Terraform configuration\u0026#34;)) { exit 1 } # Step 3: Format Invoke-TerraformCommand \u0026#34;fmt\u0026#34; \u0026#34;Formatting Terraform code\u0026#34; # Step 4: Plan Write-Host \u0026#34;`nCreating Terraform plan...\u0026#34; -ForegroundColor Yellow Write-Host \u0026#34;This shows what resources will be created (NEW VPC + subnets + EC2s).\u0026#34; -ForegroundColor Gray if (-not (Invoke-TerraformCommand \u0026#34;plan -out=tfplan\u0026#34; \u0026#34;Planning deployment\u0026#34;)) { exit 1 } # Step 5: Confirm Write-Host \u0026#34;`nReady to deploy infrastructure?\u0026#34; -ForegroundColor Yellow Write-Host \u0026#34;This will create a NEW VPC with all resources (NAT Gateway costs about $32/month).\u0026#34; -ForegroundColor Red Write-Host \u0026#34;Resources to be created:\u0026#34; -ForegroundColor Cyan Write-Host \u0026#34;- New VPC (10.0.0.0/16)\u0026#34; -ForegroundColor White Write-Host \u0026#34;- Internet Gateway\u0026#34; -ForegroundColor White Write-Host \u0026#34;- Public and Private Subnets\u0026#34; -ForegroundColor White Write-Host \u0026#34;- NAT Gateway (about $32/month)\u0026#34; -ForegroundColor White Write-Host \u0026#34;- 2 EC2 instances (t2.micro - free tier)\u0026#34; -ForegroundColor White Write-Host \u0026#34;- Security Groups, Route Tables, VPC Flow Logs\u0026#34; -ForegroundColor White $Confirmation = Read-Host \u0026#34;Type \u0026#39;yes\u0026#39; to proceed with deployment\u0026#34; if ($Confirmation -eq \u0026#34;yes\u0026#34;) { # Step 6: Apply if (Invoke-TerraformCommand \u0026#34;apply tfplan\u0026#34; \u0026#34;Applying Terraform plan\u0026#34;) { Write-Host \u0026#34;`nDEPLOYMENT SUCCESSFUL!\u0026#34; -ForegroundColor Green Write-Host \u0026#34;======================================\u0026#34; -ForegroundColor Green # Show outputs Write-Host \u0026#34;`nInfrastructure Details:\u0026#34; -ForegroundColor Cyan terraform output # Cleanup Remove-Item \u0026#34;tfplan\u0026#34; -ErrorAction SilentlyContinue Write-Host \u0026#34;`nNext Steps:\u0026#34; -ForegroundColor Yellow Write-Host \u0026#34;1. Check AWS Console to see created resources\u0026#34; -ForegroundColor White Write-Host \u0026#34;2. Use SSH commands from output to connect to instances\u0026#34; -ForegroundColor White Write-Host \u0026#34;3. Test connectivity: Public -\u0026gt; Private instance\u0026#34; -ForegroundColor White Write-Host \u0026#34;4. When done testing, run: terraform destroy\u0026#34; -ForegroundColor White Write-Host \u0026#34;`nUseful AWS Console Links:\u0026#34; -ForegroundColor Yellow Write-Host \u0026#34;- VPC Dashboard: https://console.aws.amazon.com/vpc/\u0026#34; -ForegroundColor White Write-Host \u0026#34;- EC2 Dashboard: https://console.aws.amazon.com/ec2/\u0026#34; -ForegroundColor White Write-Host \u0026#34;- CloudWatch Logs: https://console.aws.amazon.com/cloudwatch/\u0026#34; -ForegroundColor White } } else { Write-Host \u0026#34;Deployment cancelled by user.\u0026#34; -ForegroundColor Red Remove-Item \u0026#34;tfplan\u0026#34; -ErrorAction SilentlyContinue } # ========================================== # 5. HELPFUL COMMANDS # ========================================== Write-Host \u0026#34;`nUSEFUL TERRAFORM COMMANDS:\u0026#34; -ForegroundColor Magenta Write-Host \u0026#34;======================================\u0026#34; -ForegroundColor Magenta Write-Host \u0026#34;terraform show # Show current state\u0026#34; -ForegroundColor White Write-Host \u0026#34;terraform output # Show outputs again\u0026#34; -ForegroundColor White Write-Host \u0026#34;terraform state list # List all resources\u0026#34; -ForegroundColor White Write-Host \u0026#34;terraform plan # Plan changes\u0026#34; -ForegroundColor White Write-Host \u0026#34;terraform apply # Apply changes\u0026#34; -ForegroundColor White Write-Host \u0026#34;terraform destroy # Delete all resources\u0026#34; -ForegroundColor White Write-Host \u0026#34;`nTROUBLESHOOTING:\u0026#34; -ForegroundColor Magenta Write-Host \u0026#34;======================================\u0026#34; -ForegroundColor Magenta Write-Host \u0026#34;- Error about credentials: Run \u0026#39;aws configure\u0026#39;\u0026#34; -ForegroundColor White Write-Host \u0026#34;- Error about regions: Check availability_zone variable\u0026#34; -ForegroundColor White Write-Host \u0026#34;- Error about key pairs: Create key pair in EC2 console first\u0026#34; -ForegroundColor White Write-Host \u0026#34;- Error about permissions: Ensure your AWS user has admin rights\u0026#34; -ForegroundColor White Write-Host \u0026#34;`nDeployment script completed!\u0026#34; -ForegroundColor Green 1. Prerequisite Checks Checks if:\nTerraform is installed (terraform --version) AWS CLI is installed (aws --version) AWS CLI credentials are configured (aws sts get-caller-identity) If any of these checks fail, the script exits early to prevent wasted runs.\nPrevents runtime errors later by confirming the local environment is ready. 2. Project Directory Setup Checks if you\u0026rsquo;re inside the terraform-vpc-lab directory. If you\u0026rsquo;re in the user-data subfolder, moves up one level. Verifies critical Terraform files are present (main.tf, variables.tf, etc.). Creates a terraform.tfvars template if it doesn\u0026rsquo;t exist, reminding the user to fill in actual values. Prevents mistakes from running Terraform in the wrong folder or with missing config.\n3. Public IP Check Retrieves the users current public IP using an external API https://ifconfig.me/ip. Alerts if the placeholder 203.0.113.1/32 is still in your terraform.tfvars. Ensures Security Group home_ip is set properly thus allowing SSH in.\n4. Terraform Deployment Steps Uses a helper function Invoke-TerraformCommand that:\nRuns a Terraform command.\nTracks execution time.\nChecks success or failure.\nProvides colorful, clear user feedback.\nCommands it runs in sequence:\nterraform init → initializes backend + providers.\nterraform validate → checks configuration syntax.\nterraform fmt → formats the Terraform code.\nterraform plan -out=tfplan → creates an executable plan file.\nPrompts the user:\nExplicit user confirmation before applying. terraform apply tfplan → deploys the infrastructure.\nAfter apply:\nShows terraform output.\nCleans up the tfplan file.\nLists helpful AWS console links.\nAutomates best practices and reduces human error in the apply phase.\n5. Helpful Commands + Troubleshooting At the end, it prints:\nCommon Terraform CLI commands for post-deployment work.\nTroubleshooting tips for:\nCredential issues\nRegion mismatches\nKey pair errors\nPermissions problems\nGives the user guidance on what to do after deployment or if something goes wrong.\nWhy Utilize PowerShell Deployment Clear, user-friendly feedback using color and prompts. Safety checks to prevent misconfigured runs. Scripted generation of tfvars template to help beginners. Modular Terraform command function for reusability. Includes time tracking and success/failure detection. Potential Improvements Add error handling for:\nNetwork timeouts when fetching public IP. terraform command not found inside restricted environments. Support for custom -var-file input if multiple environments (dev, prod). Support an optional terraform destroy flag or mode for teardown automation. Log output to a file for post-run audit. Best Practices and Notes Use environment variables or terraform.tfvars to avoid hardcoding sensitive values (like your home IP) in .tf files.\nAlways run terraform plan before apply to review changes.\nEnsure your AMI IDs are up-to-date for your target region.\nKeep .tfstate files secure, especially when using sensitive outputs.\nUse .gitignore to exclude sensitive files if committing the project to version control.\n","permalink":"https://blog.tillynet.com/cloud-engineering-labs/aws-lab-1/building-secure-aws-infrastructure-with-terraform---complete-lab-guide/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eThis document summarizes the technical work completed to build, configure, and automate a secure AWS VPC lab using Terraform. It covers manual and automated infrastructure deployment steps, with detailed explanations for each Terraform configuration component, designed to help replicate the setup at any time in the future.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"objectives\"\u003eObjectives\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eBuild a segmented AWS VPC network with public and private subnets.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLaunch EC2 instances in both public and private subnets.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSecure access using Security Groups.\u003c/p\u003e","title":"Building Secure AWS Infrastructure with Terraform - Complete Lab Guide"},{"content":"Overview This post documents how I implemented a production-style Public Key Infrastructure (PKI) inside my Samba 4 Active Directory (AD) lab. The project simulates how enterprise environments separate certificate authority responsibilities to promote security and manageability. I built a trust chain starting with an offline Root CA, promoted my Samba DC as an Intermediate CA, and automated service certificate signing for applications like Traefik and Authentik.\nWhy a PKI Chain of Trust? In enterprise networks, it\u0026rsquo;s a best practice to:\nKeep the Root Certificate Authority (CA) offline to minimize risk\nDelegate signing authority to an Intermediate CA\nSign service certificates (LDAPS, HTTPS, etc.) from the Intermediate\nInitially, my Samba AD DC handled both Root CA and service cert issuance, which is not secure long-term. This project restructures that into a hardened, tiered architecture.\nPKI Topology Root CA (Offline)\r└── Intermediate CA (Samba DC)\r├── samba-ldaps.crt\r├── authentik.tld\r└── traefik.tld Phase 1: Offline Root CA This was done on a separate offline VM or system.\nmkdir -p ~/rootCA/{certs,crl,newcerts,private,csr} touch ~/rootCA/index.txt echo 1000 \u0026gt; ~/rootCA/serial chmod 700 ~/rootCA/private Generate the Root CA key and certificate:\nopenssl genrsa -aes256 -out ~/rootCA/private/rootCA.key.pem 4096 openssl req -x509 -new -key ~/rootCA/private/rootCA.key.pem \\ -sha256 -days 3650 -out ~/rootCA/certs/rootCA.cert.pem \\ -subj \u0026#34;/C=US/ST=State/O=Lab/OU=PKI/CN=Root CA\u0026#34; Phase 2: Intermediate CA (Samba DC) On the Samba DC, generate the Intermediate CA private key and CSR:\nmkdir -p /usr/local/samba/private/pki/intermediate cd /usr/local/samba/private/pki/intermediate openssl genrsa -out intermediate.key.pem 4096 openssl req -new -key intermediate.key.pem \\ -out intermediate.csr.pem \\ -subj \u0026#34;/C=US/ST=State/O=Lab/OU=CA/CN=Intermediate CA\u0026#34; Transfer the CSR to the offline Root CA host, then:\nopenssl ca -config openssl_root.cnf \\ -extensions v3_intermediate_ca \\ -days 1825 -notext -md sha256 \\ -in csr/intermediate.csr.pem \\ -out certs/intermediate.cert.pem Create the chain:\ncat certs/intermediate.cert.pem certs/rootCA.cert.pem \u0026gt; certs/ca-chain.cert.pem Transfer intermediate.cert.pem and ca-chain.cert.pem back to the Samba server.\nSamba TLS Configuration Save the following files:\n/usr/local/samba/private/tls/\r├── intermediate.key\r├── intermediate.crt\r├── ca-chain.crt Edit smb.conf:\n[global]\rtls enabled = yes\rtls keyfile = /usr/local/samba/private/tls/intermediate.key\rtls certfile = /usr/local/samba/private/tls/intermediate.crt\rtls cafile = /usr/local/samba/private/tls/ca-chain.crt Restart the Samba service:\nsystemctl restart samba-ad-dc Test it:\nopenssl s_client -connect samba.domain.lan:636 -showcerts Service Certificate Automation To issue certs for services like Traefik and Authentik, I wrote a bash script:\n/usr/local/samba/private/tls/sign_service_cert.sh Script Highlights #!/bin/bash FQDN=\u0026#34;$1\u0026#34; CERT_DIR=\u0026#34;/usr/local/samba/private/tls/$FQDN\u0026#34; mkdir -p \u0026#34;$CERT_DIR\u0026#34; openssl req -new -nodes -newkey rsa:2048 \\ -keyout \u0026#34;$CERT_DIR/$FQDN.key\u0026#34; \\ -out \u0026#34;$CERT_DIR/$FQDN.csr\u0026#34; \\ -subj \u0026#34;/CN=$FQDN\u0026#34; \\ -config \u0026lt;(cat \u0026lt;\u0026lt;EOF [ req ] default_bits = 2048 prompt = no default_md = sha256 req_extensions = v3_req distinguished_name = dn [ dn ] CN = $FQDN [ v3_req ] basicConstraints = CA:FALSE keyUsage = digitalSignature, keyEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [ alt_names ] DNS.1 = $FQDN EOF ) openssl x509 -req \\ -in \u0026#34;$CERT_DIR/$FQDN.csr\u0026#34; \\ -CA /usr/local/samba/private/tls/intermediate.crt \\ -CAkey /usr/local/samba/private/tls/intermediate.key \\ -CAcreateserial \\ -out \u0026#34;$CERT_DIR/$FQDN.crt\u0026#34; \\ -days 825 -sha256 \\ -extfile \u0026lt;(cat \u0026lt;\u0026lt;EOF [ v3_req ] basicConstraints = CA:FALSE keyUsage = digitalSignature, keyEncipherment extendedKeyUsage = serverAuth subjectAltName = @alt_names [ alt_names ] DNS.1 = $FQDN EOF ) cat \u0026#34;$CERT_DIR/$FQDN.crt\u0026#34; /usr/local/samba/private/tls/ca-chain.crt \u0026gt; \u0026#34;$CERT_DIR/$FQDN-fullchain.crt\u0026#34; Usage:\n./sign_service_cert.sh authentik.domain.lan Traefik + Authentik Certificate Integration Problem The authentik outpost container failed to connect due to:\ntls: failed to verify certificate: x509: certificate signed by unknown authority Solution: Dockerfile CA Injection Create a Dockerfile to embed the Root CA into the outpost container:\nFROM ghcr.io/goauthentik/proxy:latest\rUSER root\rCOPY ./certs/rootCA.crt /usr/local/share/ca-certificates/rootCA.crt\rRUN apt update \u0026amp;\u0026amp; apt install -y ca-certificates \u0026amp;\u0026amp; update-ca-certificates\rUSER 1000 Build the container:\ndocker compose build --no-cache authentik-outpost Verify it inside the running container:\ndocker exec -it traefik-authentik-outpost bash openssl s_client -connect authentik.domain.lan:443 -CAfile /etc/ssl/certs/ca-certificates.crt You should see:\nVerify return code: 0 (ok) Active Directory GPO To deploy trust across my Windows clients, I used Group Policy to distribute the Root CA certificate only, not the entire chain. This prevents clients from implicitly trusting intermediate certificates not issued by the Root.\nTo deploy:\nOpen Group Policy Management Console\nCreate or edit a GPO\nNavigate to: Computer Configuration \u0026gt; Policies \u0026gt; Windows Settings \u0026gt; Security Settings \u0026gt; Public Key Policies \u0026gt; Trusted Root Certification Authorities\nImport rootCA.cert.pem\nAfter syncing (gpupdate /force), all domain-joined clients should trust the Root.\nSummary This project resulted in a secure, enterprise-style certificate infrastructure for my home lab:\nHardened Root CA hosted offline\nSamba promoted to Intermediate CA\nSigned certificates for LDAP, Traefik, and Authentik\nTrusted CA rolled out via AD GPO\nDocker containers extended to trust my internal CA\nThis foundation supports advanced secure services like LDAPS, OIDC, and internal HTTPS endpoints with verified trust.\n","permalink":"https://blog.tillynet.com/on-premise-engineering-labs/building-a-proper-pki-chain-of-trust-in-a-samba-ad-lab/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eThis post documents how I implemented a production-style Public Key Infrastructure (PKI) inside my Samba 4 Active Directory (AD) lab. The project simulates how enterprise environments separate certificate authority responsibilities to promote security and manageability. I built a trust chain starting with an offline Root CA, promoted my Samba DC as an Intermediate CA, and automated service certificate signing for applications like Traefik and Authentik.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"why-a-pki-chain-of-trust\"\u003eWhy a PKI Chain of Trust?\u003c/h2\u003e\n\u003cp\u003eIn enterprise networks, it\u0026rsquo;s a best practice to:\u003c/p\u003e","title":"Building a Proper PKI Chain of Trust in a Samba AD Lab"},{"content":"Introduction This guide details how I provisioned Authentik as a middleware authentication provider integrated with Traefik reverse proxy, using TLS certificates issued by my internal Samba Active Directory Certificate Authority (CA). The result is a secure, SSO-enabled reverse proxy setup that leverages LDAPS to enforce access control for authorized users in my Active Directory.\nThis solution enables:\nTrusted HTTPS access to services proxied through Traefik.\nSSO enforcement using Authentik via the forwardAuth middleware.\nCertificate-based trust rooted in my Samba 4 AD CA.\nAuthentik outpost (proxy) deployment as a sidecar container to manage authentication flows.\nPrerequisites Working Traefik reverse proxy deployed via Docker Compose\nAuthentik server accessible at https://authentik.example.lan\nValid internal CA-signed certificates for both Traefik and Authentik\nSamba 4 running as internal CA with LDAPS enabled\nDocker and Docker Compose installed\nDNS records pointing to correct internal service IPs (e.g., traefik.example.lan)\nAuthentik Setup Notes Authentik was configured to use LDAPS for user directory integration against Samba AD.\nUsers from a specific group (e.g., AuthorizedSSOUsers) were assigned access to the Traefik application.\nAn embedded outpost was initially tested, but the dedicated container provided cleaner integration.\nDirectory Layout 📁 Directory Layout /home/username/traefik-authentik/ ├── docker-compose.yml # Main Docker Compose file ├── Dockerfile # Custom Dockerfile for the Authentik outpost ├── .env # (Optional) Environment variables file ├── certs/ # Internal CA and TLS certs │ ├── ca.crt # Internal Samba AD CA certificate (PEM format) │ ├── authentik.example.lan.crt # Authentik SSL certificate │ ├── authentik.example.lan.key # Authentik SSL key │ ├── traefik.example.lan.crt # Traefik dashboard SSL certificate │ └── traefik.example.lan.key # Traefik dashboard SSL key ├── traefik/ │ ├── traefik.yml # Static Traefik configuration │ ├── middleware.yml # Middleware definition for Authentik SSO │ └── tls.yml # TLS options and certificate stores Notes: The certs/ folder is mounted into containers to provide all necessary trusted CA and TLS materials. All certificates must be in PEM format. The Dockerfile should use a multi-stage approach or add the CA and call update-ca-certificates as root. The traefik/ folder holds Traefik\u0026rsquo;s dynamic configuration files. You can substitute /home/username/traefik-authentik/ with any appropriate directory, but all relative volume mounts in docker-compose.yml should align. 1. Traefik Configuration Overview traefik.yml global: checkNewVersion: false sendAnonymousUsage: false accessLog: {} log: level: TRACE api: dashboard: true insecure: false debug: false entryPoints: web: address: :80 http: redirections: entryPoint: to: websecure scheme: https websecure: address: :443 serversTransport: insecureSkipVerify: true # Useful during internal CA testing phase providers: docker: exposedByDefault: false endpoint: \u0026#39;unix:///var/run/docker.sock\u0026#39; watch: true file: directory: /etc/traefik/conf/ watch: true Global Settings -global: root-level settings for global Traefik behavior\ncheckNewVersion: false: Disables automatic checking for new Traefik versions (I prefer to update manually) sendAnonymousUsage: false: Prevents ending anonymous usage statistics to Traefik devs AccessLog Configuration accessLog: {}: Enables access logging with default settings (empty config block) General Logging Configuration log.level: TRACE: Sets the logging level to the most verbose (TRACE), useful for debugging and troubleshooting API and Dashboard api.dasbhoard: true: Enables the Traefik web dashboard api.insecure: false: Dashboard is only accessible via a secure entry point (not exposed on HTTP) api.debug: false: Disables the debug endpoint, which can expose sensitive info Entry Points (Ports) entryPoints: Defines how Traefik listens for incoming traffic web.address: :80: Listens on port 80 (HTTP) http.redirections.entryPoint.to: websecure: Redirects all HTTP traffic to HTTPS scheme: https: Specifies the protocol for redirection websecure.address: :443: Listens on port 443 (HTTPS) TLS Transport Options serversTransport.insecureSkipVerify: true: Disables certificate verification when Traefik communicates with backend services. Use only for internal services or testing, as it bypasses TLS validation. Providers providers.docker: Enables Docker as a dynamic configuration provider.\nexposedByDefault: false: Services are not automatically exposed to Traefik unless explicitly enabled via labels.\nendpoint: Communicates with Docker through the local Unix socket.\nwatch: true: Automatically reloads config when Docker changes (e.g., containers start/stop).\nproviders.file: Enables static/dynamic config from a file directory.\ndirectory: /etc/traefik/conf/: Path to directory containing additional dynamic configuration files (like routers, middlewares, TLS certs).\nwatch: true: Traefik watches this directory for live changes and reloads as needed.\nconf/middleware.yml http: middlewares: authentik: forwardAuth: address: http://NAME-OF-AUTHENTIK-OUTPOST:9000/outpost.goauthentik.io/auth/traefik trustForwardHeader: true authResponseHeaders: - X-authentik-username - X-authentik-groups - X-authentik-entitlements - X-authentik-email - X-authentik-name - X-authentik-uid - X-authentik-jwt - X-authentik-meta-jwks - X-authentik-meta-outpost - X-authentik-meta-provider - X-authentik-meta-app - X-authentik-meta-version This middleware.yml file configures Traefik to forward requests to the Authentik outpost container to enforce authentication and pass key user identity headers.\nhttp: This is the root section for all HTTP-related dynamic configuration in Traefik (e.g., routers, services, middlewares).\nmiddlewares: Begins the definition of one or more middleware objects that can be referenced by routers.\nauthentik: The name you\u0026rsquo;ve assigned to this middleware. This will be used when attaching it to a router using the label traefik.http.routers.\u0026lt;name\u0026gt;.middlewares=authentik@file.\nforwardAuth: Specifies that this middleware uses a forward authentication model, where Traefik sends each incoming request to an external authentication server (in this case, Authentik) before passing it to the backend service.\naddress: The URL that Traefik will forward incoming requests to for authentication.\nThis must point to the Authentik Outpost service’s endpoint that is designed to work with Traefik as a forwardAuth provider. trustForwardHeader: true: Tells Traefik to forward X-Forwarded-* headers from the client (e.g., real client IP, host info). This is important when the authentication server (Authentik) needs those headers to make access decisions.\nauthResponseHeaders: This section lists headers returned from Authentik that should be passed through to the final backend service.\nThese headers typically contain user metadata, group memberships, entitlements, and JWT tokens. They\u0026rsquo;re essential for the application to know who is logged in and what they\u0026rsquo;re authorized to do.\nconf/tls.yml tls: certificates - certFile: /var/traefik/certs/traefik.example.lan.crt keyFile: /var/traefik/certs/traefik.example.lan.key - certFile: /var/traefik/certs/authentik.example.lan.crt keyFile: /var/traefik/certs/authentik.example.lan.key stores: default: defaultCertificate: certFile: /var/traefik/certs/traefik.example.lan.crt keyFile: /var/traefik/certs/traefik.example.lan.key options: default: minVersion: VersionTLS12 sniStrict: true curvePreferences: - CurveP256 - CurveP384 - CurveP521 cipherSuites: - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 - TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256 - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 The tls.yml file Specifies TLS certificate and key files signed by the internal Samba CA.\nSummary This configuration:\nDefines and serves multiple domain-specific certificates.\nUses strong security defaults (TLS 1.2+, strict SNI, PFS ciphers).\nEnsures fallback TLS service via the default certificate store.\nRoot Key tls: Root section for configuring TLS certificates, stores, and security options. TLS Certificates certificates: List of TLS certificates that Traefik will serve.\ncertFile: Path to the public certificate file (PEM format).\nkeyFile: Path to the associated private key.\nEach certificate is matched against incoming requests by their SNI (Server Name Indication) hostname (e.g., traefik.example.lan, authentik.example.lan).\nDefault Certificate Store stores: Defines named TLS stores. The default store is used when no matching SNI is found. defaultCertificate: A fallback certificate that Traefik will use if no specific certificate matches the request\u0026rsquo;s hostname. TLS Security Options options: Defines reusable TLS options (like security policies).\ndefault: The default TLS policy to apply if not overridden.\nminVersion: VersionTLS12: Only allow TLS 1.2 or higher (disables older, insecure TLS versions).\nsniStrict: true: Requires that a matching certificate for the SNI is present, otherwise Traefik will reject the connection.\nCurve Preferences curvePreferences: Preferred elliptic curves for ECDHE (Elliptic Curve Diffie-Hellman Ephemeral) key exchange.\nThis list defines the order of preference for stronger cryptographic curves.\nCipher Suites cipherSuites: Explicitly defines the allowed cipher suites for TLS 1.2 connections (TLS 1.3 has its own fixed ciphers).\nSuites here support strong AEAD encryption and PFS (Perfect Forward Secrecy) via ECDHE.\n2. Docker Compose Configuration docker-compose.yml Create a unified docker-compose.yml that includes both traefik and the authentik-outpost:\nservices: traefik: container_name: traefik image: traefik:latest ports: - 80:80 - 443:443 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro - filepathto/traefik.yml:/etc/traefik/traefik.yml:ro - filepathto/traefik/config/:/etc/traefik/conf:ro - filepathto/traefik/certs/:/var/traefik/certs/:rw networks: - frontend - backend labels: - traefik.enable=true #enables Traefik reverse proxy for this container - traefik.http.routers.traefik.rule=Host(`traefik.example.lan`) #defines a router that activates when requests match the domain specified - traefik.http.routers.traefik.entrypoints=websecure #binds the router to HTTPS (websecure) entry point - traefik.http.routers.traefik.tls=true #enables TLS for this router - traefik.http.routers.traefik.middlewares=authentik@file #applies the middleware named authentik from the file provider (in my case middleware.yml) for auth - traefik.http.routers.traefik.service=api@internal #tells Traefik to serve its internal API/dasbhoard when this route is hit restart: unless-stopped authentik-outpost: container_name: traefik-authentik-outpost image: custom-authentik-outpost #use custom image to load trusted internal CA cert into container volumes: - filepathto/certs:/certs:ro restart: unless-stopped networks: - frontend - backend environment: - AUTHENTIK_HOST=https://authentik.example.lan - AUTHENTIK_INSECURE=false #requires valid HTTPS connection (doesn\u0026#39;t skip tls checks) - REQUESTS_CA_BUNDLE=/certs/ca.crt #instructs Python-based tools (used by Authentik Proxy) to trust internal CA for TLS - LOG_LEVEL=debug - AUTHENTIK_HOST_BROWSER=https://authentik.example.lan #used when public and internal addresses differ - AUTHENTIK_TOKEN=xxxxxxxxxxxxxxx # Replace with Authentik outpost token. Token is necessary to authenticate the outpost with the Authentik server. DO NOT EXPOSE networks: frontend: external: true backend: external: true Highlights:\nTraefik loads configuration from mounted YAML files.\nAuthentik outpost uses the internal CA to validate Authentik\u0026rsquo;s certificate.\nIf AUTHENTIK_INSECURE=false fails due to CA trust, temporarily flip it to true.\nNote: Replace custom-authentik-outpost with the name of the custom image (see next step).\n3. Custom Dockerfile for Outpost (Trust Internal CA) To resolve certificate validation issues, a custom image was created by appending your Samba AD CA to the container\u0026rsquo;s trust store:\nDockerfile for Custom Outpost Image To trust my internal Samba CA, I built a custom image for the Authentik proxy outpost:\nDockerfile \u0026ndash; name the file exactly like this\nFROM ghcr.io/goauthentik/proxy:latest\rCOPY ./certs/ca.crt /usr/local/share/ca-certificates/ca.crt\rRUN update-ca-certificates Then build the file with:\nsudo docker build -t custom-authentik-outpost . This step ensured the Authentik outpost container would trust my internal certificate hierarchy.\n4. Authentik Configuration In the Authentik dashboard:\nApplication Name: traefik-dashboard\nSlug: traefik-dashboard\nLaunch URL: https://traefik.example.lan/dashboard/\nProvider: The created proxy provider (in my case traefik-forward-auth)\nPolicy engine mode: any\nProvider (Proxy) Type: Proxy\nName: traefik-forward-auth\nAuthorization flow: default-provider-authorization-explicit-consent (Authorize Application)\nUse forward auth (single application)\nthe external host address should be the FQDN that you\u0026rsquo;ll want to access the application at in my case that is: https://traefik.tillynet.lan since I want to protect my Traefik dashboard with Authentik Token Validity: hours=24\nOutpost Create a new Authentik Outpost to deploy in the same docker compose config file as traefik (see the traefik docker-compose.yml above for deploying the outpost as a docker container)\nName: traefik-authentik-outpost\nType: Proxy\nIntegration: Local Docker connection\nApplications: Link your traefik-dashboard application\nToken: After outpost copy your Authentik token and paste it in docker-compose.yml\nHealthy check: Ensure Authentik sees outpost as healthy after the docker container for the outpost has been deployed\n6. Start the Stack docker compose up -d You should now be able to access:\nhttps://traefik.example.lan/dashboard/ — protected by Authentik SSO 6. Troubleshooting Tips Ensure your internal CA is correctly mounted and trusted in the custom container.\nIf you see x509: certificate signed by unknown authority, the CA is likely missing.\nUse docker logs for both Traefik and Outpost to trace errors.\nCheck firewall/NAT between 172.x Docker bridge and your internal CA if connection reset by peer occurs. MAKE SURE YOU ARE NOT USING THE DEFAULT DOCKER NETWORK.\n7. ## Final Outcome You should now be able to:\nVisit https://traefik.example.lan/dashboard/\nBe redirected to Authentik SSO\nLog in via your authorized AD credentials using LDAPS\nGain access to the secured dashboard with a valid TLS certificate\nThis project brought together reverse proxying, identity management, and internal PKI in a way that’s reproducible and production-ready for any homelab setup.\n","permalink":"https://blog.tillynet.com/on-premise-engineering-labs/provisioning-authentik-middleware-with-traefik-reverse-proxy-using-internal-samba-ad-ca/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis guide details how I provisioned \u003cstrong\u003eAuthentik\u003c/strong\u003e as a middleware authentication provider integrated with \u003cstrong\u003eTraefik\u003c/strong\u003e reverse proxy, using TLS certificates issued by my internal \u003cstrong\u003eSamba Active Directory Certificate Authority (CA)\u003c/strong\u003e. The result is a secure, SSO-enabled reverse proxy setup that leverages \u003cstrong\u003eLDAPS\u003c/strong\u003e to enforce access control for authorized users in my Active Directory.\u003c/p\u003e\n\u003cp\u003eThis solution enables:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eTrusted HTTPS access to services proxied through Traefik.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eSSO enforcement using Authentik via the forwardAuth middleware.\u003c/p\u003e","title":"Provisioning Authentik Middleware with Traefik Reverse Proxy Using Internal Samba AD CA"},{"content":"Overview This guide documents the process of deploying the Traefik reverse proxy using Docker Compose with TLS termination handled by a self-hosted internal certificate authority (CA). It includes all key configuration decisions, troubleshooting steps, and final validation outcomes.\nObjectives Deploy Traefik using Docker Compose\nEnable HTTPS via static and dynamic configuration\nLoad a custom certificate signed by an internal CA\nValidate secure access to the Traefik dashboard\nEnvironment Operating System: Ubuntu Server\nTraefik Version: 2.11\nDocker \u0026amp; Docker Compose\nInternal PKI: Self-hosted CA issuing trusted certificates\nDomain: Custom internal domain (e.g., *.example.lan)\nDirectory Structure ~/traefik/ ├── traefik.yml # Static configuration ├── docker-compose.yml # Docker service definition ├── config/ │ └── dynamic.yml # Dynamic configuration └── certs/ ├── example.lan.crt # Server certificate ├── example.lan.key # Private key └── ca.crt # Root CA certificate (optional for clients) Step-by-Step Configuration 1. Static Configuration (traefik.yml) entryPoints: web: address: \u0026#34;:80\u0026#34; websecure: address: \u0026#34;:443\u0026#34; api: dashboard: true log: level: DEBUG providers: file: filename: /home/user/traefik/config/dynamic.yml watch: true 2. Dynamic Configuration (config/dynamic.yml) tls: certificates: - certFile: /home/user/traefik/certs/example.lan.crt keyFile: /home/user/traefik/certs/example.lan.key http: routers: traefik-dashboard: rule: \u0026#34;Host(`traefik.example.lan`)\u0026#34; entryPoints: - websecure service: api@internal tls: true 3. Docker Compose File (docker-compose.yml) version: \u0026#34;3.8\u0026#34; services: traefik: image: traefik:v2.11 container_name: traefik restart: unless-stopped ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; volumes: - ./traefik.yml:/home/user/traefik/traefik.yml - ./config/dynamic.yml:/home/user/traefik/config/dynamic.yml - ./certs:/home/user/traefik/certs Note: All file paths are absolute within the container for consistency.\nTroubleshooting Process Problem: Curl to HTTPS endpoint failed (Connection Refused) Symptoms:\ncurl -vk https://traefik.example.lan returns connection refused.\nPort 443 shown as open via ss -tuln, but no container binding occurred.\nResolutions Attempted:\nVerified ports 80/443 availability (netstat, lsof, ss)\nEnsured docker-compose down fully removed container states\nRestarted Docker service to release potentially held ports\nProblem: No Traefik logs visible Symptoms: docker logs traefik showed no output\nFix: Added log.level: DEBUG to traefik.yml and confirmed config was mounted properly\nProblem: Dashboard loads with default self-signed certificate Symptoms: Dashboard displayed a browser warning for TRAEFIK DEFAULT CERT\nFix:\nVerified dynamic config was correctly referenced and mounted\nConfirmed cert and key filenames were correct\nRestarted Traefik after changing mount paths to match container expectations\nFinal Fix: Proper Mounting and Configuration Paths All paths in the YAML files were made fully absolute and consistently mounted into the container\nDocker Compose volumes were validated against container paths\nAfter restarting the container stack, the browser showed the correct certificate issued by the internal CA\nFinal Validation Verified TLS certificate via browser: matched CN=traefik.example.lan, signed by internal root CA\nAccessed dashboard via https://traefik.example.lan:443\nNo browser warnings when root CA was installed in local trust store\nConclusion This deployment demonstrates a secure and customizable method to run Traefik with HTTPS backed by an internal certificate authority. The setup supports Docker-based dynamic service exposure and can serve as a foundation for SSO, mTLS, or zero-trust architectures.\nFuture steps may include:\nIntegrating with Authentik for OIDC\nAdding automatic TLS renewal via internal CA workflows\nUsing Traefik middlewares for authentication, rate-limiting, or header injection\n","permalink":"https://blog.tillynet.com/on-premise-engineering-labs/provisioning-traefik-with-docker-compose-and-tls-termination-via-internal-ca/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eThis guide documents the process of deploying the Traefik reverse proxy using Docker Compose with TLS termination handled by a self-hosted internal certificate authority (CA). It includes all key configuration decisions, troubleshooting steps, and final validation outcomes.\u003c/p\u003e\n\u003ch2 id=\"objectives\"\u003eObjectives\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDeploy Traefik using Docker Compose\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eEnable HTTPS via static and dynamic configuration\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eLoad a custom certificate signed by an internal CA\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eValidate secure access to the Traefik dashboard\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"environment\"\u003eEnvironment\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eOperating System: Ubuntu Server\u003c/p\u003e","title":"Provisioning Traefik with Docker Compose and TLS Termination via Internal CA"},{"content":"This guide documents the process of securely binding a Samba 4 Active Directory (AD) server to Authentik using LDAPS. The integration allows Authentik to use the AD as an identity source, enabling centralized authentication across applications via SAML/OIDC while Samba 4 maintains the authoritative user directory.\nOverview Samba 4 AD acts as the LDAP and Kerberos provider.\nAuthentik serves as the Identity Provider (IdP) using the LDAP source for authentication.\nLDAPS is used to securely transmit credentials between Authentik and Samba.\nPrerequisites A working Samba 4 Active Directory Domain Controller\nA running Authentik instance (Docker or native)\nDNS resolution and time synchronization between the two systems\nSamba server with LDAPS enabled and a trusted certificate\nStep 1: Enable LDAPS on Samba 4 Generate an internal CA and a server certificate for Samba: # Generate internal CA openssl genrsa -out ca.key 4096 openssl req -x509 -new -nodes -key ca.key -sha256 -days 3650 -out ca.crt # Generate Samba key and CSR openssl genrsa -out samba.key 4096 openssl req -new -key samba.key -out samba.csr # Sign server certificate openssl x509 -req -in samba.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out samba.crt -days 825 -sha256 Update /etc/samba/smb.conf: tls enabled = yes tls keyfile = /etc/samba/ssl/samba.key tls certfile = /etc/samba/ssl/samba.crt tls cafile = /etc/samba/ssl/ca.crt Restart Samba: systemctl restart samba-ad-dc Test LDAPS: openssl s_client -connect \u0026lt;samba_fqdn\u0026gt;:636 -CAfile ca.crt Step 2: Create a Bind User in Samba Create a service account in AD for Authentik to bind:\nsamba-tool user create authentik-bind Assign a strong password and note the DN (e.g., CN=authentik-bind,CN=Users,DC=example,DC=lan).\nStep 3: Upload CA to Authentik Navigate to Certificates in the Authentik admin UI.\nCreate a new certificate and upload your ca.crt.\nName it appropriately (e.g., Internal AD CA).\nStep 4: Configure LDAP Source in Authentik Go to Directory \u0026gt; LDAP Sources \u0026gt; Create.\nFill in the fields:\nServer URI: ldaps://\u0026lt;samba_fqdn\u0026gt;\nTLS Verification Certificate: Select your uploaded CA cert\nBind CN: Full DN of the bind user\nBind Password: The service account password\nBase DN: DC=example,DC=lan\nUser Object Filter: (objectClass=person)\nGroup Object Filter: (objectClass=group)\nGroup Membership Field: member\nObject Uniqueness Field: objectSid\nSelect appropriate user/group property mappings (default Active Directory mappings are recommended).\nSave and test the connection.\nStep 5: Add LDAP Source to Authentication Flow Go to Flows \u0026gt; default-authentication-flow \u0026gt; Edit.\nAdd a new Source (Login) stage.\nSelect your Samba 4 LDAP source.\nSave the flow.\nStep 6: Sync Users Navigate to Directory \u0026gt; LDAP Source.\nClick Manual Sync to import users.\nUsers should appear under Users, with their DN, UPN, and objectSid attributes visible.\nNotes Authentik does not write back to Samba AD. Any changes to user details in Authentik are local and will be overwritten on sync.\nAlways secure LDAPS using a trusted internal CA or public CA to prevent man-in-the-middle attacks.\nSyncs can be scheduled or triggered manually depending on your directory update policies.\nOutcome With this setup, Authentik now authenticates users against Samba 4 AD using secure LDAPS. Authentik remains the central SSO provider for web applications, while Samba manages users and groups.\nRelated Posts Provisioning Authentik for SSO on a Self-Hosted Ubuntu Server (Docker-Based) Provisioning Samba Active Directory Domain Controller and Windows Domain Integration\n","permalink":"https://blog.tillynet.com/on-premise-engineering-labs/integrating-samba-4-active-directory-with-authentik-via-ldaps/","summary":"\u003cp\u003eThis guide documents the process of securely binding a Samba 4 Active Directory (AD) server to Authentik using LDAPS. The integration allows Authentik to use the AD as an identity source, enabling centralized authentication across applications via SAML/OIDC while Samba 4 maintains the authoritative user directory.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eSamba 4 AD\u003c/strong\u003e acts as the LDAP and Kerberos provider.\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eAuthentik\u003c/strong\u003e serves as the Identity Provider (IdP) using the LDAP source for authentication.\u003c/p\u003e","title":"Integrating Samba 4 Active Directory with Authentik via LDAPS"},{"content":"This guide documents the step-by-step process used to provision an Authentik identity provider server on a self-hosted Ubuntu Server using Docker and Docker Compose. This setup is suitable for advanced home lab environments and follows production-grade containerization practices.\nPrerequisites A fresh or existing Ubuntu 22.04 or 24.04 LTS server. sudo privileges on the system. Static IP and DNS configuration recommended. System updates applied. Step 1: Install Docker Engine Follow the official Docker post-install guide to install and configure Docker for non-root use:\nReference: Docker Post-install Guide\n# Update and install required packages sudo apt update \u0026amp;\u0026amp; sudo apt upgrade -y sudo apt install -y ca-certificates curl gnupg # Add Docker\u0026#39;s official GPG key sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg \\ | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg # Add the Docker repository echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] \\ https://download.docker.com/linux/ubuntu \\ $(lsb_release -cs) stable\u0026#34; | sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null # Install Docker Engine sudo apt update sudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin # Optional: Add your user to the docker group to avoid using sudo with every docker command sudo usermod -aG docker $USER newgrp docker Step 2: Install Docker Compose (Standalone) Authentik uses docker-compose.yml to manage multi-container services.\nReference: Docker Compose Install Guide\n# Download Docker Compose binary sudo curl -L \u0026#34;https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m)\u0026#34; \\ -o /usr/local/bin/docker-compose # Set permissions sudo chmod +x /usr/local/bin/docker-compose # Verify installation docker-compose version Step 3: Create Authentik Directory and Configuration Reference: Authentik Docker Install Guide\n# Create and navigate to the installation directory mkdir -p ~/authentik cd ~/authentik #Download the official docker-compose.yml curl -o docker-compose.yml https://goauthentik.io/docker-compose.yml #Create an .env file to override confiugration values cat \u0026lt;\u0026lt;EOF \u0026gt; .env AUTHENTIK_SECRET_KEY=$(openssl rand -hex 32) POSTGRES_PASSWORD=$(openssl rand -hex 16) AUTHENTIK_EMAIL__FROM=\u0026#34;admin@example.com\u0026#34; AUTHENTIK_EMAIL__HOST=\u0026#34;localhost\u0026#34; EOF Step 4: Start Authentik Services Start the containers using Docker Compose:\ndocker-compose pull # Pull latest images docker-compose up -d # Start in detached mode Step 5: Access Web Interface Once running, access Authentik at:\nhttp://\u0026lt;your-server-ip\u0026gt;:9000\ror\rhttps://\u0026lt;your-server-ip\u0026gt;:9443 Step 6: Initial Setup Wizard I had some trouble getting the initial setup wizard for Authentik to cooperate with me. The wizard would not let me setup the default akdmin account on a http connection. To resolve this issue do the following:\nBe sure that the docker container for authentik-server is listening on port 9443:\nsudo ss -tulpn | grep LISTEN Access the initial setup wizard using the link below:\nhttps://\u0026lt;your server\u0026#39;s IP or hostname\u0026gt;:9443/if/flow/initial-setup/ Related Posts Provisioning Samba Active Directory Domain Controller and Windows Domain Integration Integrating Samba 4 Active Directory with Authentik via LDAPS\n","permalink":"https://blog.tillynet.com/on-premise-engineering-labs/provisioning-authentik-for-sso-on-a-self-hosted-ubuntu-server-docker-based/","summary":"\u003cp\u003eThis guide documents the step-by-step process used to provision an \u003ca href=\"https://goauthentik.io\"\u003eAuthentik\u003c/a\u003e identity provider server on a self-hosted Ubuntu Server using Docker and Docker Compose. This setup is suitable for advanced home lab environments and follows production-grade containerization practices.\u003c/p\u003e\n\u003ch2 id=\"prerequisites\"\u003ePrerequisites\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eA fresh or existing Ubuntu 22.04 or 24.04 LTS server.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esudo\u003c/code\u003e privileges on the system.\u003c/li\u003e\n\u003cli\u003eStatic IP and DNS configuration recommended.\u003c/li\u003e\n\u003cli\u003eSystem updates applied.\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"step-1-install-docker-engine\"\u003eStep 1: Install Docker Engine\u003c/h2\u003e\n\u003cp\u003eFollow the official Docker post-install guide to install and configure Docker for non-root use:\u003c/p\u003e","title":"Provisioning Authentik for SSO on a Self-Hosted Ubuntu Server (Docker-Based)"},{"content":"1. Server Preparation OS: Ubuntu Server 24.04.2 LTS\nInitial Setup:\nStatic IP address manually configured\nIP: 172.30.30.30/24\nGateway: 172.30.30.1\nDNS (initially): 172.21.21.21 (Pi-hole)\nInstalled basic utilities (OpenSSH, networking tools)\n2. Samba Installation and Configuration Installation Commands:\nsudo apt update sudo apt full-upgrade sudo apt install samba krb5-config krb5-user winbind smbclient Service Management:\nDisabled default Samba services to prepare for AD DC mode:\nsudo systemctl disable smbd nmbd winbind sudo systemctl stop smbd nmbd winbind Provision Domain Controller:\nsudo samba-tool domain provision --use-rfc2307 --interactive Realm: TILLYNET.LAN\nDomain: TILLYNET\nServer Role: Domain Controller (DC)\nDNS Backend: SAMBA_INTERNAL\nDNS Forwarder: Initially pointed to Pi-hole (172.21.21.21)\nPost-Provision:\nSamba auto-generated clean /etc/samba/smb.conf. 3. Troubleshooting During Provisioning Provisioning Error: Existing smb.conf:\nDeleted pre-existing /etc/samba/smb.conf before reprovisioning. DNS Conflict with systemd-resolved:\nOverwrote /etc/resolv.conf to manually point to 127.0.0.1. Kerberos KDC Lookup Failure:\nEncountered \u0026ldquo;Cannot find KDC\u0026rdquo; errors until DNS pointed correctly. DNS Port 53 Not Listening Initially:\nRestarted samba-ad-dc to bind correctly. Benign DNS Update Errors (Exit Code 29):\nIgnored initial race conditions during service startup. SRV Record Lookup Failure:\nSRV records appeared correctly after service stabilization. No **dns forwarder** Command:\nConfirmed that DNS forwarder must be set during domain provision. 4. Kerberos Configuration Kerberos File Setup: /etc/krb5.conf overwritten with minimal:\n[libdefaults] default_realm = TILLYNET.LAN dns_lookup_realm = false dns_lookup_kdc = true 5. DNS Forwarding and Testing DNS Forwarding:\nSet during provisioning; no samba-tool command available post-provision. DNS Functionality Testing:\ndig @127.0.0.1 google.com host -t SRV _kerberos._udp.tillynet.lan samba-tool dns query 127.0.0.1 tillynet.lan @ ALL Confirmed correct A records and SRV records.\n6. Windows Client Domain Join Windows Version: Windows 11 Pro\nActions:\nConfigured PC to use Samba server as DNS.\nJoined to domain TILLYNET.LAN via System Properties.\nCreated new domain administrative account tillyadmin.\n7. Profile Migration Tool Used: ForensIT User Profile Wizard (Community Edition)\nAction: Migrated old local user profile to domain user (tillyadmin).\nOutcome:\nFiles migrated\nSome environmental conflicts detected (e.g., SSH agent issues, mismatched user folders)\n8. Git and SSH Environment Setup Challenges:\nSSH agent issues (error connecting to agent: No such file or directory).\nIncorrect user profile folder (C:\\Users\\micha used instead of C:\\Users\\tillyadmin).\nDiagnosis:\nDomain login identity correct (tillynet\\tillyadmin).\nFilesystem path inherited from old local user.\nPlan for Correction:\nFully remove the broken tillyadmin profile.\nReprovision fresh tillyadmin domain account.\nCreate clean C:\\Users\\tillyadmin profile.\nReconfigure SSH keys and Git environment under clean domain context.\nRelated Posts Provisioning Authentik for SSO on a Self-Hosted Ubuntu Server (Docker-Based)\nProvisioning Authentik for SSO on a Self-Hosted Ubuntu Server (Docker-Based)\n","permalink":"https://blog.tillynet.com/on-premise-engineering-labs/provisioning-samba-active-directory-domain-controller-and-windows-domain-integration/","summary":"\u003ch2 id=\"1-server-preparation\"\u003e1. Server Preparation\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eOS:\u003c/strong\u003e Ubuntu Server 24.04.2 LTS\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eInitial Setup:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eStatic IP address manually configured\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eIP: \u003ccode\u003e172.30.30.30/24\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGateway: \u003ccode\u003e172.30.30.1\u003c/code\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eDNS (initially): \u003ccode\u003e172.21.21.21\u003c/code\u003e (Pi-hole)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eInstalled basic utilities (OpenSSH, networking tools)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"2-samba-installation-and-configuration\"\u003e2. Samba Installation and Configuration\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eInstallation Commands:\u003c/strong\u003e\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo apt update\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo apt full-upgrade\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo apt install samba krb5-config krb5-user winbind smbclient\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eService Management:\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eDisabled default Samba services to prepare for AD DC mode:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo systemctl disable smbd nmbd winbind\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003esudo systemctl stop smbd nmbd winbind\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eProvision Domain Controller:\u003c/strong\u003e\u003c/p\u003e","title":"Provisioning Samba Active Directory Domain Controller and Windows Domain Integration"},{"content":"Background This post documents the troubleshooting process I followed to resolve an internet connectivity issue affecting virtual machines (VMs) hosted on a Proxmox VE node running on a Wi-Fi-only Dell XPS 15. The VMs were deployed on a routed internal subnet (172.30.30.0/24) with static routes configured through a pfSense firewall. While the VMs could communicate with internal hosts—including DNS and gateway IPs—they were unable to reach external internet addresses such as 8.8.8.8.\nThe setup avoided NAT by design to maintain full route visibility, relying instead on static routing and properly scoped firewall rules.\nEnvironment Overview Proxmox Host: Dell XPS 15 running Proxmox VE 8.4.1 on top of Debian 12\nProxmox VM Subnet: 172.30.30.0/24\nHost Wi-Fi IP: Routed via 172.21.21.15 on VLAN 21\nGateway (pfSense): 172.21.21.1 with a static route pointing to 172.30.30.0/24\nFirewall Role: pfSense acts as central gateway and inter-VLAN router\nIssue Summary Despite a correct static IP configuration on the Arch Linux VM, and successful ping tests to internal IPs (e.g., DNS on 172.21.21.21 and pfSense gateway at 172.21.21.1), the VM could not reach external addresses.\nTroubleshooting Steps 1. Validated VM IP Configuration Set a static IP on the VM from the live Arch Linux installer:\nip addr add 172.30.30.10/24 dev enp0s18\rip link set enp0s18 up\rip route add default via 172.30.30.1\recho \u0026#34;nameserver \u0026lt;172.21.21.21\u0026gt;\u0026#34; \u0026gt; /etc/resolv.conf Ping to internal IPs succeeded, confirming basic layer 3 connectivity.\n2. Captured Outbound Packets Ran tcpdump on the Proxmox host Wi-Fi interface:\nsudo tcpdump -i wlp0s20f3 host 8.8.8.8 Confirmed that ICMP packets were leaving the host to the internet.\n3. Monitored pfSense Interfaces Used pfSense’s built-in packet capture utility to validate:\nOutbound ICMP requests were reaching the WAN interface\nNo ICMP replies were returning\nARP traffic on the WAN was unrelated to the issue\n4. Created an Outbound NAT Rule Realized that pfSense was not NAT\u0026rsquo;ing the routed subnet. Added a rule under: Firewall \u0026gt; NAT \u0026gt; Outbound:\nSource: 172.30.30.0/24\nInterface: WAN\nTranslation: Interface Address\nMode: Hybrid Outbound NAT\n5. Re-tested with Packet Capture Still no success—packets left, but replies were dropped.\n6. Reviewed Firewall Rules Found a restrictive rule on the VLAN 21 interface that only allowed 172.30.30.0/24 to access hosts within VLAN 21. This prevented pfSense from responding to traffic that was destined for external addresses.\n7. Corrected Firewall Rule Modified the rule to allow outbound traffic from 172.30.30.0/24 to any destination. Immediately after saving:\nVM was able to ping 8.8.8.8\ncurl ifconfig.me returned the public IP, confirming full internet access\nOutcome The VM now has stable internet access with proper routing and NAT handling, while retaining the benefits of internal subnet isolation and firewall control. The root issue stemmed from a well-intended but overly strict firewall rule that blocked replies from beyond the VLAN scope.\nNext Steps Refactor firewall rules for tighter security once validation is complete\nConsider isolating NAT-enabled vs. routed-only subnets\nExplore using systemd-networkd for persistent network configs in Arch VMs\n","permalink":"https://blog.tillynet.com/on-premise-engineering-labs/resolving-internet-connectivity-issues-for-proxmox-vms-on-a-routed-subnet/","summary":"\u003ch2 id=\"background\"\u003eBackground\u003c/h2\u003e\n\u003cp\u003eThis post documents the troubleshooting process I followed to resolve an internet connectivity issue affecting virtual machines (VMs) hosted on a Proxmox VE node running on a Wi-Fi-only Dell XPS 15. The VMs were deployed on a routed internal subnet (\u003ccode\u003e172.30.30.0/24\u003c/code\u003e) with static routes configured through a pfSense firewall. While the VMs could communicate with internal hosts—including DNS and gateway IPs—they were unable to reach external internet addresses such as \u003ccode\u003e8.8.8.8\u003c/code\u003e.\u003c/p\u003e","title":"Resolving Internet Connectivity Issues for Proxmox VMs on a Routed Subnet"},{"content":"Overview In this post, I document how I deployed Proxmox VE on a Dell XPS 15 laptop with no physical Ethernet interface. This machine was added to my home lab as a standalone hypervisor, running independently from my main Protectli-based Proxmox node.\nBecause the XPS lacks wired connectivity, I had to work through some unique networking constraints, including bridging over Wi-Fi and enabling connectivity for guest virtual machines. This write-up covers the initial NAT-based setup and the transition to a cleaner routed network model with static routes via pfSense.\nEnvironment Summary Hardware: Dell XPS 15 (no Ethernet NIC)\nHost OS: Debian 12 (Bookworm) base installation\nHypervisor: Proxmox VE 8.4.1 installed manually\nWireless Interface: wlp0s20f3 (connected to VLAN 21 - Production)\nVirtual Bridge: vmbr0 (for LXC and VM traffic on isolated subnet)\nInstalling Proxmox VE 8.4.1 on top of Debian 12 /etc/network/interfaces iface wlp0s20f3 iface wlp0s20f3 inet static address 172.21.21.15/24 gateway 172.21.21.1 wpa-ssid xxxxxx wpa-psk xxxxxxx /etc/hosts 127.0.0.1 localhost.localdomain localhost 127.21.21.15 xps15.tillynet.lan xps15 Reset Interface ifdown wlp0s20f3 ifup wlp0s20f3 Proxmox apt Repository nano /etc/apt/sources.list.d/pve-install-repo.list deb [arch=amd64] http://download.proxmox.com/debian/pve bookworm pve-no-subscription This adds the Proxmox apt repository in the sources.list.d folder.\nwget http://download.proxmox.com/debian/proxmox-release-bookworm.gpg -O /etc/apt/trusted.gpg.d/proxmox-release-bookworm.gpg This adds the Proxmox gpg key and places it into a specific folder so that apt will find it.\napt update apt full-upgrade Makes sure that everything is up to date before installing Proxmox. Once done, Proxmox can be installed from apt packages with the command below.\napt install proxmox-ve postfix open-iscsi apt remove os-prober Initial Network Setup with NAT Due to the Linux kernel’s limitation on bridging wireless interfaces directly, I created a bridge (vmbr0) with no attached physical ports. Initially, I used NAT (masquerading) to allow outbound internet access for containers and VMs.\n/etc/network/interfaces auto vmbr0 iface vmbr0 inet static address 172.30.30.1/24 bridge_ports none bridge_stp off bridge_fd 0 Do not use bridge-ports, bridge-stp, or bridge-fd — those will fail validation in Debian/Proxmox deployment.\nEnable IP Forwarding nano /etc/sysctl.conf net.ipv4.ip_forward= 1 NAT Rule (iptables) sudo iptables -t nat -A POSTROUTING -s 172.30.30.0/24 -o wlp0s20f3 -j MASQUERADE Make NAT Rule persistent (install iptables-persistent) sudo apt install iptables-persistent This approach allowed outbound traffic but made the Proxmox node act as a NAT gateway. The firewall (pfSense) would only see the host’s IP (172.21.21.15) and not the internal clients.\nTransition to Routed Networking with Static Route To enable full visibility and allow routed traffic from other VLANs, I removed the NAT rule and configured pfSense with a static route to the Proxmox-hosted subnet.\nRemove NAT sudo iptables -t nat -D POSTROUTING -s 172.30.30.0/24 -o wlp0s20f3 -j MASQUERADE Save Cleaned-up Rules sudo iptables-save \u0026gt; /etc/iptables/rules.v4 pfSense Static Route Configuration Destination Network: 172.30.30.0/24\nGateway: 172.21.21.15 (Proxmox host IP)\nInterface: Production VLAN (VLAN 21)\nFirewall Rules: Allowed inter-VLAN access from trusted zones\nLXC/VM Guest Network Settings IP Address: 172.30.30.x\nSubnet Mask: 255.255.255.0\nGateway: 172.30.30.1\nDNS: 172.21.21.21 (internal Pi-hole)\nWith this configuration, all traffic is routed properly between pfSense and the isolated Proxmox subnet, and there\u0026rsquo;s no longer a need for NAT.\nOutcome This setup enabled my Wi-Fi-only XPS laptop to function as a fully routed Proxmox hypervisor on a dedicated subnet. By avoiding NAT, I maintained visibility and control over LXC and VM traffic from my central firewall. The solution is scalable and works well within my VLAN-segmented home lab.\nFuture Plans Add lightweight shared storage (e.g., NFS over VLAN 21)\nAutomate Proxmox LXC deployment and backups via Ansible\nPossibly integrate into a Proxmox cluster using a third quorum-only node\n","permalink":"https://blog.tillynet.com/on-premise-engineering-labs/deploying-a-proxmox-node-on-a-wi-fi-only-dell-xps-laptop/","summary":"\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eIn this post, I document how I deployed Proxmox VE on a Dell XPS 15 laptop with no physical Ethernet interface. This machine was added to my home lab as a standalone hypervisor, running independently from my main Protectli-based Proxmox node.\u003c/p\u003e\n\u003cp\u003eBecause the XPS lacks wired connectivity, I had to work through some unique networking constraints, including bridging over Wi-Fi and enabling connectivity for guest virtual machines. This write-up covers the initial NAT-based setup and the transition to a cleaner routed network model with static routes via pfSense.\u003c/p\u003e","title":"Deploying a Proxmox Node on a Wi-Fi-Only Dell XPS Laptop"},{"content":"Current Running Version of TillyNet TillyNet is my custom-built home lab environment designed to practice enterprise-grade network segmentation, high availability, and security enforcement using open-source tools. This architecture simulates production-level infrastructure and showcases my capabilities in network engineering, virtualization, firewall administration, and Linux system management.\nNetwork Topology Network Design Objectives Layer 2/3 segmentation using VLANs Centralized routing and firewalling with pfSense Hypervisor-based infrastructure using Proxmox VE Containerized services for DNS and wireless controller management Policy-based access control enforced through inter-VLAN firewall rules Minimal trust zones with internal DNS filtering and strict pathing VLAN Overview VLAN ID Purpose Subnet 1 Default/legacy 172.16.7.0/24 14 Guest Wi-Fi 172.16.14.0/24 21 Production DNS 172.21.21.0/24 99 Management Access 172.16.99.0/24 666 Native Trunk VLAN N/A Each VLAN is routed via a pfSense firewall using a router-on-a-stick model over a single trunk interface connected to a managed Cisco Catalyst switch.\nCore Infrastructure Router/Firewall pfSense (virtualized) on a dedicated x86 appliance Handles inter-VLAN routing, DHCP, NAT, and firewall policy enforcement Configured with strict rules: Inter-VLAN traffic is blocked by default Each VLAN is only permitted DNS access to a local recursive DNS server Admin GUI access is restricted to the management VLAN Proxmox Virtualization Host: Protectli Vault (fanless x86 appliance)\nProxmox VE 8.3 running:\npfSense VM (firewall/gateway) LXC container: Pi-hole DNS (VLAN 21) LXC container: Omada Controller (VLAN 99) Network bridges:\nvmbr0: Management (backup) vmbr1: Trunked interface to switch (VLANs 14, 21, 99) vmbr1.99: Tagged VLAN interface for host-level MGMT access Linux Networking (Proxmox) The Proxmox host is configured using Linux network bridges with VLAN-aware capabilities to support secure, segmented networking for containers and virtual machines. All networking is statically defined in /etc/network/interfaces, offering full control and reproducibility of the setup.\nThis design ensures:\nVLAN tagging at the hypervisor level\nTrunk delivery of VLANs to LXCs and pfSense VM\nHost isolation through a dedicated VLAN interface (vmbr1.99)\nCompatibility with PCI passthrough NICs for physical routing\nNetwork Interface Layout Interface Role Type IP Address Notes enp2s0 pfSense LAN PCI passthru — Routed to switch for VLAN trunking enp3s0 pfSense WAN PCI passthru — Connected to ISP modem enp1s0 MGMT bridge uplink Physical — Tagged VLAN trunk to Catalyst enp4s0 Backup management Physical — Static untagged link vmbr0 Backup mgmt bridge Linux bridge 172.16.7.15/24 Management fallback IP vmbr1 VLAN trunk bridge Linux bridge — Tagged VLAN trunk to containers vmbr1.99 Host MGMT interface VLAN subif 172.16.99.15/24 Used for Proxmox admin access /etc/network/interfaces Configuration\nauto lo iface lo inet loopback # Backup Management Physical NIC iface enp4s0 inet manual # MGMT Trunk NIC iface enp1s0 inet manual # PCI Passthrough to pfSense iface enp2s0 inet manual iface enp3s0 inet manual # Backup Management Bridge (Plan to decommission) auto vmbr0 iface vmbr0 inet static address 172.16.7.15/24 bridge-ports enp4s0 bridge-stp off bridge-fd 0 bridge-vlan-aware yes bridge-vids 2-4094 dns-nameservers 1.1.1.1 8.8.8.8 # MGMT Trunk Bridge auto vmbr1 iface vmbr1 inet manual bridge-ports enp1s0 bridge-stp off bridge-fd 0 bridge-vlan-aware yes bridge-vids 2-4094 # VLAN 99 Subinterface for Proxmox Host auto vmbr1.99 iface vmbr1.99 inet static address 172.16.99.15/24 gateway 172.16.99.1 VLAN Usage by Containers Each LXC container is assigned to the appropriate VLAN through Proxmox’s vlan tag setting, while still connected to the same vmbr1 bridge. This enables seamless multi-VLAN networking without needing additional physical NICs.\nOmada Controller:\nBridge: vmbr1 VLAN Tag: 99 IP: 172.16.99.35/24 Pi-hole DNS:\nBridge: vmbr1 VLAN Tag: 21 IP: 172.21.21.21/24 With this configuration, containers receive only the VLAN traffic they are explicitly assigned, and host-level access is limited to a single tagged VLAN interface — a model that mirrors enterprise best practices in virtual networking.\nHost-Level VLAN Interface To maintain separation between the Proxmox host and the container traffic, a dedicated subinterface vmbr1.99 is configured for the management VLAN. This allows host-level SSH and web access only from the management network.\nauto vmbr1.99 iface vmbr1.99 inet static address 172.16.99.15/24 gateway 172.16.99.1 vlan-raw-device vmbr1 LXC Automation \u0026amp; Cron Jobs To maintain the health and performance of containerized services within TillyNet, I’ve implemented lightweight automation using cron inside each LXC. This approach keeps core services updated and resilient without the overhead of full-scale configuration management tools — while still remaining extensible.\nPi-hole (LXC 300) Automated via root cron job:\n# Run Pi-hole gravity update daily at 2:00 AM 0 2 * * * /usr/local/bin/pihole updateGravity \u0026gt; /var/log/pihole_cron.log 2\u0026gt;\u0026amp;1 This ensures the ad-blocking and threat feed lists are kept up-to-date without manual intervention.\nOmada Controller (LXC 200) Automated via cron for regular backups:\n# Backup Omada site config every day at 3:00 AM 0 3 * * * /opt/tplink/omada/data/autobackup.sh \u0026gt;\u0026gt; /var/log/omada_backup.log 2\u0026gt;\u0026amp;1 The backup script syncs the controller config and wireless SSID/site layout to a local or external backup target.\nSystem-Wide (Both LXCs) General update routine:\n# Security updates every Sunday at 4:00 AM 0 4 * * 0 apt update \u0026amp;\u0026amp; apt -y upgrade \u0026gt;\u0026gt; /var/log/apt_cron.log 2\u0026gt;\u0026amp;1 This ensures both containers remain patched and secured, with logs rotated weekly via logrotate.\nServices \u0026amp; Roles Service Location IP (Subnet) VLAN Notes Firewall/Gateway pfSense VM Trunked interface All Routes all VLANs DNS Filtering Pi-hole LXC 172.21.21.21/24 21 Internal DNS for all VLANs WAP Control Omada Controller 172.16.99.35/24 99 Manages EAP access points Management GUI Proxmox Host 172.16.99.15/24 99 VLAN-tagged virtual interface Switching Layer Cisco Catalyst 2960-C switch Configured with trunk ports for uplinks and Proxmox host Native VLAN 666 used to isolate untagged traffic Access and trunk ports statically assigned to appropriate VLANs STP (PVST) with system-id extension enabled Wireless Infrastructure Access Point: TP-Link Omada EAP series Guest Wi-Fi SSID isolated via VLAN 14 WPA2/WPA3 mixed security VLAN tagging applied per SSID to ensure proper segmentation Security Posture DNS Centralization: All VLANs rely on a local Pi-hole for DNS queries Access Control: Only specific ports (e.g., DNS, admin access) allowed Microsegmentation: Each VLAN is isolated; no lateral movement allowed GUI Lockdown: Firewall admin access restricted to trusted VLAN Guest Isolation: Guest devices have zero access to internal infrastructure Learning Outcomes \u0026amp; Skills Demonstrated Advanced VLAN trunking and switch configuration Implementation of router-on-a-stick using pfSense Design of containerized services using LXC on Proxmox Mastery of firewall rule creation and policy-based routing Application of zero trust principles in a home network Real-world exposure to enterprise wireless configuration Linux and open-source toolchain integration Final Notes This deployment represents the current live version of TillyNet and serves both as a personal learning platform and a functional demonstration of scalable, secure network design. Each layer is intentionally crafted to mirror best practices seen in production environments across SMB and enterprise infrastructure.\nFuture plans include using Ansible for automated provisioning of LXC containers and firewall rule templating. This is version 2.0 of TillyNet. Future versions will expand automation, introduce container orchestration, and deploy a backup DNS service.\n","permalink":"https://blog.tillynet.com/on-premise-engineering-labs/current-running-version-of-tillynet/","summary":"\u003ch1 id=\"current-running-version-of-tillynet\"\u003eCurrent Running Version of TillyNet\u003c/h1\u003e\n\u003cp\u003eTillyNet is my custom-built home lab environment designed to practice enterprise-grade network segmentation, high availability, and security enforcement using open-source tools. This architecture simulates production-level infrastructure and showcases my capabilities in network engineering, virtualization, firewall administration, and Linux system management.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"network-topology\"\u003eNetwork Topology\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"Image\" loading=\"lazy\" src=\"/images/tillynet_mermaid.png\"\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"network-design-objectives\"\u003eNetwork Design Objectives\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLayer 2/3 segmentation\u003c/strong\u003e using VLANs\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eCentralized routing and firewalling\u003c/strong\u003e with pfSense\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHypervisor-based infrastructure\u003c/strong\u003e using Proxmox VE\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eContainerized services\u003c/strong\u003e for DNS and wireless controller management\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePolicy-based access control\u003c/strong\u003e enforced through inter-VLAN firewall rules\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMinimal trust zones\u003c/strong\u003e with internal DNS filtering and strict pathing\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"vlan-overview\"\u003eVLAN Overview\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eVLAN ID\u003c/th\u003e\n          \u003cth\u003ePurpose\u003c/th\u003e\n          \u003cth\u003eSubnet\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003eDefault/legacy\u003c/td\u003e\n          \u003ctd\u003e172.16.7.0/24\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e14\u003c/td\u003e\n          \u003ctd\u003eGuest Wi-Fi\u003c/td\u003e\n          \u003ctd\u003e172.16.14.0/24\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e21\u003c/td\u003e\n          \u003ctd\u003eProduction DNS\u003c/td\u003e\n          \u003ctd\u003e172.21.21.0/24\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e99\u003c/td\u003e\n          \u003ctd\u003eManagement Access\u003c/td\u003e\n          \u003ctd\u003e172.16.99.0/24\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e666\u003c/td\u003e\n          \u003ctd\u003eNative Trunk VLAN\u003c/td\u003e\n          \u003ctd\u003eN/A\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eEach VLAN is routed via a pfSense firewall using a router-on-a-stick model over a single trunk interface connected to a managed Cisco Catalyst switch.\u003c/p\u003e","title":"Current Running Version of TillyNet"},{"content":"Creating a Network Topology Using Graphviz and diagrams Package in Python This post outlines my initial attempts at visualizing the architecture of my home lab environment using Python-based tools. Given the complexity of my setup, traditional visualization methods weren’t sufficient. After coming across a post about using the graphviz engine with the diagrams Python package, I decided to experiment with it.\nThe toolchain proved useful for rendering simple network flows, and while it doesn’t fully capture the complexity of my environment, it’s a valuable starting point for documentation and network planning. Below is a walkthrough of how to get started, along with a sample script and the resulting topology diagram.\nTools Used Python – Scripting language used to build the topology diagram. Graphviz – Graph rendering engine that powers diagram generation. Diagrams – Python library for programmatically defining and visualizing infrastructure components. Installation Installation is straightforward. Follow the official documentation on the diagrams website.\nWindows users: Be sure to add the Graphviz\\bin directory to your system\u0026rsquo;s user PATH:\nPress Win + R and type sysdm.cpl to open System Properties. Navigate to the Advanced tab and click Environment Variables. Under User variables, select Path, then click Edit. Add a new entry pointing to your Graphviz bin directory. Click OK to save and close the dialogs. My Graphviz Python Script from diagrams import Cluster, Diagram from diagrams.onprem.network import Internet from diagrams.generic.network import Switch from diagrams.generic.os import Windows from diagrams.generic.device import Mobile from diagrams.onprem.network import Pfsense from diagrams.onprem.proxmox import Pve from diagrams.onprem.proxmox import ProxmoxVE from diagrams.generic.network import Router from diagrams.generic.network import VPN with Diagram(\u0026#34;TillyNet Home Lab Topology\u0026#34;, show=True, direction=\u0026#34;LR\u0026#34;): internet = Internet(\u0026#34;ISP\u0026#34;) switch = Switch(\u0026#34;Catalyst 2960-C\u0026#34;) openvpn = VPN(\u0026#34;OpenVPN Tunnel\u0026#34;) with Cluster(\u0026#34;.\\nVirtualizated Stack\u0026#34;): pfsense = Pfsense(\u0026#34;pfSense FW\u0026#34;) proxmox = Pve(\u0026#34;Hypervisor\u0026#34;) with Cluster(\u0026#34;VLAN 99 - Management\u0026#34;): mgmt_pc = Windows(\u0026#34;Win Mgmt PC\u0026#34;) ap_mgmt = Router(\u0026#34;EAP670 AP\u0026#34;,) omada = ProxmoxVE(\u0026#34;Omada Controller\u0026#34;) proxmox_mgmt = Pve(\u0026#34;Proxmox Management\u0026#34;) with Cluster(\u0026#34;VLAN 14 - Guest\u0026#34;): guest_wifi = Mobile(\u0026#34;Guest Devices\u0026#34;) with Cluster(\u0026#34;.\\nVLAN 21 - Production\u0026#34;): pihole = ProxmoxVE(\u0026#34;Pi-hole\u0026#34;) with Cluster(\u0026#34;.\\nRemote Management\u0026#34;): remote_mgmt = Mobile(\u0026#34;Remote Manager\u0026#34;) # Connections internet \u0026gt;\u0026gt; pfsense pfsense \u0026gt;\u0026gt; switch switch \u0026gt;\u0026gt; [mgmt_pc, omada, proxmox_mgmt, ap_mgmt, pihole] ap_mgmt \u0026gt;\u0026gt; guest_wifi remote_mgmt \u0026gt;\u0026gt; openvpn openvpn \u0026gt;\u0026gt; mgmt_pc Generated Network Topology Current TillyNet Home Lab Environment - Summary The TillyNet home lab is a virtualized, VLAN-segmented network environment designed to simulate enterprise-grade infrastructure and support hands-on experimentation with routing, firewalling, and network automation technologies. It is built around a Proxmox VE hypervisor hosted on a Protectli Vault VP2420, which runs multiple virtual machines and LXC containers to provide core network services.\nKey components include:\nProxmox VE: Serves as the core hypervisor with both VM and container-based workloads. It utilizes PCI passthrough for direct access to physical NICs.\npfSense Firewall (VM): Operates as the primary gateway using a router-on-a-stick configuration via a trunked LAN interface to a Cisco Catalyst 2960-C switch. It handles all inter-VLAN routing, DHCP, and firewall policies.\nVLAN Configuration:\nVLAN 1 – Native LAN\nVLAN 14 – Guest network, isolated and tagged for wireless SSID on the access point\nVLAN 21 – Production network housing internal services like DNS (Pi-hole)\nVLAN 99 – Management network, used for administrative access to Proxmox, Omada Controller, and the access point\nPi-hole (LXC): Runs as the local recursive DNS resolver for the production VLAN.\nOmada Controller (LXC): Manages the TP-Link EAP670 access point and wireless network provisioning.\nTP-Link EAP670 (AP): Connected via a trunk port to the Catalyst switch to support both VLAN 14 (guest) and VLAN 99 (management).\nCisco Catalyst 2960-C Switch: Provides L2 switching with trunk and access port configurations to support VLAN segmentation and inter-device communication.\nThis architecture allows for advanced testing of enterprise networking scenarios, including network segmentation, access control, VLAN trunking, recursive DNS, and wireless network integration — all within a self-hosted environment.\nFinal Thoughts The diagrams library combined with graphviz is a powerful tool for basic network visualization. While it may not be suited for highly detailed or dynamic environments, it offers a clean way to represent logical infrastructure layouts. I plan to continue exploring more advanced or specialized network visualization tools, but this is a solid foundation for documenting my home lab.\n","permalink":"https://blog.tillynet.com/on-premise-engineering-labs/experimenting-with-network-topologies/","summary":"\u003ch1 id=\"creating-a-network-topology-using-graphviz-and-diagrams-package-in-python\"\u003eCreating a Network Topology Using Graphviz and diagrams Package in Python\u003c/h1\u003e\n\u003cp\u003eThis post outlines my initial attempts at visualizing the architecture of my home lab environment using Python-based tools. Given the complexity of my setup, traditional visualization methods weren’t sufficient. After coming across a post about using the \u003ccode\u003egraphviz\u003c/code\u003e engine with the \u003ccode\u003ediagrams\u003c/code\u003e Python package, I decided to experiment with it.\u003c/p\u003e\n\u003cp\u003eThe toolchain proved useful for rendering simple network flows, and while it doesn’t fully capture the complexity of my environment, it’s a valuable starting point for documentation and network planning. Below is a walkthrough of how to get started, along with a sample script and the resulting topology diagram.\u003c/p\u003e","title":"Experimenting With Network Topologies"},{"content":"Obsidian → Hugo → GitHub → Hostinger Automation Workflow This post documents the full setup of my static site publishing pipeline that automates taking blog posts from Obsidian, rendering them with Hugo, pushing the output to GitHub, and then having Hostinger automatically update my website via webhook.\nTools Used Obsidian – Markdown note-taking and blog writing Hugo – Static site generator using the PaperMod theme Hugo PaperMod Wiki My Hugo yaml File GitHub – Repository with two branches: master → Hugo project \u0026amp; content hostinger → Static site output Hostinger – Web hosting platform with webhook integration Python – Automation script for content syncing and site deployment My Automation Script 📁 Folder Structure 📂 Local_Obsidian_Vault/ └── my-home-lab-journey/ ├── post-1/ └── index.md ├── post-2/ └── index.md ├── post-3/ └── index.md └── ... 📂 tillynetblog/ ├── hugo.yaml ├── content/ │ └── my-home-lab-journey/ ├── public/ ├── themes/ │ └── PaperMod/ └── static/ └── images └── css/custom.css What the Automation Script Does Imports import os import shutil import subprocess import re os: for filesystem operations like path handling shutil: for copying/removing files and directories subprocess: for running shell commands like hugo and git re: for finding image links in markdown via regular expressions Configuration obsidian_dir = r\u0026#34;C:\\Users\\micha\\Documents\\Local_Obsidian_Vault\\my-home-lab-journey\u0026#34; hugo_root_dir = r\u0026#34;C:\\Users\\micha\\Documents\\tillynetblog\u0026#34; hugo_content_dir = os.path.join(hugo_root_dir, \u0026#34;content\u0026#34;, \u0026#34;my-home-lab-journey\u0026#34;) attachments_dir = r\u0026#34;C:\\Users\\micha\\Documents\\Local_Obsidian_Vault\\assets\\images\u0026#34; static_images_dir = os.path.join(hugo_root_dir, \u0026#34;static\u0026#34;, \u0026#34;images\u0026#34;) about_src = r\u0026#34;C:\\Users\\micha\\Documents\\Local_Obsidian_Vault\\pages\\about.md\u0026#34; about_dst_dir = os.path.join(hugo_root_dir, \u0026#34;content\u0026#34;, \u0026#34;about\u0026#34;) about_dst = os.path.join(about_dst_dir, \u0026#34;index.md\u0026#34;) base_url = \u0026#34;https://blog.tillynet.com\u0026#34; Sets paths to: Obsidian content (obsidian_dir) Hugo blog folder Where to copy markdown posts Where Obsidian images are and where to move them Source/destination for the about.md file The blog\u0026rsquo;s base URL for Hugo STEP 1: Copy Markdown Posts if os.path.exists(hugo_content_dir): shutil.rmtree(hugo_content_dir) shutil.copytree(obsidian_dir, hugo_content_dir) print(\u0026#34;✔ Copied markdown posts from Obsidian.\u0026#34;) Deletes existing markdown content in Hugo Copies fresh markdown files from Obsidian Confirms the copy STEP 2: Convert Image Embeds and Copy Images for subdir, _, files in os.walk(hugo_content_dir): for filename in files: if filename.endswith(\u0026#34;.md\u0026#34;): ... iterates through every markdown file in the blog content images = re.findall(r\u0026#39;\\[\\[([^]]*\\.png)\\]\\]\u0026#39;, content) Looks for image link like (example.png) for image in images: markdown_image = f\u0026#34;![Image](/images/{image.replace(\u0026#39; \u0026#39;, \u0026#39;%20\u0026#39;)})\u0026#34; Replaces with standard Markdown image syntax src_image = os.path.join(attachments_dir, image) if os.path.exists(src_image): shutil.copy(src_image, static_images_dir) copies matching images from Obsidian to Hugo\u0026rsquo;s static image folder with open(md_path, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as file: file.write(content) Overwrites the file with the updated content print(\u0026#34;✔ Processed images and updated markdown links.\u0026#34;) Confirms processing is complete STEP 3: Copy About Page if os.path.exists(about_src): os.makedirs(about_dst_dir, exist_ok=True) shutil.copyfile(about_src, about_dst) print(\u0026#34;✔ Updated About page as /about/index.md.\u0026#34;) else: print(\u0026#34;⚠ About page not found in Obsidian vault; skipping.\u0026#34;) Copies about.md from Obsidian to Hugo content Skips and warns if the file is missing STEP 4: Clean Existing public/ Folder public_dir = os.path.join(hugo_root_dir, \u0026#34;public\u0026#34;) if os.path.exists(public_dir): shutil.rmtree(public_dir) print(\u0026#34;✔ Cleaned existing public/ folder.\u0026#34;) Removes the old public/ folder Forces Hugo to rebuild the entire static site from scratch, picking up all new pages and changes STEP 5: Build the Hugo Site subprocess.run([\u0026#34;hugo\u0026#34;, \u0026#34;--buildDrafts\u0026#34;, \u0026#34;--buildFuture\u0026#34;, \u0026#34;-b\u0026#34;, base_url], cwd=hugo_root_dir, check=True) print(\u0026#34;✔ Hugo site built with baseURL.\u0026#34;) Builds the site using the hugo command with my specified BaseURL STEP 6: Push Source Files to GitHub master subprocess.run([\u0026#34;git\u0026#34;, \u0026#34;checkout\u0026#34;, \u0026#34;master\u0026#34;], cwd=hugo_root_dir, check=True) subprocess.run([\u0026#34;git\u0026#34;, \u0026#34;add\u0026#34;, \u0026#34;.\u0026#34;], cwd=hugo_root_dir, check=True) subprocess.run([\u0026#34;git\u0026#34;, \u0026#34;commit\u0026#34;, \u0026#34;-m\u0026#34;, \u0026#34;Update blog content\u0026#34;], cwd=hugo_root_dir, check=False) subprocess.run([\u0026#34;git\u0026#34;, \u0026#34;push\u0026#34;, \u0026#34;origin\u0026#34;, \u0026#34;master\u0026#34;], cwd=hugo_root_dir, check=True) print(\u0026#34;✔ Pushed changes to GitHub master.\u0026#34;) Commits and pushes all changes to the master branch STEP 7: Deploy Public Folder to hostinger Branch subprocess.run([\u0026#34;git\u0026#34;, \u0026#34;subtree\u0026#34;, \u0026#34;split\u0026#34;, \u0026#34;--prefix\u0026#34;, \u0026#34;public\u0026#34;, \u0026#34;-b\u0026#34;, \u0026#34;hostinger-deploy\u0026#34;], cwd=hugo_root_dir, check=True) subprocess.run([\u0026#34;git\u0026#34;, \u0026#34;push\u0026#34;, \u0026#34;origin\u0026#34;, \u0026#34;hostinger-deploy:hostinger\u0026#34;, \u0026#34;--force\u0026#34;], cwd=hugo_root_dir, check=True) subprocess.run([\u0026#34;git\u0026#34;, \u0026#34;branch\u0026#34;, \u0026#34;-D\u0026#34;, \u0026#34;hostinger-deploy\u0026#34;], cwd=hugo_root_dir, check=True) print(\u0026#34;✔ Deployed public/ folder to GitHub hostinger branch.\u0026#34;) Splits public/ into a new branch Pushes it forcefully to the hostinger branch on GitHub (used for site deployment) Deletes the temporary hostinger-deploy branch This is necessary for our webhook to recognize the changes Issues I Encountered CSS Not Applying on Deployed Site Root Cause: Hostinger was caching fingerprinted CSS\nFix:\nDisabled Hugo asset fingerprinting in hugo.yaml:\nassets: disableFingerprinting: true Verified correct path: assets/css/custom.css\nLessons Learned Static site deployment workflows require full control over content paths and assets Asset fingerprinting can break styling if not handled properly Obsidian\u0026rsquo;s [[embed]] syntax must be converted for Hugo compatibility Git subtree pushing keeps deployment clean and isolated to public/ Have my python automation script remove the old public/ folder so that Hugo is forced to rebuild the entire static site from scratch thus picking up all new pages and changes Final Outcome End-to-end deployment is fully automated\nBlog posts are written in Obsidian subfolders\nSingle Python script deploys everything to GitHub and Hostinger\nDuplicate post issues and CSS bugs resolved\nDark/light theme and custom styles work across devices\nResume Bullet Built an automated static site deployment pipeline linking Obsidian, Hugo (PaperMod), GitHub, and Hostinger via webhook; resolved CSS asset fingerprinting issues, enabled Hugo theme integration, and automated blog post syncing using Python.\n","permalink":"https://blog.tillynet.com/on-premise-engineering-labs/building-my-publishing-pipeline-obsidian-hugo-github-hostinger/","summary":"\u003ch1 id=\"obsidian--hugo--github--hostinger-automation-workflow\"\u003eObsidian → Hugo → GitHub → Hostinger Automation Workflow\u003c/h1\u003e\n\u003cp\u003eThis post documents the full setup of my \u003cstrong\u003estatic site publishing pipeline\u003c/strong\u003e that automates taking blog posts from \u003cstrong\u003eObsidian\u003c/strong\u003e, rendering them with \u003cstrong\u003eHugo\u003c/strong\u003e, pushing the output to \u003cstrong\u003eGitHub\u003c/strong\u003e, and then having \u003cstrong\u003eHostinger\u003c/strong\u003e automatically update my website via webhook.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"tools-used\"\u003eTools Used\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eObsidian\u003c/strong\u003e – Markdown note-taking and blog writing\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHugo\u003c/strong\u003e – Static site generator using the \u003ca href=\"https://github.com/adityatelange/hugo-PaperMod\"\u003ePaperMod theme\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/adityatelange/hugo-PaperMod/wiki/Installation\"\u003eHugo PaperMod Wiki\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/michaeltilly/tillynetblog/blob/master/hugo.yaml\"\u003eMy Hugo yaml File\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGitHub\u003c/strong\u003e – Repository with two branches:\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003emaster\u003c/code\u003e → Hugo project \u0026amp; content\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ehostinger\u003c/code\u003e → Static site output\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eHostinger\u003c/strong\u003e – Web hosting platform with webhook integration\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ePython\u003c/strong\u003e – Automation script for content syncing and site deployment \u003ca href=\"https://github.com/michaeltilly/tillynetblog/blob/master/deploy_tillynetblog2.py\"\u003eMy Automation Script\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"-folder-structure\"\u003e📁 Folder Structure\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-plaintext\" data-lang=\"plaintext\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e📂 Local_Obsidian_Vault/\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  └── my-home-lab-journey/\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      ├── post-1/\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t        └── index.md\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      ├── post-2/\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t        └── index.md\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      ├── post-3/\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t        └── index.md\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      └── ...\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e📂 tillynetblog/\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  ├── hugo.yaml\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  ├── content/\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  │   └── my-home-lab-journey/\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  ├── public/\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  ├── themes/\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  │   └── PaperMod/\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e  └── static/\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\t  └── images\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e      └── css/custom.css\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003chr\u003e\n\u003ch2 id=\"what-the-automation-script-does\"\u003eWhat the Automation Script Does\u003c/h2\u003e\n\u003ch3 id=\"imports\"\u003eImports\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003eos\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003eshutil\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003esubprocess\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003ere\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eos\u003c/strong\u003e: for filesystem operations like path handling\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eshutil\u003c/strong\u003e: for copying/removing files and directories\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esubprocess\u003c/strong\u003e: for running shell commands like \u003ccode\u003ehugo\u003c/code\u003e and \u003ccode\u003egit\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003ere\u003c/strong\u003e: for finding image links in markdown via regular expressions\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"configuration\"\u003eConfiguration\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eobsidian_dir\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"sa\"\u003er\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;C:\\Users\\micha\\Documents\\Local_Obsidian_Vault\\my-home-lab-journey\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ehugo_root_dir\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"sa\"\u003er\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;C:\\Users\\micha\\Documents\\tillynetblog\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ehugo_content_dir\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eos\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ejoin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ehugo_root_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;my-home-lab-journey\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eattachments_dir\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"sa\"\u003er\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;C:\\Users\\micha\\Documents\\Local_Obsidian_Vault\\assets\\images\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003estatic_images_dir\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eos\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ejoin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ehugo_root_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;static\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;images\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eabout_src\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"sa\"\u003er\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;C:\\Users\\micha\\Documents\\Local_Obsidian_Vault\\pages\\about.md\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eabout_dst_dir\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eos\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ejoin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ehugo_root_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;content\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;about\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eabout_dst\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eos\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ejoin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eabout_dst_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;index.md\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003ebase_url\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;https://blog.tillynet.com\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eSets paths to:\n\u003cul\u003e\n\u003cli\u003eObsidian content (\u003ccode\u003eobsidian_dir\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eHugo blog folder\u003c/li\u003e\n\u003cli\u003eWhere to copy markdown posts\u003c/li\u003e\n\u003cli\u003eWhere Obsidian images are and where to move them\u003c/li\u003e\n\u003cli\u003eSource/destination for the \u003ccode\u003eabout.md\u003c/code\u003e file\u003c/li\u003e\n\u003cli\u003eThe blog\u0026rsquo;s base URL for Hugo\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"step-1-copy-markdown-posts\"\u003eSTEP 1: Copy Markdown Posts\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eos\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eexists\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ehugo_content_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eshutil\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ermtree\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ehugo_content_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003eshutil\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecopytree\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eobsidian_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ehugo_content_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;✔ Copied markdown posts from Obsidian.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eDeletes existing markdown content in Hugo\u003c/li\u003e\n\u003cli\u003eCopies fresh markdown files from Obsidian\u003c/li\u003e\n\u003cli\u003eConfirms the copy\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"step-2-convert-image-embeds-and-copy-images\"\u003eSTEP 2: Convert Image Embeds and Copy Images\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003esubdir\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003e_\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003efiles\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003eos\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewalk\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ehugo_content_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003efilename\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003efiles\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e        \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003efilename\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eendswith\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;.md\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"o\"\u003e...\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eiterates through every markdown file in the blog content\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"n\"\u003eimages\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ere\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003efindall\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"sa\"\u003er\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39;\\[\\[([^]]*\\.png)\\]\\]\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003econtent\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eLooks for image link like (\u003ccode\u003eexample.png\u003c/code\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003eimage\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"n\"\u003eimages\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"n\"\u003emarkdown_image\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"sa\"\u003ef\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;![Image](/images/\u003c/span\u003e\u003cspan class=\"si\"\u003e{\u003c/span\u003e\u003cspan class=\"n\"\u003eimage\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ereplace\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s1\"\u003e\u0026#39; \u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s1\"\u003e\u0026#39;%20\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"si\"\u003e}\u003c/span\u003e\u003cspan class=\"s2\"\u003e)\u0026#34;\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eReplaces with standard Markdown image syntax\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"n\"\u003esrc_image\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eos\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ejoin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eattachments_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eimage\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eos\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eexists\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esrc_image\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                    \u003cspan class=\"n\"\u003eshutil\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecopy\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esrc_image\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003estatic_images_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003ecopies matching images from Obsidian to Hugo\u0026rsquo;s static image folder\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e            \u003cspan class=\"k\"\u003ewith\u003c/span\u003e \u003cspan class=\"nb\"\u003eopen\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003emd_path\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;w\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eencoding\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;utf-8\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003efile\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e                \u003cspan class=\"n\"\u003efile\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ewrite\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003econtent\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eOverwrites the file with the updated content\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;✔ Processed images and updated markdown links.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eConfirms processing is complete\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"step-3-copy-about-page\"\u003eSTEP 3: Copy About Page\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eos\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eexists\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eabout_src\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eos\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emakedirs\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eabout_dst_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eexist_ok\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eshutil\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ecopyfile\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eabout_src\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eabout_dst\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;✔ Updated About page as /about/index.md.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eelse\u003c/span\u003e\u003cspan class=\"p\"\u003e:\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;⚠ About page not found in Obsidian vault; skipping.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eCopies \u003ccode\u003eabout.md\u003c/code\u003e from Obsidian to Hugo content\u003c/li\u003e\n\u003cli\u003eSkips and warns if the file is missing\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"step-4-clean-existing-public-folder\"\u003eSTEP 4: Clean Existing \u003ccode\u003epublic/\u003c/code\u003e Folder\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003epublic_dir\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eos\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ejoin\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ehugo_root_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;public\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"n\"\u003eos\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epath\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eexists\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epublic_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"n\"\u003eshutil\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ermtree\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epublic_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;✔ Cleaned existing public/ folder.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eRemoves the old \u003ccode\u003epublic/\u003c/code\u003e folder\u003c/li\u003e\n\u003cli\u003eForces Hugo to rebuild the entire static site from scratch, picking up all new pages and changes\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"step-5-build-the-hugo-site\"\u003eSTEP 5: Build the Hugo Site\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003esubprocess\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erun\u003c/span\u003e\u003cspan class=\"p\"\u003e([\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;hugo\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;--buildDrafts\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;--buildFuture\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;-b\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ebase_url\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"n\"\u003ecwd\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ehugo_root_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003echeck\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;✔ Hugo site built with baseURL.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eBuilds the site using the \u003ccode\u003ehugo\u003c/code\u003e command with my specified \u003ccode\u003eBaseURL\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"step-6-push-source-files-to-github-master\"\u003eSTEP 6: Push Source Files to GitHub \u003ccode\u003emaster\u003c/code\u003e\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003esubprocess\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erun\u003c/span\u003e\u003cspan class=\"p\"\u003e([\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;git\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;checkout\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;master\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"n\"\u003ecwd\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ehugo_root_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003echeck\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003esubprocess\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erun\u003c/span\u003e\u003cspan class=\"p\"\u003e([\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;git\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;add\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"n\"\u003ecwd\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ehugo_root_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003echeck\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003esubprocess\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erun\u003c/span\u003e\u003cspan class=\"p\"\u003e([\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;git\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;commit\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;-m\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;Update blog content\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"n\"\u003ecwd\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ehugo_root_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003echeck\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003esubprocess\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erun\u003c/span\u003e\u003cspan class=\"p\"\u003e([\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;git\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;push\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;origin\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;master\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"n\"\u003ecwd\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ehugo_root_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003echeck\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;✔ Pushed changes to GitHub master.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eCommits and pushes all changes to the \u003ccode\u003emaster\u003c/code\u003e branch\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"step-7-deploy-public-folder-to-hostinger-branch\"\u003eSTEP 7: Deploy Public Folder to \u003ccode\u003ehostinger\u003c/code\u003e Branch\u003c/h3\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-python\" data-lang=\"python\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003esubprocess\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erun\u003c/span\u003e\u003cspan class=\"p\"\u003e([\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;git\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;subtree\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;split\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;--prefix\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;public\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;-b\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;hostinger-deploy\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"n\"\u003ecwd\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ehugo_root_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003echeck\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003esubprocess\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erun\u003c/span\u003e\u003cspan class=\"p\"\u003e([\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;git\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;push\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;origin\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;hostinger-deploy:hostinger\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;--force\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"n\"\u003ecwd\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ehugo_root_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003echeck\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"n\"\u003esubprocess\u003c/span\u003e\u003cspan class=\"o\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003erun\u003c/span\u003e\u003cspan class=\"p\"\u003e([\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;git\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;branch\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;-D\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;hostinger-deploy\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"n\"\u003ecwd\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ehugo_root_dir\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003echeck\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"kc\"\u003eTrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"nb\"\u003eprint\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s2\"\u003e\u0026#34;✔ Deployed public/ folder to GitHub hostinger branch.\u0026#34;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cul\u003e\n\u003cli\u003eSplits \u003ccode\u003epublic/\u003c/code\u003e into a new branch\u003c/li\u003e\n\u003cli\u003ePushes it forcefully to the \u003ccode\u003ehostinger\u003c/code\u003e branch on GitHub (used for site deployment)\u003c/li\u003e\n\u003cli\u003eDeletes the temporary \u003ccode\u003ehostinger-deploy\u003c/code\u003e branch\n\u003cul\u003e\n\u003cli\u003eThis is necessary for our webhook to recognize the changes\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"issues-i-encountered\"\u003eIssues I Encountered\u003c/h2\u003e\n\u003ch3 id=\"css-not-applying-on-deployed-site\"\u003eCSS Not Applying on Deployed Site\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cstrong\u003eRoot Cause:\u003c/strong\u003e Hostinger was caching fingerprinted CSS\u003c/p\u003e","title":"Building My Publishing Pipeline: Obsidian → Hugo → GitHub → Hostinger"},{"content":"Pi-hole DNS Migration to Production VLAN Overview This phase of the HomeLab project documents the successful migration of the internal Pi-hole DNS server from the flat LAN network to a newly created and isolated Production VLAN. This change enhances security, improves segmentation, and prepares the environment for scalable DNS resolution across all other VLANs.\nObjectives Create and configure a dedicated Production VLAN on pfSense Migrate the Pi-hole LXC container to the new VLAN Ensure inter-VLAN DNS resolution using Pi-hole Apply proper firewall rules to restrict unnecessary access Update DHCP DNS settings across all VLANs Network Summary Component Before Migration After Migration Pi-hole Location LAN network (untagged) Production VLAN (tagged) VLAN ID - Production VLAN ID Subnet LAN Subnet Production Subnet Pi-hole IP LAN Assigned IP VLAN-assigned static IP Access Open to LAN Inter-VLAN DNS only (port 53) Steps Performed 1. Created Production VLAN in pfSense Navigated to Interfaces \u0026gt; Assignments \u0026gt; VLANs Assigned a unique VLAN tag and set the parent interface (LAN) Created new interface, enabled it, and assigned a static IPv4 gateway 2. Updated Pi-hole LXC Container Configuration Edited the container via Proxmox: pct set \u0026lt;CTID\u0026gt; -net0 name=eth0,bridge=vmbr0,tag=\u0026lt;VLAN_ID\u0026gt;,ip=\u0026lt;Pi-hole_IP\u0026gt;/24,gw=\u0026lt;VLAN_Gateway\u0026gt; ","permalink":"https://blog.tillynet.com/on-premise-engineering-labs/pi-hole-dns-migration-to-production-vlan/","summary":"\u003ch1 id=\"pi-hole-dns-migration-to-production-vlan\"\u003ePi-hole DNS Migration to Production VLAN\u003c/h1\u003e\n\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cp\u003eThis phase of the HomeLab project documents the successful migration of the internal Pi-hole DNS server from the flat LAN network to a newly created and isolated \u003cstrong\u003eProduction VLAN\u003c/strong\u003e. This change enhances security, improves segmentation, and prepares the environment for scalable DNS resolution across all other VLANs.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"objectives\"\u003eObjectives\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eCreate and configure a dedicated Production VLAN on pfSense\u003c/li\u003e\n\u003cli\u003eMigrate the Pi-hole LXC container to the new VLAN\u003c/li\u003e\n\u003cli\u003eEnsure inter-VLAN DNS resolution using Pi-hole\u003c/li\u003e\n\u003cli\u003eApply proper firewall rules to restrict unnecessary access\u003c/li\u003e\n\u003cli\u003eUpdate DHCP DNS settings across all VLANs\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"network-summary\"\u003eNetwork Summary\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eComponent\u003c/th\u003e\n          \u003cth\u003eBefore Migration\u003c/th\u003e\n          \u003cth\u003eAfter Migration\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePi-hole Location\u003c/td\u003e\n          \u003ctd\u003eLAN network (untagged)\u003c/td\u003e\n          \u003ctd\u003eProduction VLAN (tagged)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eVLAN ID\u003c/td\u003e\n          \u003ctd\u003e-\u003c/td\u003e\n          \u003ctd\u003eProduction VLAN ID\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eSubnet\u003c/td\u003e\n          \u003ctd\u003eLAN Subnet\u003c/td\u003e\n          \u003ctd\u003eProduction Subnet\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ePi-hole IP\u003c/td\u003e\n          \u003ctd\u003eLAN Assigned IP\u003c/td\u003e\n          \u003ctd\u003eVLAN-assigned static IP\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eAccess\u003c/td\u003e\n          \u003ctd\u003eOpen to LAN\u003c/td\u003e\n          \u003ctd\u003eInter-VLAN DNS only (port 53)\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch2 id=\"steps-performed\"\u003eSteps Performed\u003c/h2\u003e\n\u003ch3 id=\"1-created-production-vlan-in-pfsense\"\u003e1. Created Production VLAN in pfSense\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eNavigated to \u003cstrong\u003eInterfaces \u0026gt; Assignments \u0026gt; VLANs\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eAssigned a unique VLAN tag and set the parent interface (LAN)\u003c/li\u003e\n\u003cli\u003eCreated new interface, enabled it, and assigned a static IPv4 gateway\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"2-updated-pi-hole-lxc-container-configuration\"\u003e2. Updated Pi-hole LXC Container Configuration\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eEdited the container via Proxmox:\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003epct \u003cspan class=\"nb\"\u003eset\u003c/span\u003e \u0026lt;CTID\u0026gt; -net0 \u003cspan class=\"nv\"\u003ename\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003eeth0,bridge\u003cspan class=\"o\"\u003e=\u003c/span\u003evmbr0,tag\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u0026lt;VLAN_ID\u0026gt;,ip\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u0026lt;Pi-hole_IP\u0026gt;/24,gw\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u0026lt;VLAN_Gateway\u0026gt;\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e","title":"Pi-hole DNS Migration to Production VLAN"},{"content":"Remote SDN Recovery \u0026amp; VLAN Isolation via VPN \u0026amp; Shell Access Overview In this project, I successfully re-established full access to my network management stack (Omada Controller, Proxmox GUI, and LXC containers) after losing access due to misconfigured VLAN trunking. The recovery was performed entirely off-site, using only a mobile device connected via an OpenVPN tunnel.\nBackground I transitioned my home lab management network to VLAN 99 to achieve full traffic isolation. In the process of re-tagging Proxmox and LXC container traffic and reconfiguring Cisco switch trunk ports, I lost access to the Omada Controller and Proxmox GUI. With no local access to the console, I needed a remote solution.\nObjective Regain access to VLAN 99 (management) Restore Omada Controller GUI and Proxmox Web GUI Ensure all traffic on management interfaces is VLAN 99 tagged Tools Used Proxmox VE 8.3 pfSense (OpenVPN Server \u0026amp; Firewall) TP-Link Omada Controller (LXC) SSH (mobile terminal access) OpenVPN (Client-to-Site) VLAN-aware Linux bridges (vmbr1) Cisco Catalyst switch (trunk config) Steps Performed Connected to VPN Tunnel 1 (LAN access)\nUsed an existing OpenVPN client-to-site tunnel connected to the LAN network Leveraged this connection as a starting point Established VPN Tunnel 2 (MGMT access)\nConfigured a new OpenVPN tunnel to reach VLAN 99 Verified routing and DNS from VPN client to MGMT subnet Accessed Proxmox Shell Remotely\nUsed SSH to access the Proxmox server shell Reviewed \u0026amp; Edited /etc/network/interfaces\nCreated a Linux Bridge vmbr1 on enp1s0 Configured vmbr1 as VLAN-aware with a VLAN 99 subinterface Reconfigured Omada Controller LXC\nSet static IP from VLAN 99 Edited /etc/pve/lxc/\u0026lt;vmid\u0026gt;.conf to tag interface correctly Updated pfSense Firewall Rules\nAllowed VPN access to Proxmox, Omada Controller, and LXCs Verified GUI access over VPN Safely Restarted Proxmox Networking Stack\nReloaded network stack without reboot to preserve SSH session Verified LXC \u0026amp; Controller Connectivity\nConfirmed restored access to Omada Controller \u0026amp; Proxmox GUI Checked VLAN 99 trunking on Cisco switch Secured VLAN 99\nBlocked all external access to VLAN 99 Allowed only OpenVPN tunnel access Tested Full Remote Management Over VPN\nOutcome Full recovery of Proxmox and Omada Controller Management services isolated to VLAN 99 VLAN trunking corrected on switch Secure remote access via OpenVPN tunnel All executed off-site using a mobile SSH client Screenshots \u0026amp; CLI Snippets (To be added) Network configuration diagrams VLAN trunking layout (Cisco Catalyst Switch) show vlan brief 1 default active 14 GUEST active 21 PRODUCTION active 99 MANAGEMENT active 666 BLACKHOLE active show interfaces trunk Port Mode Encapsulation Status Native vlan Fa0/7 on 802.1q trunking 666 Fa0/8 on 802.1q trunking 666 Gi0/2 on 802.1q trunking 666 Port Vlans allowed on trunk Fa0/7 1,14,21,99,666 Fa0/8 1,14,21,99,666 Gi0/2 1,14,21,99,666 Port Vlans allowed and active in management domain Fa0/7 1,14,21,99,666 Fa0/8 1,14,21,99,666 Gi0/2 1,14,21,99,666 Proxmox Interfaces cat /etc/network/interfaces auto lo iface lo inet loopback iface enp4s0 inet manual #Proxmox Management iface enp1s0 inet manual #MGMT Bridge Link iface enx60189502f716 inet manual iface enp2s0 inet manual #PfSense LAN iface enp3s0 inet manual #PfSense WAN auto vmbr0 iface vmbr0 inet static address fallback.management.ip/24 bridge-ports enp4s0 bridge-stp off bridge-fd 0 bridge-vlan-aware yes bridge-vids 2-4094 dns-nameservers 1.1.1.1 8.8.8.8 #Native Proxmox Management auto vmbr1 iface vmbr1 inet manual bridge-ports enp1s0 bridge-stp off bridge-fd 0 bridge-vlan-aware yes bridge-vids 2-4094 #MGMT Bridge auto vmbr1.99 iface vmbr1.99 inet static address management.ip/24 gateway management.gateway.ip #Proxmox MGMT 99 source /etc/network/interfaces.d/* Omada Controller Network Config cat /etc/network/interfaces #TP-Link Omada Controller (Ubuntu 22.04) arch: amd64 cores: 1 features: nesting=1 hostname: omada memory: 2304 net0:name=eth0,bridge=vmbr1,firewall=1,gw=management.gateway.ip,hwaddr=macaddress,ip=management.ip/24,tag=99,type=veth onboot: 1 ostype: ubuntu rootfs: local-lvm:vm-200-disk-0,size=8G startup: order=3,up=30,down=120 swap: 512 unprivileged: 1 openvpn status Lessons Learned Always test VLAN changes with fallback access Proxmox shell is essential for remote recovery OpenVPN enables secure remote SDN administration Avoid dual gateway configurations on bridges Future Improvements Add fallback management IP on separate VLAN Setup out-of-band access (serial/IPMI) Automate Proxmox network config backups Resume Bullet Performed live remote SDN infrastructure recovery using OpenVPN and SSH from a mobile device; reconfigured VLAN-tagged Proxmox bridges, updated pfSense firewall rules, and restored access to critical network services including Omada Controller and LXC containers on an isolated management VLAN. ","permalink":"https://blog.tillynet.com/on-premise-engineering-labs/remote-sdn-recovery-vlan-isolation-via-vpn-shell-access/","summary":"\u003ch2 id=\"remote-sdn-recovery--vlan-isolation-via-vpn--shell-access\"\u003eRemote SDN Recovery \u0026amp; VLAN Isolation via VPN \u0026amp; Shell Access\u003c/h2\u003e\n\u003ch3 id=\"overview\"\u003eOverview\u003c/h3\u003e\n\u003cp\u003eIn this project, I successfully re-established full access to my network management stack (Omada Controller, Proxmox GUI, and LXC containers) after losing access due to misconfigured VLAN trunking. The recovery was performed entirely \u003cstrong\u003eoff-site\u003c/strong\u003e, using only a mobile device connected via an \u003cstrong\u003eOpenVPN tunnel\u003c/strong\u003e.\u003c/p\u003e\n\u003chr\u003e\n\u003ch3 id=\"background\"\u003eBackground\u003c/h3\u003e\n\u003cp\u003eI transitioned my home lab management network to VLAN 99 to achieve full traffic isolation. In the process of re-tagging Proxmox and LXC container traffic and reconfiguring Cisco switch trunk ports, I lost access to the Omada Controller and Proxmox GUI. With no local access to the console, I needed a remote solution.\u003c/p\u003e","title":"Remote SDN Recovery \u0026 VLAN Isolation via VPN \u0026 Shell Access"},{"content":"02 - Migrating to a Dedicated Management VLAN (VLAN 99) This phase documents the migration of all core management infrastructure to an isolated VLAN (VLAN 99) for improved security, network segmentation, and long-term scalability. This included Proxmox GUI access, Omada Controller LXC, and strict firewall rules enforced via pfSense.\nGoals Remove critical services from default/native VLAN Assign a dedicated, isolated VLAN (VLAN 99) for: Proxmox management GUI Omada Controller (LXC) Trunk management VLAN through switch to Proxmox Implement firewall rules to allow remote admin access only Preserve service availability during transition Pre-Migration Topology Component Network VLAN Interface Description Proxmox Host LAN VLAN 1 vmbr0 Static IP via native VLAN Omada Controller LAN VLAN 1 vmbr0 LXC container, web GUI on port 8043 Pi-hole LAN VLAN 1 vmbr0 DNS LXC VPN Tunnel LAN VLAN 1 pfSense Remote client-to-site access Post-Migration Topology Component Network VLAN Interface Description Proxmox Host Management VLAN 99 vmbr1.99 Tagged IP for GUI access via vmbr1 Omada Controller Management VLAN 99 vmbr1 (tagged) LXC container with VLAN tag 99 Pi-hole LAN VLAN 1 vmbr0 LXC container Trunk Port (Switch) Trunked Port 1,99,14 enp1s0 Connected to VLAN-aware bridge vmbr1 VPN Tunnel Routed to MGMT VLAN 99 pfSense Allows external admin access to VLAN 99 Migration Steps 1. Create VLAN 99 in pfSense Interfaces \u0026gt; Assignments \u0026gt; VLANs Created VLAN 99 on the LAN parent interface Assigned it as a new interface and renamed it to MGMT Enabled the interface and set a static IP (management subnet) 2. Configure Proxmox Bridge for VLAN Tagging Created a new VLAN-aware bridge and subinterface in /etc/network/interfaces:\nauto vmbr1 iface vmbr1 inet manual bridge-ports enp1s0 bridge-stp off bridge-fd 0 bridge-vlan-aware yes bridge-vids 2-4094 auto vmbr1.99 iface vmbr1.99 inet static address \u0026lt;management_ip\u0026gt;/24 gateway \u0026lt;management_gateway\u0026gt; ","permalink":"https://blog.tillynet.com/on-premise-engineering-labs/creating-network-management-isolation/","summary":"\u003ch1 id=\"02---migrating-to-a-dedicated-management-vlan-vlan-99\"\u003e02 - Migrating to a Dedicated Management VLAN (VLAN 99)\u003c/h1\u003e\n\u003cp\u003eThis phase documents the migration of all core management infrastructure to an isolated VLAN (VLAN 99) for improved security, network segmentation, and long-term scalability. This included Proxmox GUI access, Omada Controller LXC, and strict firewall rules enforced via pfSense.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"goals\"\u003eGoals\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eRemove critical services from default/native VLAN\u003c/li\u003e\n\u003cli\u003eAssign a dedicated, isolated VLAN (VLAN 99) for:\n\u003cul\u003e\n\u003cli\u003eProxmox management GUI\u003c/li\u003e\n\u003cli\u003eOmada Controller (LXC)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eTrunk management VLAN through switch to Proxmox\u003c/li\u003e\n\u003cli\u003eImplement firewall rules to allow remote admin access only\u003c/li\u003e\n\u003cli\u003ePreserve service availability during transition\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"pre-migration-topology\"\u003ePre-Migration Topology\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eComponent\u003c/th\u003e\n          \u003cth\u003eNetwork\u003c/th\u003e\n          \u003cth\u003eVLAN\u003c/th\u003e\n          \u003cth\u003eInterface\u003c/th\u003e\n          \u003cth\u003eDescription\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eProxmox Host\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eLAN\u003c/td\u003e\n          \u003ctd\u003eVLAN 1\u003c/td\u003e\n          \u003ctd\u003evmbr0\u003c/td\u003e\n          \u003ctd\u003eStatic IP via native VLAN\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eOmada Controller\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eLAN\u003c/td\u003e\n          \u003ctd\u003eVLAN 1\u003c/td\u003e\n          \u003ctd\u003evmbr0\u003c/td\u003e\n          \u003ctd\u003eLXC container, web GUI on port 8043\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003ePi-hole\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eLAN\u003c/td\u003e\n          \u003ctd\u003eVLAN 1\u003c/td\u003e\n          \u003ctd\u003evmbr0\u003c/td\u003e\n          \u003ctd\u003eDNS LXC\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eVPN Tunnel\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eLAN\u003c/td\u003e\n          \u003ctd\u003eVLAN 1\u003c/td\u003e\n          \u003ctd\u003epfSense\u003c/td\u003e\n          \u003ctd\u003eRemote client-to-site access\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch2 id=\"post-migration-topology\"\u003ePost-Migration Topology\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eComponent\u003c/th\u003e\n          \u003cth\u003eNetwork\u003c/th\u003e\n          \u003cth\u003eVLAN\u003c/th\u003e\n          \u003cth\u003eInterface\u003c/th\u003e\n          \u003cth\u003eDescription\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eProxmox Host\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eManagement\u003c/td\u003e\n          \u003ctd\u003eVLAN 99\u003c/td\u003e\n          \u003ctd\u003evmbr1.99\u003c/td\u003e\n          \u003ctd\u003eTagged IP for GUI access via \u003ccode\u003evmbr1\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eOmada Controller\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eManagement\u003c/td\u003e\n          \u003ctd\u003eVLAN 99\u003c/td\u003e\n          \u003ctd\u003evmbr1 (tagged)\u003c/td\u003e\n          \u003ctd\u003eLXC container with VLAN tag 99\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003ePi-hole\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eLAN\u003c/td\u003e\n          \u003ctd\u003eVLAN 1\u003c/td\u003e\n          \u003ctd\u003evmbr0\u003c/td\u003e\n          \u003ctd\u003eLXC container\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eTrunk Port (Switch)\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eTrunked Port\u003c/td\u003e\n          \u003ctd\u003e1,99,14\u003c/td\u003e\n          \u003ctd\u003eenp1s0\u003c/td\u003e\n          \u003ctd\u003eConnected to VLAN-aware bridge \u003ccode\u003evmbr1\u003c/code\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eVPN Tunnel\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eRouted to MGMT\u003c/td\u003e\n          \u003ctd\u003eVLAN 99\u003c/td\u003e\n          \u003ctd\u003epfSense\u003c/td\u003e\n          \u003ctd\u003eAllows external admin access to VLAN 99\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch2 id=\"migration-steps\"\u003eMigration Steps\u003c/h2\u003e\n\u003ch3 id=\"1-create-vlan-99-in-pfsense\"\u003e1. Create VLAN 99 in pfSense\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eInterfaces \u0026gt; Assignments \u0026gt; VLANs\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eCreated VLAN 99 on the LAN parent interface\u003c/li\u003e\n\u003cli\u003eAssigned it as a new interface and renamed it to \u003ccode\u003eMGMT\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eEnabled the interface and set a static IP (management subnet)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"2-configure-proxmox-bridge-for-vlan-tagging\"\u003e2. Configure Proxmox Bridge for VLAN Tagging\u003c/h3\u003e\n\u003cp\u003eCreated a new VLAN-aware bridge and subinterface in \u003ccode\u003e/etc/network/interfaces\u003c/code\u003e:\u003c/p\u003e","title":"Creating Network Management Isolation"},{"content":"HomeLab: Initial Network Setup This project documents the first working phase of my home network infrastructure built on top of Proxmox, using pfSense as a virtual firewall/router and LXC containers to host internal services. The design lays the foundation for a scalable, secure, and isolated home lab environment.\nOverview Hypervisor: Proxmox VE running on Protectli Vault VP2420 (Intel J6412, 4 NICs) Router/Firewall: pfSense VM Internal Services: Pi-hole (LXC) – local recursive DNS + ad-blocking Omada Controller (LXC) – TP-Link AP management LAN Devices: Proxmox host, personal workstation, AP, switch Guest Devices: IoT \u0026amp; roommate devices on isolated VLAN 14 Infrastructure at a Glance Component Description pfSense VM with two PCI-passthrough NICs (WAN + LAN) vmbr0 Bridge for LAN (Proxmox + LXCs) vmbr1 Reserved for future VLAN tagging (e.g., mgmt) Pi-hole LXC container for DNS (on LAN) Omada Ctrl LXC container managing TP-Link EAP670 AP Cisco Switch Access switch trunking VLANs to Proxmox/AP Guest VLAN 14 WiFi-only VLAN for roommate \u0026amp; IoT devices Setup Timeline Phase 1 – Core Infrastructure Flashed Proxmox onto Protectli Vault Created pfSense VM with 2 passthrough NICs: WAN: connected to ISP modem LAN: connected to Cisco switch (trunk-ready) Configured vmbr0 as LAN bridge in Proxmox Gave Proxmox host static IP on the LAN network Phase 2 – Internal Services Provisioned LXC container for Pi-hole Static IP assigned Configured upstream DNS servers (e.g., Cloudflare) Provisioned LXC container for Omada Controller Used to manage TP-Link EAP670 AP Served on LAN via Omada web GUI (port 8043) Phase 3 – Wireless \u0026amp; Guest VLAN Set up VLAN 14 in pfSense (Guest Network) Trunked VLANs through switch port to Omada AP Created isolated wireless SSID mapped to VLAN 14 Configured firewall rules in pfSense: Guests can access WAN only Blocked access to LAN and Pi-hole Verified DHCP lease and internet access for guests Observed isolated traffic from personal network Security Practices Created distinct VLANs for guest vs personal network Isolated Pi-hole to LAN access only Disabled inter-VLAN routing from Guest → LAN Assigned firewall rules by interface in pfSense Reserved management services for trusted VLAN only To-Do / Next Steps Create VLAN 99 for network management Move Proxmox GUI and Omada Controller to VLAN 99 Add remote access via OpenVPN (completed later) Diagram full topology and backup strategy Network Diagram Lessons Learned Always reserve a static fallback IP for management pfSense is extremely powerful when paired with LXC containers VLANs and firewall rules are critical to proper isolation Omada Controller offers enterprise-like wireless management Resume Bullet (from this phase) Deployed full virtual home network lab with pfSense firewall, VLAN isolation, and internal services (DNS, WiFi controller) using Proxmox and LXC containers; implemented guest network segregation and trunked VLANs across Cisco infrastructure. ","permalink":"https://blog.tillynet.com/on-premise-engineering-labs/the-big-bang-how-it-all-began/","summary":"\u003ch1 id=\"homelab-initial-network-setup\"\u003eHomeLab: Initial Network Setup\u003c/h1\u003e\n\u003cp\u003eThis project documents the first working phase of my home network infrastructure built on top of Proxmox, using pfSense as a virtual firewall/router and LXC containers to host internal services. The design lays the foundation for a scalable, secure, and isolated home lab environment.\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"overview\"\u003eOverview\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eHypervisor:\u003c/strong\u003e Proxmox VE running on Protectli Vault VP2420 (Intel J6412, 4 NICs)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRouter/Firewall:\u003c/strong\u003e pfSense VM\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInternal Services:\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003ePi-hole (LXC) – local recursive DNS + ad-blocking\u003c/li\u003e\n\u003cli\u003eOmada Controller (LXC) – TP-Link AP management\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eLAN Devices:\u003c/strong\u003e Proxmox host, personal workstation, AP, switch\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGuest Devices:\u003c/strong\u003e IoT \u0026amp; roommate devices on isolated VLAN 14\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"infrastructure-at-a-glance\"\u003eInfrastructure at a Glance\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eComponent\u003c/th\u003e\n          \u003cth\u003eDescription\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003epfSense\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eVM with two PCI-passthrough NICs (WAN + LAN)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003evmbr0\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eBridge for LAN (Proxmox + LXCs)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003evmbr1\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eReserved for future VLAN tagging (e.g., mgmt)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003ePi-hole\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eLXC container for DNS (on LAN)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eOmada Ctrl\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eLXC container managing TP-Link EAP670 AP\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eCisco Switch\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eAccess switch trunking VLANs to Proxmox/AP\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003cstrong\u003eGuest VLAN 14\u003c/strong\u003e\u003c/td\u003e\n          \u003ctd\u003eWiFi-only VLAN for roommate \u0026amp; IoT devices\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch2 id=\"setup-timeline\"\u003eSetup Timeline\u003c/h2\u003e\n\u003ch3 id=\"phase-1--core-infrastructure\"\u003ePhase 1 – Core Infrastructure\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eFlashed Proxmox onto Protectli Vault\u003c/li\u003e\n\u003cli\u003eCreated pfSense VM with 2 passthrough NICs:\n\u003cul\u003e\n\u003cli\u003eWAN: connected to ISP modem\u003c/li\u003e\n\u003cli\u003eLAN: connected to Cisco switch (trunk-ready)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eConfigured \u003ccode\u003evmbr0\u003c/code\u003e as LAN bridge in Proxmox\u003c/li\u003e\n\u003cli\u003eGave Proxmox host static IP on the LAN network\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"phase-2--internal-services\"\u003ePhase 2 – Internal Services\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eProvisioned LXC container for Pi-hole\n\u003cul\u003e\n\u003cli\u003eStatic IP assigned\u003c/li\u003e\n\u003cli\u003eConfigured upstream DNS servers (e.g., Cloudflare)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eProvisioned LXC container for Omada Controller\n\u003cul\u003e\n\u003cli\u003eUsed to manage TP-Link EAP670 AP\u003c/li\u003e\n\u003cli\u003eServed on LAN via Omada web GUI (port 8043)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"phase-3--wireless--guest-vlan\"\u003ePhase 3 – Wireless \u0026amp; Guest VLAN\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eSet up VLAN 14 in pfSense (Guest Network)\u003c/li\u003e\n\u003cli\u003eTrunked VLANs through switch port to Omada AP\u003c/li\u003e\n\u003cli\u003eCreated isolated wireless SSID mapped to VLAN 14\u003c/li\u003e\n\u003cli\u003eConfigured firewall rules in pfSense:\n\u003cul\u003e\n\u003cli\u003eGuests can access WAN only\u003c/li\u003e\n\u003cli\u003eBlocked access to LAN and Pi-hole\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eVerified DHCP lease and internet access for guests\u003c/li\u003e\n\u003cli\u003eObserved isolated traffic from personal network\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"security-practices\"\u003eSecurity Practices\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eCreated distinct VLANs for guest vs personal network\u003c/li\u003e\n\u003cli\u003eIsolated Pi-hole to LAN access only\u003c/li\u003e\n\u003cli\u003eDisabled inter-VLAN routing from Guest → LAN\u003c/li\u003e\n\u003cli\u003eAssigned firewall rules by interface in pfSense\u003c/li\u003e\n\u003cli\u003eReserved management services for trusted VLAN only\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"to-do--next-steps\"\u003eTo-Do / Next Steps\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eCreate VLAN 99 for network management\u003c/li\u003e\n\u003cli\u003eMove Proxmox GUI and Omada Controller to VLAN 99\u003c/li\u003e\n\u003cli\u003eAdd remote access via OpenVPN (completed later)\u003c/li\u003e\n\u003cli\u003eDiagram full topology and backup strategy\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"network-diagram\"\u003eNetwork Diagram\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"Image\" loading=\"lazy\" src=\"/images/TillyNet_OG.drawio.png\"\u003e\u003c/p\u003e","title":"The Big Bang (How it All Began)"}]